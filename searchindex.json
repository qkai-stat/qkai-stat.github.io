{"categories":[{"title":"linux","uri":"https://qkai-stat.github.io/categories/linux/"},{"title":"R语言","uri":"https://qkai-stat.github.io/categories/r%E8%AF%AD%E8%A8%80/"},{"title":"学术杂谈","uri":"https://qkai-stat.github.io/categories/%E5%AD%A6%E6%9C%AF%E6%9D%82%E8%B0%88/"},{"title":"笔记","uri":"https://qkai-stat.github.io/categories/%E7%AC%94%E8%AE%B0/"},{"title":"诗词歌赋","uri":"https://qkai-stat.github.io/categories/%E8%AF%97%E8%AF%8D%E6%AD%8C%E8%B5%8B/"}],"posts":[{"content":"变量选择是高维统计建模中的重要内容。自1996年Tibshirani提出LASSO后，基于正则化模型的变量选择研究日益壮大、独领风潮。LASSO采用 $l _1$ 惩罚，得益于 $l _1$ 罚项在零点的奇异性，LASSO可以同时实现系数稀疏化（变量选择）和系数估计。事实上，LASSO估计可以看成一个软阈值（soft thresholding）准则，即它将绝对值较小的系数估计直接压缩成零，而较大的系数估计则做了一个有偏估计。因此，虽然LASSO优点众多，但是不容忽视的是LASSO有对大系数低估的缺陷。本文介绍了Fan和Li（2001）提出一种非凸罚项，称之为SCAD（smoothly clipped absolute deviation）罚项。与LASSO不同，SCAD非凸，其对应的正则化模型不仅能够同时实现变量选择和参数估计，还拥有更加好的统计性质，即Oracle性。\n1 问题的引入 考虑线性模型\n$$ y = X\\beta + \\epsilon \\qquad (1) $$\n其中 $y\\in \\mathbb{R}^n$ 是观测向量，$X\\in \\mathbb{R}^{n\\times p}$ 是设计阵（design matrix），$\\beta\\in \\mathbb{R}^p$ 是系数向量，$n$ 是样本的个数，$p$ 是变量的个数。我们假定模型（1）是稀疏的，即存在一部分系数为零（对应的变量为冗余变量）、其余系数非零（对应的变量为重要变量）。不失一般性，我们假定前 $q$ 个变量重要，则 $\\beta _1, \\ldots, \\beta _q \\neq 0$；后$p - q$ 个变量冗余，则 $\\beta _{q+1}, \\ldots, \\beta _p = 0$。此外，本文研究的是固定维情形，即 $p$ 随着 $n$ 的增大保持不变。\n在前言中，我们曾提到LASSO罚项对大系数低估的缺陷。那么，一个良好的罚项应当具有怎样的性质呢？考虑关于模型（1）的惩罚最小二乘\n$$ arg\\min _{\\beta} \\frac12 \\Vert y - X\\beta\\Vert^2 + \\lambda\\sum _{i=1}^p p _j(\\vert \\beta _j\\vert) $$\n其中 $\\lambda\u0026gt;0$ 是调节参数（tuning parameter），控制系数的压缩程度。$p _j(|\\cdot|)$ 是系数的罚项，对不同的 $\\beta _j$，$p _j(|\\cdot|)$可以允许不同。令 $\\lambda p(|\\cdot|) = p _{\\lambda}(|\\cdot|)$，则罚项 $p(|\\cdot|)$ 可以看成是 $\\lambda$ 的函数。\nFan和Li（2001）认为，一个好的罚函数应当产生具有如下三个性质的估计量：\n 无偏性（unbiasedness）：当真实的未知参数很大（绝对值）时，估计量应该渐近无偏，从而避免产生不必要的模型偏度。 稀疏性（sparsity）：估计量是一个阈值准则（thresholding rule），即它可以自动将绝对值较小的系数估计直接压缩到零以降低模型复杂度。 连续性（continuity）：估计量应当是基于数据得到的 $\\hat{\\beta}^{ols}$ 的连续函数，以减少模型预测的不稳定性。  为了探究罚项对系数估计的影响，我们考虑一种特殊的情形，即设计阵正交（orthogonal）。若设计阵正交，则最小二乘估计满足 $\\hat{\\beta}^{ols} = (X^TX)^{-1}X^Ty = X^Ty$，上述惩罚最小二乘可以改写成\n$$ \\begin{split} \u0026amp;arg\\min _{\\beta} \\frac12 \\Vert y - X\\beta\\Vert^2 + \\lambda\\sum _{i=1}^p p _j(\\vert \\beta _j\\vert)\\newline \\Leftrightarrow \u0026amp;arg\\min _{\\beta} \\frac12 \\Vert y - XX^Ty + XX^Ty - X\\beta\\Vert^2 + \\lambda\\sum _{i=1}^p p _j(\\vert \\beta _j\\vert)\\newline \\Leftrightarrow \u0026amp;arg\\min _{\\beta} \\frac12 \\Vert y - XX^Ty\\Vert^2 + \\frac12 \\sum _{j=1}^p (\\hat{\\beta} _j^{ols} - \\beta _j)^2 + \\lambda\\sum _{i=1}^p p _j(\\vert \\beta _j\\vert)\\newline \\Leftrightarrow \u0026amp;arg\\min _{\\beta} \\frac12 \\sum _{j=1}^p (\\hat{\\beta} _j^{ols} - \\beta _j)^2 + \\lambda\\sum _{i=1}^p p _j(\\vert \\beta _j\\vert)\\newline \\end{split} $$\n上述最小化问题等价于逐个元素的最小化问题，即\n$$ arg\\min _{\\theta} \\frac 12 (\\hat{\\beta} _j^{ols} - \\theta)^2 + p _{\\lambda}(\\vert \\theta\\vert) $$\n对 $\\theta$ 求导数得到\n$$ \\theta - \\hat{\\beta} _j^{ols} + p _{\\lambda}'(|\\theta|)\\text{sgn}(\\theta) = \\text{sgn}(\\theta)(|\\theta| + p _{\\lambda}'(|\\theta|)) - \\hat{\\beta} _j^{ols} \\qquad (2) $$\n我们分析上述一阶导数的不同情况可能产生的估计量：\n  对较大的 $|\\theta|$ 有 $p _{\\lambda}'(|\\theta|) = 0$ 是较大系数估计无偏的充分条件。若对较大的 $|\\theta|$ 有 $p _{\\lambda}'(|\\theta|) = 0$，很容易看出当 $|z|$ 很大时，对应的估计为 $\\hat{\\theta} = \\hat{\\beta} _j^{ols}$。这样，若真实参数 $|\\theta|$ 很大，则相应的 $|\\hat{\\beta} _j^{ols}|$ 也高概率很大。因此，估计量 $\\hat{\\theta} = \\hat{\\beta} _j^{ols}$ 近似无偏。\n  一阶导数中 $|\\theta| + p _{\\lambda}'(|\\theta|)$ 最小值为正是估计量为阈值准则的充分条件。下图展示了 $|\\theta| + p _{\\lambda}'(|\\theta|)$ 和 $\\theta$ 的关系图。（1）当 $|z| \u0026lt; \\min _{\\theta\\neq 0} \\lbrace|\\theta| + p _{\\lambda}'(|\\theta|)\\rbrace$ 时，即图中的 $z _1$，则导数式（2）的结果对正的 $\\theta$ 始终为正（负的 $\\theta$ 始终为负），故 $\\hat{\\theta} = 0$；当 $|z| \u0026gt; \\min _{\\theta\\neq 0} \\lbrace|\\theta| + p _{\\lambda}'(|\\theta|)\\rbrace$ 时，即图中的 $z _2$，此时产生两个交点，较大的点对应就是惩罚最小二乘估计量。\n  最小化问题 $\\min \\lbrace|\\theta| + p _{\\lambda}'(|\\theta|)\\rbrace$ 在零点达到是估计量关于 $\\hat{\\beta} _j^{ols}$ 连续的充分条件。如前述，若 $\\min \\lbrace|\\theta| + p _{\\lambda}'(|\\theta|)\\rbrace$ 在零点达到，则当 $|z|$ 不断减小直至小于 $\\min \\lbrace|\\theta| + p _{\\lambda}'(|\\theta|)\\rbrace$ 时，估计量连续的变化到零；当 $|z| \u0026lt; \\min _{\\theta\\neq 0} \\lbrace|\\theta| + p _{\\lambda}'(|\\theta|)\\rbrace$ 时，估计量为零。所以，此时的估计量关于 $\\hat{\\beta} _j^{ols}$ 连续。  基于上述的分析，我们回头来看几个常见的罚项：岭罚项、硬阈值（hard thresholding）罚项和LASSO罚项。首先，岭罚项 $p _{\\lambda}(|\\theta|) = \\lambda \\theta^2$，它的 $p _{\\lambda}'(|\\theta|)$ 在 $|\\beta|$ 较大时非零，故不满足无偏性；$|\\theta| + p _{\\lambda}'(|\\theta|)$ 最小值始终为零，故不满足稀疏性；但可以看出岭估计具有连续性。其次，硬阈值罚项 $p _{\\lambda}(|\\theta|) = \\lambda^2 - (|\\theta| - \\lambda^2)^2I(|\\theta| \u0026lt; \\lambda)$，它的 。最后，LASSO罚项 $p _{\\lambda}(|\\theta|) = \\lambda |\\theta|$，它的$p _{\\lambda}'(|\\theta|)$ 在 $|\\beta|$ 较大时非零，故不满足无偏性；但可以看出LASSO估计具有稀疏性和连续性。下图分别展示了（a）硬阈值罚项和（b）LASSO罚项的示意图。\n显然，根据前面的分析，已有的罚项（岭罚项、硬阈值罚项和LASSO罚项）都不能同时具有上述三种良好的性质。现在，我们给出Fan和Li（2001）所提出的非凸罚项，即SCAD（smoothly clipped absolute deviation）罚项，示意图如上图（c）所示，公式为\n$$ p _\\lambda(|\\theta|) = \\lambda\\left\\lbrace I(|\\theta| \\leq \\lambda) + \\frac{(a\\lambda - |\\theta|) _+}{(a - 1)\\lambda}I(|\\theta| \u0026gt; \\lambda)\\right\\rbrace $$\n其中 $a \u0026gt; 2$ 是某个既定的常数。Fan和Li（2001）通过数值方法计算基于平方损失的不同 $a$ 的取值对应的Bayes风险发现，当 $a = 3.7$ 时，Bayes风险较低且模型通常表现的较好。因此，Fan和Li（2001）建议一般取 $a = 3.7$。详细内容请参见论文的第5页，2.2小节上的段落讨论。根据SCAD的表达式，可以验证，SCAD罚项同时具有无偏性、稀疏性和连续性这三种性质。\n3 固定维下SCAD估计量的统计性质 本节我们讨论SCAD罚项得到的估计量的大样本性质。为表述方便，令 $\\beta _0 = (\\beta _{10}, \\ldots, \\beta _{p0})^T = (\\beta _0(1)^T, \\beta _0(2)^T)^T$ 为真实系数。不失一般性，假设前 $q$ 个系数 $\\beta _0(1)\\neq 0$，后 $p-q$ 个系数 $\\beta _0(2)=0$；再令 $I _1(\\beta _0(1), 0)$ 是已知 $\\beta _0(2)=0$ 时Fisher信息矩阵。记 $V _i = (X _i, Y _i), i = 1, \\ldots, n$。令 $L(\\beta)$ 为基于观测 $V _1, \\ldots, V _n$ 的对数似然函数（log-liklihood function），以及 $Q(\\beta) = L(\\beta) - n\\sum _{i=1}^p p _{\\lambda}(|\\beta _j|)$ 为惩罚似然函数。此外，我们还需给出以下和证明密切相关的正则条件（regular conditions）：\n【C1】观测 $V _i$ 之间相互独立且同分布于基于某个测度 $\\mu$、密度函数为 $f(V,\\beta)$ 的分布。$f(V,\\beta)$ 具有常规的支持（common support）并且可识别。此外，$f$ 的一阶、二阶对数导数满足\n$$ \\mathbb{E} _{\\beta} \\left( \\frac{\\partial \\log f(V, \\beta)}{\\partial \\beta _j}\\right) = 0, j = 1, \\ldots, p $$\n和\n$$ I _{jk}(\\beta) = \\mathbb{E} _{\\beta}\\left( \\frac{\\partial}{\\partial \\beta _j}\\log f(V, \\beta)\\frac{\\partial}{\\partial \\beta _k}\\log f(V, \\beta)\\right) = \\mathbb{E} _{\\beta}\\left( -\\frac{\\partial^2}{\\partial \\beta _j \\beta _k}\\log f(V, \\beta)\\right) $$\n【C2】Fisher信息矩阵\n$$ I(\\beta) = \\mathbb{E}\\left(\\left( \\frac{\\partial}{\\partial \\beta}\\log f(V, \\beta)\\right)\\left( \\frac{\\partial}{\\partial \\beta}\\log f(V, \\beta)\\right)^T\\right) $$\n是有限的（暗示最大、最小特征根有限）且在 $\\beta = \\beta _0$ 处正定。\n【C3】存在包含真是参数 $\\beta _0$ 的开集 $\\omega$ 使得，$\\forall \\beta\\in \\omega$，对几乎所有的 $V$ 对应的密度函数 $f(V, \\beta)$ 可以进行三阶求导。\n以上条件都是保证经典极大似然估计量渐近正态性的正则条件。\n基于以上表述，我们首先证明SCAD估计量具有 $\\sqrt{n}$ 一致性，这是LASSO估计所不具有的性质（参见我关于LASSO性质讨论的博客，3.3.2小节），具体结果如定理1所述。\n【定理1】假设C1-C3满足。若 $\\max\\lbrace |p _{\\lambda}''(|\\beta _{j0}|):\\beta _{j0}\\neq 0|\\rbrace$，则 $Q(\\beta)$ 存在一个局部最大值点 $\\hat{\\beta}$ 满足 $\\Vert \\hat{\\beta} - \\beta _0\\Vert = O _p(n^{-1/2} + a _n)$，其中 $a _n = \\max\\lbrace p _{\\lambda}'(|\\beta _{j0}|): \\beta _{j0}\\neq 0\\rbrace$。\n【证明】令 $\\alpha _n = n^{-1/2} + a _n$。为了证明定理1的结论，我们只需表明对任意给定的 $\\epsilon \u0026gt; 0$，存在一个大的常数 $C\u0026gt;0$ 使得\n$$ \\text{Pr}\\left\\lbrace \\sup _{\\Vert u\\Vert = C} Q(\\beta _0 + \\alpha _n u) \u0026lt; Q(\\beta _0)\\right\\rbrace \\geq 1 - \\epsilon \\qquad (3) $$\n成立。因为这表明至少以概率 $1 - \\epsilon$ 在球域 $\\lbrace \\beta _0 + \\alpha _n u: \\Vert u\\Vert \\leq C\\rbrace$ 中存在一个局部最大值点，也即存在一个局部最大值点使得 $\\Vert \\hat{\\beta} - \\beta _0\\Vert = O _p(n^{-1/2} + a _n)$。\n考虑到 $p _{\\lambda}(0) = 0$ 以及 $\\beta _0(2) = 0$，我们有\n$$ D _n (u) = Q(\\beta _0 + \\alpha _n u) - Q(\\beta _0) \\leq L(\\beta _0 + \\alpha _n u) - L(\\beta _0) - n\\sum _{j=1}^q (p _{\\lambda}(|\\beta _{j0} + \\alpha _n u _j|) - p _{\\lambda}(|\\beta _{j0}|)) $$\n令 $L\u0026rsquo;(\\beta _0)$ 是 $L$ 在 $\\beta = \\beta _0$ 处的一阶导数，则根据标准的泰勒展示有\n$$ \\begin{split} D _n (u) \\leq \u0026amp;\\alpha _n L\u0026rsquo;(\\beta _0) - \\frac 12u^TI(\\beta _0)u n\\alpha _n^2(1 + o _p(1))\\newline \u0026amp;- \\sum _{j=1}^q(n\\alpha _n p _{\\lambda}'(|\\beta _{j0}|)\\text{sgn}(\\beta _{j0})u _j + n\\alpha _n^2 p _{\\lambda}''(|\\beta _{j0}|u _j^2(1 + o _p(1)))) \\end{split} \\qquad (4) $$\n注意到 $n^{-1/2}L\u0026rsquo;(\\beta _0) = O _p(1)$。因此，式(4)不等式右侧的第一项的阶等于 $O _p(n^{1/2}\\alpha _n) = O _p(n\\alpha^2)$。选择充分大的常数 $C$, 式（4）不等式右侧的第二项将在 $\\Vert u\\Vert = C$ 下一致地控制第一项的大小。注意到，使用柯西不等式，式（4）不等式右侧的第三项可以由下面界定\n$$ \\sqrt{q} n \\alpha _n a _n \\Vert u\\Vert + n\\alpha^2 \\max\\lbrace |p _{\\lambda}''(|\\beta _{j0}|):\\beta _{j0}\\neq 0|\\rbrace \\Vert u\\Vert $$\n当 $n\\rightarrow \\infty$ 时，这同样被第二项控制大小。因此，$D _n(u) \u0026lt; 0, n\\rightarrow \\infty$，概率不等式（3）成立，即定理1的结论成立。$\\blacksquare$\n结合SCAD罚项公式，当 $\\lambda\\rightarrow 0, n\\rightarrow \\infty$ 时，存在某个 $N\u0026gt;0$，使得 $n \u0026gt; N$ 时满足 $\\lambda \u0026lt; \\min\\lbrace|\\beta _{j0}|:\\beta _{j0}\\neq 0\\rbrace$，我们始终有 $a _n = 0$。因此，SCAD估计量具有 $\\sqrt{n}$ 一致性。当然，这里有个小问题，即 $\\lambda$ 的衰减速率要快于 $\\min\\lbrace|\\beta _{j0}|:\\beta _{j0}\\neq 0\\rbrace$ 的衰减速率，这在Fan和Peng（2004）中有细致的分析。对于LASSO而言，$a _n = \\lambda$，这表明LASSO估计要想具有$\\sqrt{n}$ 一致性，$\\lambda$ 必须具有 ${n}^{-1/2}$ 的衰减速率。然而，这跟LASSO估计的变量选择一致性对 $\\lambda$ 的要求相冲突（参见我关于LASSO性质讨论的博客，3.3.2小节）。\nSCAD估计量具有 $\\sqrt{n}$ 速率的参数估计一致性，这并不能说明SCAD估计量具有稀疏性，我们用下面的引理1来阐明该结论。\n【引理1】假设C1-C3满足。若\n$$ \\liminf _{n\\rightarrow \\infty} \\liminf _{\\theta\\rightarrow 0+} p _{\\lambda}(\\theta)/\\lambda \u0026gt; 0 $$\n以及 $\\lambda\\rightarrow 0$ 和 $\\sqrt{n}\\lambda \\rightarrow \\infty$，则依概率1成立着，对任意给定满足 $\\Vert \\beta(1) - \\beta _0(1)\\Vert = O _p(n^{-1/2})$ 的 $\\beta(1)$ 和任意常数 $C$，有\n$$ Q\\left[ \\begin{pmatrix} \\beta(1) \\newline 0\\end{pmatrix}\\right] = \\max _{\\Vert \\beta(2)\\Vert \\leq Cn^{-1/2}} Q\\left[ \\begin{pmatrix} \\beta(1) \\newline \\beta(2)\\end{pmatrix}\\right]. $$\n【证明】引理1的一个充分条件可以是，依概率1成立着，对任意给定满足 $\\Vert \\beta(1) - \\beta _0(1)\\Vert = O _p(n^{-1/2})$ 的 $\\beta(1)$ 和一个小的 $\\epsilon _n = Cn^{-1/2}$，$j = q+1, \\ldots, p$，有\n$$ \\begin{split} \\frac{\\partial Q(\\beta)}{\\partial \\beta _j} \u0026amp;\u0026lt;0 \\quad \\text{for }0\u0026lt; \\beta _j\u0026lt; \\epsilon _n\\newline \u0026amp;\u0026gt; 0\\quad \\text{for }-\\epsilon _n\u0026lt; \\beta _j\u0026lt; 0 \\end{split}\\qquad \\begin{split} \u0026amp;(5.1)\\phantom{\\frac{\\partial Q(\\beta)}{\\partial \\beta _j}}\\newline \u0026amp;(5.2) \\end{split} $$\n为了表明式（5.1）的结论，利用泰勒展示，我们有\n$$ \\begin{split} \\frac{\\partial Q(\\beta)}{\\partial \\beta _j} \u0026amp;= \\frac{\\partial L(\\beta)}{\\partial \\beta _j} - np _{\\lambda}'(|\\beta _{j}|)\\text{sgn}(\\beta _j)\\newline \u0026amp;= \\frac{\\partial L(\\beta _0)}{\\partial \\beta _j} + \\sum _{l = 1}^p \\frac{\\partial^2 L(\\beta _0)}{\\partial \\beta _j\\beta _l}(\\beta _l - \\beta _{l0}) + \\sum _{l = 1}^p\\sum _{k = 1}^p \\frac{\\partial^3 L(\\beta^ *)}{\\partial \\beta _j\\beta _l\\beta _k}(\\beta _l - \\beta _{l0})(\\beta _k - \\beta _{l0}) - np _{\\lambda}'(|\\beta _{j}|)\\text{sgn}(\\beta _j) \\end{split} $$\n其中 $\\beta^ *$ 在 $\\beta$ 和 $\\beta _0$ 之间。注意到，根据标准的理论设定（standard arguments）有\n$$ n^{-1}\\frac{\\partial L(\\beta _0)}{\\partial \\beta _j} = O _p(n^{-1/2}) $$\n以及\n$$ \\frac 1n \\frac{\\partial^2 L(\\beta _0)}{\\partial \\beta _j\\beta _l} = \\mathbb{E}\\left( \\frac{\\partial^2 L(\\beta _0)}{\\partial \\beta _j\\beta _l}\\right) + o _p(1) $$\n根据假设 $\\beta - \\beta _0 = O _p(n^{-1/2})$，我们有\n$$ \\frac{\\partial Q(\\beta)}{\\partial \\beta _j} = n\\lambda(-\\lambda^{-1}p _{\\lambda}'(|\\beta _j|)\\text{sgn}(\\beta _j) + O _p(n^{-1/2}/\\lambda)) $$\n由于 $\\liminf _{n\\rightarrow \\infty} \\liminf _{\\theta\\rightarrow 0+} p _{\\lambda}(\\theta)/\\lambda \u0026gt; 0$ 和 $\\sqrt{n}\\lambda \\rightarrow \\infty$，导数的符号被 $\\beta _j$ 的符号所控制。因此，式（5.1）和（5.2）都成立。$\\blacksquare$\n【定理2】（Oracle性）假设C1-C3满足，罚项 $p _{\\lambda}(|\\theta|)$ 满足 $\\liminf _{n\\rightarrow \\infty} \\liminf _{\\theta\\rightarrow 0+} p _{\\lambda}(\\theta)/\\lambda \u0026gt; 0$。若$\\lambda\\rightarrow 0$ 和 $\\sqrt{n}\\lambda \\rightarrow \\infty$，则依概率1成立着，定理1中的 $\\sqrt{n}$ 一致的估计量 $\\hat{\\beta} = (\\hat{\\beta}(1)^T, \\hat{\\beta}(2)^T)^T$一定满足\n 稀疏性：$\\hat{\\beta}(2) = 0$。 渐近正态性：$\\sqrt{n}(I _1(\\beta _0(1)) + \\Sigma)(\\hat{\\beta}(1) - \\beta _0(1) + (I _1(\\beta _0(1)) + \\Sigma)^{-1}b) \\rightarrow _L N(0, I _1(\\beta _0(1)))$。  其中 $\\Sigma = diag(p _{\\lambda}''(|\\beta _{10}|), \\ldots, p _{\\lambda}''(|\\beta _{q0}|))$，$b = (p _{\\lambda}'(|\\beta _{10}|)\\text{sgn}(\\beta _{10}),\\ldots, p _{\\lambda}'(|\\beta _{q0}|)\\text{sgn}(\\beta _{q0}))$，$I _1(\\beta _0(1)) = I _1(\\beta _0(1), 0)$。\n【证明】根据引理1，结论1成立。现证明结论2。根据定理1，存在 $\\hat{\\beta}(1)$ 是 $Q(\\beta(1), 0)$ 的 $\\sqrt{n}$ 一致的估计量，自然满足\n$$ \\left.\\frac{\\partial Q(\\beta)}{\\partial \\beta _j} \\right\\vert _{\\beta = (\\hat{\\beta}(1)^T, 0^T)^T} = 0, j = 1, \\ldots, q $$\n注意到 $\\hat{\\beta}(1)$ 是一致的估计量，则\n$$ \\begin{split} \u0026amp;\\left.\\frac{\\partial L(\\beta)}{\\partial \\beta _j} \\right\\vert _{\\beta = (\\hat{\\beta}(1)^T, 0^T)^T} - np _{\\lambda}'(|\\beta _{j}|)\\text{sgn}(\\beta _j)\\newline\n=\u0026amp;\\frac{\\partial L(\\beta _0)}{\\partial \\beta _j} + \\sum _{l=1}^q \\left( \\frac{\\partial^2 L(\\beta _0)}{\\partial \\beta _j\\beta _l} + o _p(1) \\right)(\\hat{\\beta} _l - \\beta _{l0}) - n[p _{\\lambda}'(|\\beta _{j0}|)\\text{sgn}(\\beta _{j0}) + (p _{\\lambda}''(|\\beta _{j0}|) + o _p(1))(\\hat{\\beta} _j - \\beta _{j0})] \\end{split} $$\n再根据Slutsky定理即可得结论。$\\blacksquare$\nSCAD估计量的Oracle性表明，如同事先知道模型的稀疏结构进行估计一般。这个性质LASSO不具备，这是因为证明中要求非零系数的估计具有$\\sqrt{n}$ 一致性，但LASSO估计不具备。Oracle性表明，相较于很多罚项，SCAD罚项具有独特的优点。\n4 结语 本文介绍了SCAD惩罚的基本思想，并探讨了固定维下的大样本性质。目前，非凸罚项的研究十分丰富且火热，了解经典的SCAD理论对学习相关的非负惩罚有着重要的帮助。关于SCAD估计在发散维下的大样本性质可参考Fan和Peng（2004），关于SCAD估计在高维下的大样本性质可参考Kim等（2008）。\n","id":0,"section":"posts","summary":"变量选择是高维统计建模中的重要内容。自1996年Tibshirani提出LASSO后，基于正则化模型的变量选择研究日益壮大、独领风潮。LAS","tags":["多元统计","变量选择","高维数据分析"],"title":"非凸惩罚之SCAD简介","uri":"https://qkai-stat.github.io/2022/09/scad/","year":"2022"},{"content":"传统的支持向量机模型（support vector machine，SVM）可以适用统计中的正则化模型框架，即损失+罚项的结构，其中SVM采用的是hinge损失函数+$l_2$ 罚项。然而，$l_2$ 罚项没有稀疏能力，这使得传统SVM在层见迭出的高维数据分析中应用受限。随着统计（和优化）领域中LASSO等具有变量选择能力的罚项的研究发展，一些学者提出稀疏SVM并讨论了估计量的相关性质。本文主要介绍了中度高维（moderately high dimensions）下非凸惩罚线性SVM的变量选择一致性，主要内容来源于对Xiang等（2016）的翻译。\n1 问题的引入 考虑二分类问题，令 $\\lbrace(X_i, Y_i)\\rbrace_{i=1}^n$ 是从未知总体分布 $P(X, Y)$ 中随机抽取的样本，其中 $Y_i\\in {-1, +1}$ 是标签，$X_i = (X_{i0}, X_{i1}, \\ldots, X_{ip})^T = (1, (X_i^ *)^T)^T$ 是协变量。那么，传统的线性SVM（linear SVM）可以写成如下的优化问题：\n$$ \\min_{\\beta} \\frac 1n \\sum_{i=1}W_i(1 - Y_iX_i^T\\beta)_+ + \\lambda(\\beta ^ *)^T\\beta ^ * $$\n其中 $\\beta = (\\beta_0, \\beta_1, \\ldots, \\beta_p)^T = (\\beta_0, (\\beta ^ *)^T)^T$ 是分类超平面的法向量（首个元素为截距项），$(1 - u)_+ = \\max{1 - u, 0}$ 是hinge损失函数，$\\lambda \u0026gt; 0$ 是正则化参数。注意到，上式与经典SVM相比多了 $W_i$，它具体的取值如下\n$$ W_i = \\begin{cases} w \u0026amp; Y_i = 1\\\n1-w \u0026amp; Y_i = -1 \\end{cases}, \\quad 0 \u0026lt; w \u0026lt; 1 $$\n是用来控制不同误分类成本而引入的权重。相较于经典SVM，这里考虑的模型形式更具一般性。\n为了研究线性加权SVM的变量选择问题，我们引入总体加权（population weighted）hinge损失 $\\mathbb{E}\\lbrace W(1 - YX^T\\beta)_+\\rbrace$。令 $\\beta_0 = (\\beta _{00}, \\beta _{01}, \\ldots, \\beta _{0p})^T = (\\beta _{00}, (\\beta _0^* )^T)^T$ 为真实参数，其定义为如下总体加权hinge损失的最小值：\n$$ \\beta_0 = arg\\min_{\\beta} \\mathbb{E}\\lbrace W(1 - YX^T\\beta)_+\\rbrace \\qquad (1) $$\n 注意到，hinge损失函数是分段线性，其中 $u\u0026gt;1$ 的部分为0，这表明最小值 $\\beta_0$ 不唯一。这种不唯一性对后续的分析有很大影响，故作者在此假定总体加权hinge损失的最小值唯一，类似的假定还出现在Koo等（2008）中。\n 我们假定 $p=p_n$ 可以随着 $n$ 增大而增大，且可以比 $n$ 大（即高维问题）。进一步，假定 $\\beta_0$ 是稀疏的，令 $A = \\lbrace 1\\leq j\\leq p: \\beta_{0j} \\neq 0\\rbrace$ 为非零系数的下标集。令 $q = |A|$ 为非零系数的个数，且可以随 $n$ 增大而增大。不失一般性，我们假定后系数的后 $p-q$ 个元素为0，即 $\\beta_0 = (\\beta_{01}^T, \\mathbf{0}^T)^T$。相应地，我们令 $X_i^T = (Z_i^T, R_i^T)$，其中 $Z_i = (X_{i0}, X_{i1}, \\ldots, X_{iq})^T = (1, (Z_i^ *)^T)$ 和 $R_i = (X_{i[q+1]}, \\ldots, X_{ip})^T$。进一步，我们用 $\\pi_+$ 和 $\\pi_-$ 来表示标签 $Y=1$ 和 $Y=-1$ 的边际概率。\n如果我们事先知道模型的稀疏结构，则可以定义Oracle估计量 $\\hat{\\beta} = (\\hat{\\beta}_1^T, \\mathbf{0}^T)^T$， 其中\n$$ \\hat{\\beta}_1 = arg\\min _{\\beta _1} \\frac 1n \\sum _{i=1}^n W_i(1 - Y_i Z_i^T \\beta _1) _+ \\qquad (2) $$\n注意到，当 $n\\rightarrow \\infty$ 时，$\\hat{\\beta} _1$ 最小化总体加权hinge损失 $\\mathbb{E}\\lbrace W(1 - YX^T\\beta) _+\\rbrace$。\n然而，模型的稀疏结构通常难以事先获知，故我们结合非凸罚项提出如下具有变量选择能力的非凸惩罚线性SVM模型：\n$$ \\min_{\\beta} Q(\\beta) = \\min_{\\beta} \\frac 1n \\sum _{i=1} W_i(1 - Y_i X_i^T\\beta) _+ + \\sum _{i=1}^{p _n} p _{{\\lambda_n}}(\\vert \\beta_j \\vert) \\qquad (3) $$\n其中 $\\lambda_n$（通常简记为 $\\lambda$）为调节参数（tuning parameter），$p_{{\\lambda_n}}(\\cdot)$ 是非凸罚项，满足以下两个假设：\n【假设1】对称函数 $p_{\\lambda}(t)$ 在 $t\\in [0, +\\infty)$ 上是非减和凹的，且 $p_{\\lambda}'(t)$ 在 $(0, \\infty)$ 上连续且满足 $p_{\\lambda}(0) = 0$。\n【假设2】存在 $a\u0026gt;1$ 满足 $\\lim_{t\\rightarrow 0+}p_{\\lambda}'(t) = \\lambda$，$p_{\\lambda}'(t) \\geq \\lambda - t/a$ 对 $0 \u0026lt; t \u0026lt; a\\lambda$ 成立，且 $p_{\\lambda_n}'(t) = 0$ 对 $t \\geq a\\lambda$ 成立。\n在后续的讨论中，我们主要考虑SCAD罚项（Fan and Li（2001））来进行相关理论分析，其他满足上述两个假设的非凸罚项，如MCP（Zhang（2008）），相关分析类似可参找原文Xiang等（2016）。SCAD罚项的表达式如下\n$$ p_{\\lambda}(|\\beta|) = \\lambda |\\beta| I(0\\leq \\beta\u0026lt; \\lambda) + \\frac{a\\lambda |\\beta| - (\\beta^2 + \\lambda^2)/2}{a-1}I(\\lambda \\leq |\\beta| \\leq a\\lambda) + \\frac{(a+1)\\lambda^2}{2}I(|\\beta|\u0026gt;a\\lambda) $$\n其中 $I(\\cdot)$ 是示性函数，$a \u0026gt; 2$ 为某个既定参数，通常取 $a = 3.7$ （Fan and Li （2001））。\n2 若干记号与假设 为了后续的表述和证明方便，我们在本小节给出一些重要的记号和前提假设，后续的证明都是基于这些假设推导的。\n首先，我们给出总体hinge损失函数的梯度向量（gradient vector）和Hessian矩阵。令 $L(\\beta_1) = \\mathbb{E}\\lbrace W(1 - YZ^T\\beta_1)_+\\rbrace$ 为只含有重要变量的线性加权hinge损失。定义 $S(\\beta_1) = (S(\\beta_1)_j)$ 是 $(q+1)$-维向量，即\n$$ S(\\beta_1) = -\\mathbb{E}\\lbrace I(1 - YX^T\\beta \\geq 0)WYZ\\rbrace $$\n同时定义 $H(\\beta_1) = (H(\\beta_1)_{jk})$ 为 $(q+1)\\times(q+1)$ 的矩阵，即\n$$ H(\\beta_1) = \\mathbb{E}\\lbrace \\delta(1 - YZ^T\\beta_1)WZZ^T\\rbrace $$\n其中 $\\delta(\\cdot)$ 是Dirac函数。可以表明，$S(\\beta_1)$ 和 $H(\\beta_1)$ 分别是 $L(\\beta_1)$ 的梯度向量和Hessian矩阵，具体参见Koo等（2008）中的引理2相关内容。\n然后，我们给出证明所需的正则化条件（regular conditions）：\n【C1】给定 $Y=1$ 或 $Y=-1$ 时 $Z^ *$ 的密度连续且在 $\\mathbb{R}^q$ 上有常规的支持。\n【C2】$\\mathbb{E}(X_j^2) \u0026lt; \\infty, 1\\leq j\\leq q$。\n【C3】真实参数 $\\beta_0$ 唯一且非零。\n【C4】$q = O(n^{c_1})$，对某个 $0\\leq c_1 \u0026lt;\\frac 12$ 成立。\n【C5】存在 $M_1 \u0026gt; 0$ 使得 $\\lambda_{\\max}(n^{-1}X_A^TX_A)\\leq M_1$，其中 $\\lambda_{\\max}(\\cdot)$ 表示最大特征值。进一步假设 $\\max_{1\\leq j\\leq n}\\Vert Z_i\\Vert = O_p(\\sqrt{q}\\log (n))$；$X_{ij}$ 为sub-Gaussian随机变量，$1\\leq i\\leq n, q+1\\leq j\\leq p$。\n【C6】存在 $M_2 \u0026gt; 0$ 使得 $\\lambda_{\\min}(H(\\beta_{01}))\\geq M_2$，其中 $\\lambda_{\\min}(\\cdot)$ 表示最小特征值。\n【C7】存在 $M_3 \u0026gt; 0$ 使得 $n^{(1-c_2)/2} \\min_{1\\leq j\\leq q}|\\beta_{0j}| \\geq M_3$，$2c_1\\leq c_2\\leq 1$。\n【C8】定义给定 $Y=1$ 或 $Y=-1$ 时 $Z^T\\beta_{01}$ 的条件密度函数分别为 $f$ 和 $g$。假定 $f$ 一致地远离 0 和 $\\infty$ 且在 1 附近，$g$ 一致地远离 0 和 $\\infty$ 且在 -1 附近。\nC1-C3和C6在Koo等（2008）中关于固定维度 $p$ 的讨论中也做相同假设，我们需要这些假设来保证oracle估计量在发散维下是一致的。C3表明最优决策函数非常数，这需要 $S(\\beta)$ 和 $H(\\beta)$ 良好定义。C4和C7是高维分析中常见的假设（Kim等（2008））。具体而言，C4表明非零系数的个数的增长速率不能超过 $\\sqrt{n}$，C7表明信号不能衰退地太快。C5类似于Zhang和Huang（2008）中的Riesz条件。注意到，这里没有对最小特征值做要求。C8表明在hinge损失函数的不可导点处要有足够的信息，这和Wang等（2012）关于quantile回归的条件（C5）类似。\n3 局部Oracle性质 本节我们建立非凸惩罚SVM的局部Oracle性质的理论，这里的局部Oracle性质意为式（2）定义的Oracle估计量 $\\hat{\\beta}$ 是非凸惩罚SVM问题（3）中 $Q(\\beta)$ 的一个局部最优解。在给出理论结果之前，我们大致勾勒一下整个证明的思路。首先，\n3.1 Oracle估计量的估计一致性 在Koo等（2008）中已经讨论过固定维下Oracle估计量的估计一致性，这里将其推广到发散维的情形，具体如引理1所述：\n【引理1】若C1-C7都满足，则Oracle估计量 $\\hat{\\beta} = (\\hat{\\beta} _1^T, \\mathbf{0}^T)^T$ 满足 $\\Vert \\hat{\\beta} _1 - \\beta _{01}\\Vert = O_p \\lbrace \\sqrt{q/n}\\rbrace, n\\rightarrow \\infty$。\n【证明】令 $l(\\beta _1) = n^{-1}\\sum _{i=1}^n W_i (1 - Y_i Z_i^T\\beta _1) _+$，易知 $\\hat{\\beta}_1 = arg\\min _{\\beta _1}l(\\beta _1)$。为了证明引理1的结论，我们只需证明：$\\forall \\eta\u0026gt;0$，存在常数 $\\Delta$ 使得对充分大的 $n$，$\\text{Pr}[\\inf _{\\Vert u\\Vert = \\Delta}l(\\beta _{01} + \\sqrt{q/n}) \u0026gt; l(\\beta _{01})] \\geq 1 - \\eta$ 成立。\n 若上面的概率不等式成立，由于 $l(\\beta_1)$ 是凸函数，则易推得至少以概率 $1-\\eta$，$\\hat{\\beta} _1$ 在球 $\\lbrace \\beta _1: \\Vert \\beta_1 - \\beta _{01}\\Vert \\leq \\Delta\\sqrt{q/n}\\rbrace$ 中，也即证明了引理1的结论。这个推理根据函数的性质即能得到：假设 $\\hat{\\beta}_1$ 在球外，定义 $\\hat{\\beta} _1$ 和 $\\beta _{01}$ 的连线上的一点 $\\tilde{\\beta} _1 = s_1 \\hat{\\beta} _1 + s_2 \\beta _{01}$，其中 $s_1 + s_2 = 1$。根据凸函数性质，$l(\\tilde{\\beta}_1) = l(s_1 \\hat{\\beta}_1 + s_2 \\beta _{01}) \\leq s_1 l(\\hat{\\beta}_1) + s_2 l(\\beta _{01}) \\leq l(\\beta _{01})$，因为 $\\hat{\\beta}_1$ 是 $l(\\beta_1)$ 最优值点。由于 $\\hat{\\beta}_1$ 在球外，根据上面的概率不等式有 $l(\\tilde{\\beta} _1) \u0026gt; l(\\beta _{01})$成立。矛盾，故假设不成立。\n 令 $\\Lambda_n(u) = nq^{-1}[l(\\beta_{01} + \\sqrt{q/n}u) - l(\\beta_{01})]$，并注意到 $\\mathbb{E}(\\Lambda_n(u)) = nq^{-1}[L(\\beta_{01} + \\sqrt{q/n}u) - L(\\beta_{01})]$。由于 $\\beta_0 = arg\\min_{\\beta}\\mathbb{E}\\lbrace W(1-YX^T\\beta)_+\\rbrace$，如果按前述将系数的后 $p-q$ 个元素令为零，则\n$$ \\beta _{01} = arg\\min _{\\beta _1}\\mathbb{E}\\lbrace W(1-YZ^T\\beta _1) _+\\rbrace = arg\\min _{\\beta _1} L(\\beta _1) $$\n由最优性条件，可知 $S(\\beta_{01}) = 0$。那么，根据$L(\\beta_1)$ 在 $\\beta_{01}$ 附近的Taylor展式可知\n$$ \\mathbb{E}(\\Lambda_n(u)) = \\frac12 u^T H(\\tilde{\\beta})u + o_p(1) $$\n其中 $\\tilde{\\beta} = \\beta_{01} + \\sqrt{q/n}tu, 0 \u0026lt; t \u0026lt; 1$。如Koo等（2008）所述，给定C1和C2，$H(\\beta_{01})$ 的第 $(j, k)$ 元素连续，故 $H(\\beta)$ 连续。根据 $H(\\beta)$ 在 $\\beta_{01}$ 处的连续性可知 $\\frac12 u^T H(\\tilde{\\beta})u = \\frac12 u^T H(\\beta_{01})u + o_p(1)$。\n定义 $W_n = -\\sum_{i=1}^n \\zeta_iW_iY_iZ_i$，其中 $\\zeta_i = I(1 - Y_iZ_i^T\\beta_{01})$。注意到，$S(\\beta_{01}) = -\\mathbb{E}(\\zeta_iW_iY_iZ_i) = 0$。再定义\n$$ R_{i,n}(u) = W_i(1 - Y_i Z_i^T(\\beta_{01} + \\sqrt{q/n}u)) _+ - W_i(1 - Y_i Z_i^T\\beta _{01}) _+ + \\zeta_iW_iY_iZ_i^T\\sqrt{q/n}u $$\n则 $\\mathbb{E}(R_{i,n}(u)) = L(\\beta_{01} + \\sqrt{q/n}u) - L(\\beta_{01})$，则\n$$ \\Lambda_n(u) = \\mathbb{E}(\\Lambda_n(u)) + \\frac{W_n^Tu}{\\sqrt{q/n}} + \\frac 1q\\sum_{i=1}^n[R_{i,n}(u) - \\mathbb{E}(R_{i,n}(u))] \\qquad (4) $$\n与Koo等（2008）中的公式（28）类似，我们有\n$$ \\frac{1}{q^2}\\sum_{i=1}^n[|R_{i,n}(u) - \\mathbb{E}(R_{i,n}(u))|^2] \\leq C \\Delta^2 \\mathbb{E}[q^{-1}(1 + \\Vert Z\\Vert^2)U\\lbrace \\sqrt{1 + \\Vert Z\\Vert^2}\\Delta \\sqrt{q/n}\\rbrace] $$\n其中 $U(t) = I(|1 - Y_1Z_i^T\\beta_{01}| \u0026lt; t)$。C2表明 $\\mathbb{E}[q^{-1}(1 + \\Vert Z\\Vert^2)] \u0026lt;\\infty$。这样，$\\forall \\epsilon \u0026gt; 0$，我们总可以选择一个常数 $C$ 使得 $\\mathbb{E}[q^{-1}(1 + \\Vert Z\\Vert^2)I\\lbrace q^{-1}(1 + \\Vert Z\\Vert^2) \u0026gt; C\\rbrace] \u0026lt; \\epsilon/2$；则\n$$ \\begin{split} \\mathbb{E}[q^{-1}(1 + \\Vert Z\\Vert^2)U\\lbrace \\sqrt{1 + \\Vert Z\\Vert^2}\\Delta \\sqrt{q/n}\\rbrace] \\leq \u0026amp;\\mathbb{E}[q^{-1}(1 + \\Vert Z\\Vert^2)I\\lbrace q^{-1}(1 + \\Vert Z\\Vert^2) \u0026gt; C\\rbrace]\\newline \u0026amp;+ C\\text{Pr}\\lbrace |1 - Y_1Z_i^T\\beta_{01}| \u0026lt; C\\Delta \\sqrt{q/n}\\rbrace \\end{split} $$\n根据C4，我们可以找到一个充分大的 $N\u0026gt;0$ 使得 $n \u0026gt; N$ 时 $\\text{Pr}\\lbrace |1 - Y_1Z_i^T\\beta_{01}| \u0026lt; C\\Delta \\sqrt{q/n} \u0026lt; \\epsilon/(2C)$ 成立。这就证明了 $\\lim_{n\\rightarrow \\infty}q^{-2}\\sum_{i=1}^n[|R_{i,n}(u) - \\mathbb{E}(R_{i,n}(u))|^2] = 0$。\n注意到 $\\mathbb{E}(W_n^Tu/\\sqrt{q/n}) = 0$ 且\n$$ \\text{Var}(W_n^Tu/\\sqrt{q/n}) \\leq Cn^{-1}q^{-1}\\sum_{i=1}^n (Z_i^Tu)^2 \\leq Cq^{-1}\\lambda_{\\max}(n^{-1}X_A^TX_A)\\Vert u\\Vert \\rightarrow 0, \\quad n\\rightarrow \\infty $$\n因此，当 $n\\rightarrow \\infty$ 时，公式（4）中的第一项将控制其他项。根据C6，我们有 $\\frac12 u^T H(\\beta_{01})u \u0026gt;0$。因此，我们可以选择充分大的 $\\Delta \u0026gt; 0$，使得对 $\\Vert u\\Vert = \\Delta$ 和充分大的 $n$，$\\Lambda_n(u) \u0026gt; 0$ 成立。$\\blacksquare$\n3.2 Hinge损失在Oracle估计处的次梯度 本小节我们分析Hinge损失在Oracle估计处的次梯度（sub-gradient）的渐近表现，为证明局部Oracle性做铺垫。令 $s(\\hat{\\beta}) = (s_0(\\hat{\\beta}), \\ldots, s_p(\\hat{\\beta}))^T$ 为hinge损失在Oracle估计处的次梯度，则\n$$ s_j(\\hat{\\beta}) = -\\frac 1n \\sum_{i=1}^n W_iY_iX_{ij}I(1 - Y_iX_i^T\\hat{\\beta} \u0026gt; 0) - \\frac 1n \\sum_{i=1}^n W_iY_iX_{ij}v_i \\qquad j = 1, \\ldots, p $$\n其中\n$$ v_i = \\begin{cases} [-1, 0] \u0026amp; 1 - Y_iX_i^T\\hat{\\beta} = 0\\newline 0 \u0026amp; \\text{otherwise} \\end{cases} $$\n在一些正则条件下，我们可以研究在Oracle估计处的次梯度的渐近表现，结果如定理1所述。\n【定理1】若C1-C8成立，且调节参数满足 $\\lambda = o(n^{-(1 - c_2)/2})$ 以及 $\\log(p)q\\log(n)n^{-1/2} = o(\\lambda)$，对于Oracle估计 $\\hat{\\beta}$，存在 $v_i^ *$ 满足\n$$ v_i^ * = \\begin{cases} [-1, 0] \u0026amp; 1 - Y_iX_i^T\\hat{\\beta} = 0\\newline 0 \u0026amp; \\text{otherwise} \\end{cases} $$\n使得对于 $v_i = v_i^ *$ 成立的 $s_j(\\hat{\\beta})$，我们能够以概率趋近1得到\n$$ \\begin{cases} s_j(\\hat{\\beta}) = 0 \u0026amp; j = 1, \\ldots, q;\\newline |\\hat{\\beta}_j|\\geq (a + \\frac 12)\\lambda \u0026amp; j = 1, \\ldots, q;\\newline |s_j(\\hat{\\beta})| \\leq \\lambda \\text{ and } |\\hat{\\beta}_j| = 0 \u0026amp; j = q+1, \\ldots, p. \\end{cases} $$\n【证明】为了证明定理1，我们首先不加证明的给出高维统计分析中常用的Bernstein不等式：\n【Bernstein不等式】假设 $\\lbrace X_i\\rbrace_{i=1}^n$ 是一些列零均值、独立、上界为 $K\u0026gt;0$ 的亚指数（sub-exponential）随机变量，则 $\\forall t \u0026gt; 0$，我们有\n$$ \\text{Pr}\\left(\\left\\vert\\sum_{i=1}^n X_i \\right\\vert \\geq t\\right) \\leq 2\\exp\\left( -\\frac{t^2/2}{\\sigma^2 + Kt/3}\\right) $$\n其中 $\\sigma^2 = \\sum_{i=1}^n \\mathbb{E}X_i^2$。这是Bernstein不等式应用最为广泛的版本。\n接下来，我们给出两个与定理1证明相关的引理，并分析详细的推导过程。\n【引理2】若调节参数 $\\lambda$ 满足定理1中的假设，则\n$$ \\text{Pr}\\left\\lbrace \\max_{q+1\\leq j\\leq p} \\frac 1n \\left\\vert \\sum_{i=1}^n W_iY_iX_{ij} I(1 - Y_iZ_i^T\\beta_{01} \\geq 0)\\right\\vert \u0026gt; \\frac{\\lambda}{2}\\right\\rbrace \\rightarrow 0, \\quad n\\rightarrow \\infty. $$\n【证明】如前述 $\\mathbb{E}(W_iY_iX_{ij} I(1 - Y_iZ_i^T\\beta_{01})) = 0$。根据C5，$X_{ij}, q+1\\leq j\\leq p$ 是sub-Gaussian随机变量，则 $\\text{Var}(W_iY_iX_{ij} I(1 - Y_iZ_i^T\\beta_{01}))$ 有上界（Rivasplata，2012），所以存在常数 $C_1, C_2 \u0026gt; 0$，根据Bernstein不等式有\n$$ \\text{Pr}\\left\\lbrace \\left\\vert \\sum_{i=1}^n W_iY_iX_{ij} I(1 - Y_iZ_i^T\\beta_{01} \\geq 0)\\right\\vert \u0026gt; \\frac{\\lambda n}{2}\\right\\rbrace \\leq 2\\exp\\left( -\\frac{n\\lambda^2}{C_1 + C_2\\lambda}\\right) = O(\\exp(-n\\lambda)) $$\n注意到\n$$ \\begin{split} \u0026amp;\\text{Pr}\\left\\lbrace \\max_{q+1\\leq j\\leq p} \\frac 1n \\left\\vert \\sum_{i=1}^n W_iY_iX_{ij} I(1 - Y_iZ_i^T\\beta_{01} \\geq 0)\\right\\vert \u0026gt; \\frac{\\lambda}{2}\\right\\rbrace\\newline = \u0026amp;\\text{Pr}\\left\\lbrace \\bigcup_{q+1\\leq j\\leq p} \\left\\lbrace\\frac 1n \\left\\vert \\sum_{i=1}^n W_iY_iX_{ij} I(1 - Y_iZ_i^T\\beta_{01} \\geq 0)\\right\\vert \u0026gt; \\frac{\\lambda}{2}\\right\\rbrace \\right\\rbrace = pO(\\exp(-n\\lambda))\\newline = \u0026amp;O(p\\exp(-n\\lambda)) = o(1) \\end{split} $$\n这是因为根据定理1中关于调节参数的假设满足时，我们有\n$$ \\frac{\\log(p)}{n\\lambda} = \\frac{\\log(p)q\\log(n)n^{-1/2}}{\\lambda}\\cdot\\frac{1}{q\\log(n)n^{1/2}} = o(1) $$\n从而证明了引理1的结论。$\\square$\n【引理3】对任意的 $\\Delta \u0026gt; 0$，若调节参数 $\\lambda$ 满足定理1中的假设，则 $$ \\begin{split} \u0026amp;\\text{Pr}\\left\\lbrace \\max_{q+1\\leq j\\leq p} \\sup_{\\Vert \\beta_1 - \\beta_{01}\\Vert\\leq \\Delta\\sqrt{q/n}} \\left\\vert \\sum_{i=1}^n W_iY_iX_{ij}\\cdot[I(1 - Y_iZ_i^T\\beta_1\\geq 0) - I(1 - Y_iZ_i^T\\beta_{01}\\geq 0)\\right.\\right.\\newline \u0026amp;\\left.\\left. \\phantom{\\max_{q+1\\leq j\\leq p} \\sup_{\\Vert \\beta_1 - \\beta_{01}\\Vert\\leq \\Delta\\sqrt{q/n}} \\sum_{i=1}^n W_iY_iX_{ij}}-\\text{Pr}\\lbrace 1 - Y_iZ_i^T\\beta_1\u0026gt;0\\rbrace + \\text{Pr}\\lbrace 1 - Y_iZ_i^T\\beta_{01} \u0026gt; 0\\rbrace]\\right\\vert \u0026gt; n\\lambda\\right\\rbrace\\rightarrow 0, \\quad n\\rightarrow \\infty. \\end{split} $$\n【证明】我们推广Welsh（1989）中定理3.1的证明方法。我们用一个半径为 $\\Delta\\sqrt{q/n^5}$ 的球构成的球网来覆盖球 $\\lbrace \\beta_1: \\Vert \\beta_1 - \\beta_{01}\\Vert \\leq \\Delta\\sqrt{q/n}\\rbrace$，可以按照下面的思路来构建球网：\n因此，球网中的小球个数满足 $N \\leq (2n^2)^q$。令这 $N$ 个小球分别为 $B(t_1), B(t_2), \\ldots, B(t_N)$，其中 $t_1, t_2, \\ldots, t_N$ 是对应的球心。令 $\\kappa_i (\\beta _1) = 1 - Y_i Z_i^T\\beta _1$，以及\n$$ \\begin{split} J _{nj _1} \u0026amp;= \\sum _{k=1}^N \\text{Pr}\\left\\lbrace\\left\\vert \\sum _{i=1}^n W_i Y_i X _{ij}[I(\\kappa _i (t _k)\\geq 0) - I(\\kappa _i(\\beta _{01})\\geq 0) - \\text{Pr}\\lbrace \\kappa_i(t _k)\\geq 0\\rbrace + \\text{Pr}\\lbrace \\kappa_i(\\beta _{01})\\geq 0\\rbrace]\\right\\vert \\geq \\frac{n\\lambda}{2}\\right\\rbrace\\newline J _{nj _2} \u0026amp;= \\sum _{k=1}^N \\text{Pr}\\left\\lbrace \\sup _{\\tilde{\\beta} _1\\in B(t _k)}\\left\\vert \\sum _{i=1}^n W_i Y_i X _{ij}[I(\\kappa _i(\\tilde{\\beta} _1)\\geq 0) - I(\\kappa_i(t _k)\\geq 0) - \\text{Pr}\\lbrace \\kappa_i(\\tilde{\\beta} _1)\\geq 0\\rbrace + \\text{Pr}\\lbrace \\kappa_i(t _k)\\geq 0\\rbrace]\\right\\vert \\geq \\frac{n\\lambda}{2}\\right\\rbrace \\end{split} $$\n一方面\n$$ \\begin{split} \u0026amp;\\text{Pr}\\left\\lbrace \\sup _{\\Vert \\beta _1 - \\beta _{01}\\Vert\\leq \\Delta\\sqrt{q/n}} \\left\\vert \\sum _{i=1}^n W _iY _iX _{ij}\\cdot[I(\\kappa _i(\\beta _1)\\geq 0) - I(\\kappa _i(\\beta _{01})\\geq 0)-\\text{Pr}\\lbrace \\kappa _i(\\beta _1)\u0026gt;0\\rbrace + \\text{Pr}\\lbrace \\kappa _i(\\beta _{01}) \u0026gt; 0\\rbrace]\\right\\vert \u0026gt; n\\lambda\\right\\rbrace\\newline =\u0026amp;\\text{Pr}\\left\\lbrace \\bigcup _{\\Vert \\beta _1 - \\beta _{01}\\Vert\\leq \\Delta\\sqrt{q/n}} \\left\\vert \\sum _{i=1}^n W _iY _iX _{ij}\\cdot[I(\\kappa _i(\\beta _1)\\geq 0) - I(\\kappa _i(\\beta _{01})\\geq 0)-\\text{Pr}\\lbrace \\kappa _i(\\beta _1)\u0026gt;0\\rbrace + \\text{Pr}\\lbrace \\kappa _i(\\beta _{01}) \u0026gt; 0\\rbrace]\\right\\vert \u0026gt; n\\lambda\\right\\rbrace\\newline \\leq\u0026amp;\\text{Pr}\\left\\lbrace \\bigcup _{\\tilde{\\beta} _1\\in B(t _k), k=1, \\ldots, N} \\left\\vert \\sum _{i=1}^n W _iY _iX _{ij}\\cdot[I(\\kappa _i(\\beta _1)\\geq 0) - I(\\kappa _i(\\beta _{01})\\geq 0)-\\text{Pr}\\lbrace \\kappa _i(\\beta _1)\u0026gt;0\\rbrace + \\text{Pr}\\lbrace \\kappa _i(\\beta _{01}) \u0026gt; 0\\rbrace]\\right\\vert \u0026gt; n\\lambda\\right\\rbrace\\newline \\leq\u0026amp;\\sum _{k=1}^N \\text{Pr}\\left\\lbrace \\sup _{\\tilde{\\beta} _1\\in B(t _k)} \\left\\vert \\sum _{i=1}^n W _iY _iX _{ij}\\cdot[I(\\kappa _i(\\tilde{\\beta} _1)\\geq 0) - I(\\kappa _i(\\beta _{01})\\geq 0)-\\text{Pr}\\lbrace \\kappa _i(\\tilde{\\beta} _1)\u0026gt;0\\rbrace + \\text{Pr}\\lbrace \\kappa _i(\\beta _{01}) \u0026gt; 0\\rbrace]\\right\\vert \u0026gt; n\\lambda\\right\\rbrace \\end{split} $$\n另一方面\n$$ \\begin{split} \u0026amp;\\sup _{\\tilde{\\beta} _1\\in B(t _k)}\\left\\vert \\sum _{i=1}^n W _iY _iX _{ij}\\cdot[I(\\kappa _i(\\tilde{\\beta} _1)\\geq 0) - I(\\kappa _i(\\beta _{01})\\geq 0)-\\text{Pr}\\lbrace \\kappa _i(\\tilde{\\beta} _1)\u0026gt;0\\rbrace + \\text{Pr}\\lbrace \\kappa _i(\\beta _{01}) \u0026gt; 0\\rbrace]\\right\\vert\\newline =\u0026amp;\\sup _{\\tilde{\\beta} _1\\in B(t _k)}\\left\\vert \\sum _{i=1}^n W _iY _iX _{ij}\\cdot[I(\\kappa _i(\\tilde{\\beta} _1)\\geq 0) - I(\\kappa _i(\\beta _{01})\\geq 0)-\\text{Pr}\\lbrace \\kappa _i(\\tilde{\\beta} _1)\u0026gt;0\\rbrace + \\text{Pr}\\lbrace \\kappa _i(\\beta _{01}) \u0026gt; 0\\rbrace \\right.\\newline \u0026amp;\\left.\\phantom{\\sup _{\\tilde{\\beta} _1\\in B(t _k)}\\sum _{i=1}^n W _iY _iX _{ij}\\cdot\\kappa _i}+I(\\kappa _i(t _k)\\geq 0) - I(\\kappa _i(t _k)\\geq 0) + \\text{Pr}(\\kappa _i(t _k)\\geq 0) - \\text{Pr}(\\kappa _i(t _k)\\geq 0)]\\right\\vert\\newline \\leq\u0026amp;\\left\\vert \\sum _{i=1}^n W _iY _iX _{ij}[I(\\kappa _i(t _k)\\geq 0) - I(\\kappa _i(\\beta _{01})\\geq 0) - \\text{Pr}\\lbrace \\kappa _i(t _k)\\geq 0\\rbrace + \\text{Pr}\\lbrace \\kappa _i(\\beta _{01})\\geq 0\\rbrace]\\right\\vert\\newline \u0026amp;+ \\sup _{\\tilde{\\beta} _1\\in B(t _k)}\\left\\vert \\sum _{i=1}^n W _iY _iX _{ij}[I(\\kappa _i(\\tilde{\\beta} _1)\\geq 0) - I(\\kappa _i(t _k)\\geq 0)-\\text{Pr}\\lbrace \\kappa _i(\\tilde{\\beta} _1)\u0026gt;0\\rbrace + \\text{Pr}\\lbrace \\kappa _i(t _k) \u0026gt; 0\\rbrace]\\right\\vert \\end{split} $$\n所以\n$$ \\begin{split} \u0026amp;\\text{Pr}\\left\\lbrace \\sup _{\\Vert \\beta _1 - \\beta _{01}\\Vert\\leq \\Delta\\sqrt{q/n}} \\left\\vert \\sum _{i=1}^n W _iY _iX _{ij}\\cdot[I(\\kappa _i(\\beta _1)\\geq 0) - I(\\kappa _i(\\beta _{01})\\geq 0)-\\text{Pr}\\lbrace \\kappa _i(\\beta _1)\u0026gt;0\\rbrace + \\text{Pr}\\lbrace \\kappa _i(\\beta _{01}) \u0026gt; 0\\rbrace]\\right\\vert \u0026gt; n\\lambda\\right\\rbrace\\newline \\leq\u0026amp;\\sum _{k=1}^N \\text{Pr}\\left\\lbrace \\sup _{\\tilde{\\beta} _1\\in B(t _k)} \\left\\vert \\sum _{i=1}^n W _iY _iX _{ij}\\cdot[I(\\kappa _i(\\tilde{\\beta} _1)\\geq 0) - I(\\kappa _i(\\beta _{01})\\geq 0)-\\text{Pr}\\lbrace \\kappa _i(\\tilde{\\beta} _1)\u0026gt;0\\rbrace + \\text{Pr}\\lbrace \\kappa _i(\\beta _{01}) \u0026gt; 0\\rbrace]\\right\\vert \u0026gt; n\\lambda\\right\\rbrace\\newline \\leq\u0026amp;\\sum _{k=1}^N \\text{Pr}\\left\\lbrace \\left\\vert \\sum _{i=1}^n W _iY _iX _{ij}[I(\\kappa _i(t _k)\\geq 0) - I(\\kappa _i(\\beta _{01})\\geq 0) - \\text{Pr}\\lbrace \\kappa _i(t _k)\\geq 0\\rbrace + \\text{Pr}\\lbrace \\kappa _i(\\beta _{01})\\geq 0\\rbrace]\\right\\vert\\right.\\newline \u0026amp;\\phantom{\\sum _{k=1}^N \\text{Pr}[[]]}\\left. + \\sup _{\\tilde{\\beta} _1\\in B(t _k)}\\left\\vert \\sum _{i=1}^n W _iY _iX _{ij}[I(\\kappa _i(\\tilde{\\beta} _1)\\geq 0) - I(\\kappa _i(t _k)\\geq 0)-\\text{Pr}\\lbrace \\kappa _i(\\tilde{\\beta} _1)\u0026gt;0\\rbrace + \\text{Pr}\\lbrace \\kappa _i(t _k) \u0026gt; 0\\rbrace]\\right\\vert\u0026gt; n\\lambda\\right\\rbrace\\newline \\leq\u0026amp;\\sum _{k=1}^N \\text{Pr}\\left\\lbrace \\left\\vert \\sum _{i=1}^n W _iY _iX _{ij}[I(\\kappa _i(t _k)\\geq 0) - I(\\kappa _i(\\beta _{01})\\geq 0) - \\text{Pr}\\lbrace \\kappa _i(t _k)\\geq 0\\rbrace + \\text{Pr}\\lbrace \\kappa _i(\\beta _{01})\\geq 0\\rbrace]\\right\\vert\\right. \u0026gt; \\frac{n\\lambda}{2} \\quad\\text{或者}\\newline \u0026amp;\\phantom{\\sum _{k=1}^N \\text{Pr}[[]]}\\left. + \\sup _{\\tilde{\\beta} _1\\in B(t _k)}\\left\\vert \\sum _{i=1}^n W _iY _iX _{ij}[I(\\kappa _i(\\tilde{\\beta} _1)\\geq 0) - I(\\kappa _i(t _k)\\geq 0)-\\text{Pr}\\lbrace \\kappa _i(\\tilde{\\beta} _1)\u0026gt;0\\rbrace + \\text{Pr}\\lbrace \\kappa _i(t _k) \u0026gt; 0\\rbrace]\\right\\vert\u0026gt; \\frac{n\\lambda}{2}\\right\\rbrace\\newline \u0026amp;\\leq J _{nj _1} + J _{nj _2} \\end{split} $$\n先分析 $J _{nj _1}$，令 $U _i = W _iY _iX _{ij}[I(\\kappa _i(t _k)\\geq 0) - I(\\kappa _i(\\beta _{01})\\geq 0) - \\text{Pr}\\lbrace \\kappa _i(t _k)\\geq 0\\rbrace + \\text{Pr}\\lbrace \\kappa _i(\\beta _{01})\\geq 0\\rbrace]$，易知 $U _i$ 是均值为0、互相独立的随机变量，且 $\\mathbb{E}(U _i^2) = \\mathbb{E}(U _i^2|Y _i=1)\\text{Pr}(Y _i=1) + \\mathbb{E}(U _i^2|Y _i=-1)\\text{Pr}(Y _i=-1)$。令 $F$ 和 $G$ 分别是给定 $Y _i=1$ 和 $Y _i=-1$ 时 $Z^T\\beta _{01}$ 的分布函数。注意到 $X _ij, j = q+1, \\ldots, p$ 是sub-Gaussian随机变量，则根据C8可以推出\n$$ \\begin{split} \\mathbb{E}(U_i^2|Y_i=1)\\text{Pr}(Y_i=1) \\leq \u0026amp;C(F_i(1 + Z_i^T(\\beta_{01} - t_k))[1 - F_i(1 + Z_i^T(\\beta_{01} - t_k))] + F_i(1)(1 - F_i(1))\\newline \u0026amp;- 2F_i[\\min(1 + Z_i^T(\\beta_{01} - t_k), 1)] + 2F_i(1)F_i(1 + Z_i^T(\\beta_{01} - t_k)))\\newline \\leq\u0026amp; C\\vert Z_i^T(t_k - \\beta_{01})\\vert \\end{split} $$\n ？？？？暂时没有想出为什么有上述的不等式放缩。\n 类似地\n$$ \\begin{split} \\mathbb{E}(U_i^2|Y_i=-1)\\text{Pr}(Y_i=-1) \\leq \u0026amp;C(G_i(-1 + Z_i^T(\\beta_{01} - t_k))[1 - G_i(-1 + Z_i^T(\\beta_{01} - t_k))] + G_i(-1)(1 - G_i(-1))\\newline \u0026amp;- 2G_i[\\max(-1 + Z_i^T(\\beta_{01} - t_k), -1)] + 2G_i(-1)G_i(-1 + Z_i^T(\\beta_{01} - t_k)))\\newline \\leq\u0026amp; C\\vert Z_i^T(t_k - \\beta_{01})\\vert \\end{split} $$\n所以\n$$ \\sum_{i=1}^n \\text{Var}(U_i) \\leq nC\\max_i \\Vert Z_i\\Vert \\Vert \\beta_{01} - t_k\\Vert = n O(\\sqrt{q}\\log(n))O(\\sqrt{q/n}) = O(\\sqrt{n}q\\log(n)) $$\n因为 $N \\leq (2n^2)^q\\leq n^{4q}$，根据Bernstein不等式，存在常数 $C_1$ 和 $C_2$ 使得\n$$ \\begin{split} J_{nj_1} \u0026amp;\\leq 2N\\exp\\left( -\\frac{n^2\\lambda^2/4}{C_1\\sqrt{n}q\\log(n) + C_2 n\\lambda}\\right) \\leq C\\exp\\left( 4q\\log(n)-\\frac{n^2\\lambda^2/4}{C_1\\sqrt{n}q\\log(n) + C_2 n\\lambda}\\right)\\newline \u0026amp;= C\\exp\\left( 4q\\log(n)-\\frac{n\\lambda/4}{C_1\\frac{\\sqrt{n}q\\log(n)}{n\\lambda} + C_2}\\right)\\newline \u0026amp;= C\\exp\\left( 4q\\log(n)-\\frac{n\\lambda/4}{C_1\\frac{\\log(p)q\\log(n)n^{-1/2}}{\\lambda}\\cdot\\frac{1}{\\log(p)} + C_2}\\right)\\newline \\end{split} $$\n又因为 $\\log(p)q\\log(n)n^{-1/2} = o(\\lambda)$，所以存在常数 $C$ 使得\n$$ \\frac{1/4}{C_1\\frac{\\log(p)q\\log(n)n^{-1/2}}{\\lambda}\\cdot\\frac{1}{\\log(p)} + C_2} \\leq C $$\n从而\n$$ J_{nj_1} \\leq C\\exp\\left( 4q\\log(n)-\\frac{n\\lambda/4}{C_1\\frac{\\log(p)q\\log(n)n^{-1/2}}{\\lambda}\\cdot\\frac{1}{\\log(p)} + C_2}\\right) \\leq C\\exp\\left( 4q\\log(n)-Cn\\lambda\\right) \\qquad (5) $$\n再分析 $J_{nj_2}$，令\n$$ V_i = I(\\kappa_i(\\tilde{\\beta}_1)\\geq 0) - I(\\kappa_i(t_k)\\geq 0)-\\text{Pr}\\lbrace \\kappa_i(\\tilde{\\beta}_1)\u0026gt;0\\rbrace + \\text{Pr}\\lbrace \\kappa_i(t_k) \u0026gt; 0\\rbrace $$\n由于 $\\kappa_i(\\tilde{\\beta}_1) = 1 - Z_i^T\\tilde{\\beta}_1 = 1 - Z_i^T(\\tilde{\\beta}_1 + t_k - t_k) = \\kappa_i(t_k)-Z_i^T(\\tilde{\\beta}_1 - t_k)$，则\n$$ \\kappa_i(\\tilde{\\beta}_1) \\geq 0 \\Rightarrow \\kappa_i(t_k) \\geq Z_i^T(\\tilde{\\beta}_1 - t_k) $$\n又 $|Z_i^T(\\tilde{\\beta}_1 - t_k)| \\leq \\Vert Z_i\\Vert\\cdot \\Vert \\tilde{\\beta}_1 - t_k\\Vert \\leq \\Vert Z_i\\Vert \\cdot \\Delta\\sqrt{q/n^5}$ ，注意到 $I(x\\geq s)$ 是 $s$ 的减函数，故 $\\forall\\tilde{\\beta}_1\\in B(t_k)$，有 $-B_i\\leq V_i\\leq A_i$，其中\n$$ \\begin{split} A_i \u0026amp;= I(\\kappa_i(t_k)\\geq -\\Vert Z_i\\Vert \\cdot \\Delta\\sqrt{q/n^5}) - I(\\kappa_i(t_k)\\geq 0)-\\text{Pr}\\lbrace \\kappa_i(t_k) \u0026gt; \\Vert Z_i\\Vert \\cdot \\Delta\\sqrt{q/n^5}\\rbrace + \\text{Pr}\\lbrace \\kappa_i(t_k) \u0026gt; 0\\rbrace\\newline B_i \u0026amp;= I(\\kappa_i(t_k)\\geq 0) - I(\\kappa_i(t_k)\\geq \\Vert Z_i\\Vert \\cdot \\Delta\\sqrt{q/n^5}) - \\text{Pr}\\lbrace \\kappa_i(t_k) \u0026gt; 0\\rbrace + \\text{Pr}\\lbrace \\kappa_i(t_k) \u0026gt; -\\Vert Z_i\\Vert \\cdot \\Delta\\sqrt{q/n^5}\\rbrace \\end{split} $$\n容易验证 $A_i, B_i$ 都大于零。\n因为 $\\forall V_i$ 都满足\n$$ \\left\\vert \\sum_{i=1}^n W_iY_iX_{ij}V_i\\right\\vert \\leq \\sum_{i=1}^n \\vert W_iY_iX_{ij}V_i\\vert \\leq \\max_{i}|X_{ij}|\\cdot\\sum_{i=1}^n \\vert V_i\\vert \\leq \\max_{i}|X_{ij}|\\cdot \\max\\left( \\sum_{i=1}^n A_i, \\sum_{i=1}^n B_i\\right) $$\n所以\n$$ \\begin{split} \u0026amp;\\text{Pr}\\left\\lbrace \\sup _{\\tilde{\\beta} _1\\in B(t _k)}\\left\\vert \\sum _{i=1}^n W _iY _iX _{ij}[I(\\kappa _i(\\tilde{\\beta} _1)\\geq 0) - I(\\kappa _i(t _k)\\geq 0) - \\text{Pr}\\lbrace \\kappa _i(\\tilde{\\beta} _1)\\geq 0\\rbrace + \\text{Pr}\\lbrace \\kappa _i(t _k)\\geq 0\\rbrace]\\right\\vert \\geq \\frac{n\\lambda}{2}\\right\\rbrace\\newline =\u0026amp;\\text{Pr}\\left\\lbrace \\sup _{\\tilde{\\beta} _1\\in B(t _k)}\\left\\vert \\sum _{i=1}^n W _iY _iX _{ij}V _i\\right\\vert \\geq \\frac{n\\lambda}{2}\\right\\rbrace \\leq \\text{Pr}\\left\\lbrace \\sup _{\\tilde{\\beta} _1\\in B(t _k)} \\max _{i}|X _{ij}|\\cdot\\sum _{i=1}^n \\vert V _i\\vert \\geq \\frac{n\\lambda}{2}\\right\\rbrace\\newline \\leq\u0026amp;\\text{Pr}\\left\\lbrace \\max _{i}|X _{ij}|\\cdot \\max\\left( \\sum _{i=1}^n A _i, \\sum _{i=1}^n B _i\\right) \\geq \\frac{n\\lambda}{2}\\right\\rbrace \\end{split} $$\n注意到\n$$ \\begin{split} \\sum_{i=1}^n A_i = \u0026amp;\\sum_{i=1}^n [I(\\kappa_i(t_k)\\geq -\\Vert Z_i\\Vert \\cdot \\Delta\\sqrt{q/n^5}) - I(\\kappa_i(t_k)\\geq 0)-\\text{Pr}\\lbrace \\kappa_i(t_k) \u0026gt; -\\Vert Z_i\\Vert \\cdot \\Delta\\sqrt{q/n^5}\\rbrace + \\text{Pr}\\lbrace \\kappa_i(t_k) \u0026gt; 0\\rbrace]\\newline \u0026amp;+ \\sum_{i=1}^n [\\text{Pr}\\lbrace \\kappa_i(t_k) \u0026gt; -\\Vert Z_i\\Vert \\cdot \\Delta\\sqrt{q/n^5}\\rbrace - \\text{Pr}\\lbrace \\kappa_i(t_k) \u0026gt; \\Vert Z_i\\Vert \\cdot \\Delta\\sqrt{q/n^5}\\rbrace] \\end{split} $$\n以及根据C8有\n$$ \\begin{split} \u0026amp;\\sum_{i=1}^n [\\text{Pr}\\lbrace \\kappa_i(t_k) \u0026gt; -\\Vert Z_i\\Vert \\cdot \\Delta\\sqrt{q/n^5}\\rbrace - \\text{Pr}\\lbrace \\kappa_i(t_k) \u0026gt; \\Vert Z_i\\Vert \\cdot \\Delta\\sqrt{q/n^5}\\rbrace]\\newline =\u0026amp;\\sum_{i=1}^n [(F_i(1 + \\Vert Z_i\\Vert \\cdot \\Delta\\sqrt{q/n^5} - Z_i^T(\\beta_{01} - t_k)) - F_i(1 - \\Vert Z_i\\Vert \\cdot \\Delta\\sqrt{q/n^5} - Z_i^T(\\beta_{01} - t_k)))\\text{Pr}(Y_i = 1)\\newline \u0026amp;\\phantom{\\sum_{i=1}^n[}+ (G_i(-1 + \\Vert Z_i\\Vert \\cdot \\Delta\\sqrt{q/n^5} - Z_i^T(\\beta_{01} - t_k)) - G_i(-1 + \\Vert Z_i\\Vert \\cdot \\Delta\\sqrt{q/n^5} - Z_i^T(\\beta_{01} - t_k)))\\text{Pr}(Y_i = -1)]\\newline \\leq\u0026amp;O(n^{-3/2}q\\log(n)) \\end{split} $$\n ？？？？不知道最后的不等式结果怎么来的。\n 令\n$$ O_i = I(\\kappa_i(t_k)\\geq -\\Vert Z_i\\Vert \\cdot \\Delta\\sqrt{q/n^5}) - I(\\kappa_i(t_k)\\geq 0)-\\text{Pr}\\lbrace \\kappa_i(t_k) \u0026gt; -\\Vert Z_i\\Vert \\cdot \\Delta\\sqrt{q/n^5}\\rbrace + \\text{Pr}\\lbrace \\kappa_i(t_k) \u0026gt; 0\\rbrace $$\n这样，对充分大的 $n$，由于 $\\lambda = o(n^{-(1-c_2)/2})$ 和C7， 我们有\n$$ \\sum_{k=1}^N \\text{Pr}\\left( \\sum_{i=1}^n A_i\\geq \\frac{n\\lambda}{2}\\right) \\leq \\sum_{k=1}^N \\text{Pr}\\left( \\sum_{i=1}^n O_i\\geq \\frac{n\\lambda}{2} - Cn^{-3/2}q\\log(n)\\right) \\leq \\sum_{k=1}^N \\text{Pr}\\left( \\sum_{i=1}^n O_i\\geq \\frac{n\\lambda}{4}\\right) $$\n注意到 $O_i$ 是均值为零、互相独立的随机变量，且类似求 $\\mathbb{E}(U_i^2)$ 可得\n$$ \\begin{split} \\mathbb{E}(O_i^2) \\leq \\mathbb{E}(I(\\kappa_i(t_k)\\geq -\\Vert Z_i\\Vert \\cdot \\Delta\\sqrt{q/n^5}) - I(\\kappa_i(t_k)\\geq 0))^2 \\leq O(n^{-5/2}q\\log(n)) \\end{split} $$\n因为 $X_{ij}, q+1\\leq j\\leq p$ 是sub-Gaussian随机变量，所以 $\\max_i |X_{ij}| = O_p(\\sqrt{\\log(n)})$，所以\n$$ \\mathbb{E}(\\max_i |X_{ij}|O_i)^2 \\leq O(n^{-5/2}q\\log^2(n)) $$\n综上，根据Bernstein不等式，存在常数 $C_1, C_2$ 使得\n$$ \\begin{split} \u0026amp;\\sum_{k=1}^N \\text{Pr}\\left( \\max_i |X_{ij}|\\sum_{i=1}^n A_i\\geq \\frac{n\\lambda}{2}\\right) \\leq \\sum_{k=1}^N \\text{Pr}\\left( \\max_i |X_{ij}|\\sum_{i=1}^n O_i\\geq \\frac{n\\lambda}{4}\\right) \\leq N\\exp\\left( -\\frac{n^2\\lambda^2/16}{C_1 n^{3/2}q\\log^2(n) + C_2 n\\lambda}\\right)\\newline \\leq\u0026amp;C\\exp\\left( 4q\\log(n) - \\frac{n^2\\lambda^2/16}{C_1 n^{-3/2}q\\log^2(n) + C_2 n\\lambda}\\right)\\newline =\u0026amp;C\\exp\\left( 4q\\log(n) - \\frac{n\\lambda/16}{C_1 \\frac{\\log(p)q\\log(n)n^{-1/2}}{\\lambda}\\cdot\\frac{\\log(n)}{n\\log(p)} + C_2}\\right)\\newline \\leq\u0026amp;C\\exp\\left( 4q\\log(n) - C n\\lambda\\right) \\end{split} $$\n类似地，可以证明 $\\sum_{k=1}^N \\text{Pr}\\left( \\max_i |X_{ij}|\\sum_{i=1}^n B_i\\geq \\frac{n\\lambda}{2}\\right) \\leq C\\exp\\left( 4q\\log(n) - C n\\lambda\\right)$。所以，我们有\n$$ J_{nj_2} \\leq C\\exp\\left( 4q\\log(n) - C n\\lambda\\right) \\qquad (6) $$\n根据式（5）和式（6），引理3中的概率满足\n$$ \\sum_{j=q+1}^p (J_{nj_1} + J_{nj_2}) \\leq C\\exp\\left( \\log(p) + 4q\\log(n) - C n\\lambda\\right) \\rightarrow 0, \\quad n\\rightarrow \\infty $$\n这就证明了引理3中的结论。$\\square$\n现在，我们来证明定理1的结论。\n（1）没有惩罚的hinge损失目标函数是凸的。根据凸优化理论，一定存在 $v _j^ *$ 使得对应的 $s _j(\\hat{\\beta}) = 0$。\n（2）注意到 $\\min _{1\\leq j\\leq q} |\\hat{\\beta} _j| \\geq \\min _{1\\leq j\\leq q}|\\beta _{0j}| - \\max _{1\\leq j\\leq q}|\\hat{\\beta} _j - \\beta _{0j}|$。根据C7可知 $n^{(1-c _2)/2}\\min _{1\\leq j\\leq q}|\\beta _{0j}| \\geq M _1$，根据定理1可知 $\\max _{1\\leq j\\leq q}|\\hat{\\beta} _j - \\beta _{0j}| = O _p(\\sqrt{q/n})$。所以，容易看出 $\\min _{1\\leq j\\leq q} |\\hat{\\beta} _j| = O _p(n^{-(1 - c _2)/2})$。由于 $\\lambda = o(n^{-(1 - c _2)/2})$，所以 $\\text{Pr}\\lbrace|\\hat{\\beta} _j|\\geq (a+\\frac 12)\\lambda\\rbrace \\rightarrow 1, j=1, \\ldots, q$ 成立。\n（3）根据Oracle估计量的构造，我们有 $\\hat{\\beta} _j = 0, j= q+1,\\ldots, p$。我们可以充分表明 $\\text{Pr}\\lbrace|s _j(\\hat{\\beta})|\u0026gt;\\lambda, \\text{for some }j = q+1, \\ldots, p\\rbrace\\rightarrow 0$。不妨令 $D = \\lbrace i: 1 - Y _iZ _i^T\\hat{\\beta} _1 = 0\\rbrace$，我们有\n$$ s _j(\\hat{\\beta}) = -\\frac 1n \\sum _{i=1}^n W _iY _iX _{ij} I(1 - Y _iZ _i^T\\hat{\\beta} _1\\geq 0) - \\frac 1n \\sum _{i\\in D} W _iY _iX _{ij}v _i $$\n其中 $-1\\leq v_i\\leq 0, i\\in D$ 以及 $v_i = 0$ 其他情况。根据C5，$(Z_i, Y_i)$ 是一般情形，则依概率1满足 $D$ 中有 $q+1$ 个元素；且 $X_{ij}$ 是sub-Gaussian，满足 $\\max_i |X_{ij}| = O(\\sqrt{\\log(n)})$。再根据C4，依概率1满足 $|n^{-1}\\sum_{i\\in D} W_iY_iX_{ij}v_i| = O(n^{-1}q\\log(q)) = o(\\lambda)$。\n 没明白怎么算出的 $|n^{-1}\\sum_{i\\in D} W_iY_iX_{ij}v_i| = O(n^{-1}q\\log(q)) = o(\\lambda)$。\n 因此，我们只需证明 $\\text{Pr}\\lbrace \\max_{q+1\\leq j\\leq q}|n^{-1}\\sum_{i=1}^n W_iY_iX_{ij}I(1 - Y_iZ_i^T\\hat{\\beta}_1\\geq 0)| \u0026gt; \\lambda\\rbrace \\rightarrow 0$。注意到\n$$ \\begin{split} \u0026amp;\\text{Pr}\\left\\lbrace \\max _{q+1\\leq j\\leq q}|n^{-1}\\sum _{i=1}^n W _iY _iX _{ij}I(1 - Y _iZ _i^T\\hat{\\beta} _1\\geq 0)| \u0026gt; \\lambda\\right\\rbrace\\newline \\leq\u0026amp;\\text{Pr}\\left\\lbrace \\max _{q+1\\leq j\\leq q}\\left|n^{-1}\\sum _{i=1}^n W _iY _iX _{ij}[I(1 - Y _iZ _i^T\\hat{\\beta} _1\\geq 0) - I(1 - Y _iZ _i^T\\beta _{01}\\geq 0)]\\right| \u0026gt; \\frac{\\lambda}{2}\\right\\rbrace\\newline \u0026amp;+ \\text{Pr}\\left\\lbrace \\max _{q+1\\leq j\\leq q}\\left|n^{-1}\\sum _{i=1}^n W _iY _iX _{ij}I(1 - Y _iZ _i^T\\beta _{01}\\geq 0)\\right| \u0026gt; \\frac{\\lambda}{2}\\right\\rbrace \\end{split} \\qquad (7) $$\n根据引理2可知，式（7）中不等式右侧的第二项为 $o_p(1)$。另一方面，根据以引理1，式（7）中不等式右侧的第二项满足\n$$ \\begin{split} \u0026amp;\\text{Pr}\\left\\lbrace \\max _{q+1\\leq j\\leq q}\\left|n^{-1}\\sum _{i=1}^n W _iY _iX _{ij}[I(1 - Y _iZ _i^T\\hat{\\beta} _1\\geq 0) - I(1 - Y _iZ _i^T\\beta _{01}\\geq 0)]\\right| \u0026gt; \\frac{\\lambda}{2}\\right\\rbrace\\newline \\leq\u0026amp;\\text{Pr}\\left\\lbrace \\max _{q+1\\leq j\\leq q}\\sup _{\\Vert \\beta _1 - \\beta _{01}\\Vert\\leq \\Delta\\sqrt{q/n}}\\left|n^{-1}\\sum _{i=1}^n W _iY _iX _{ij}[I(1 - Y _iZ _i^T\\beta _1\\geq 0) - I(1 - Y _iZ _i^T\\beta _{01}\\geq 0)\\right.\\right.\\newline \u0026amp;\\phantom{\\max _{q+1\\leq j\\leq q}\\sup _{\\Vert \\beta _1 - \\beta _{01}\\Vert\\leq \\Delta\\sqrt{q/n}}\\sum _{i=1}^n W _iY _iX _{ij}nnnnnnnnn}-\\text{Pr}\\lbrace 1 - Y _iZ _i^T\\beta _1\\geq 0\\rbrace + \\text{Pr}\\lbrace 1 - Y _iZ _i^T\\beta _{01}\\geq 0\\rbrace\\newline \u0026amp;\\left.\\left.\\phantom{\\max _{q+1\\leq j\\leq q}\\sup _{\\Vert \\beta _1 - \\beta _{01}\\Vert\\leq \\Delta\\sqrt{q/n}}\\sum _{i=1}^n W _iY _iX _{ij}nnnnnnnnn}+\\text{Pr}\\lbrace 1 - Y _iZ _i^T\\beta _1\\geq 0\\rbrace - \\text{Pr}\\lbrace 1 - Y _iZ _i^T\\beta _{01}\\geq 0\\rbrace]\\right| \u0026gt; \\frac{\\lambda}{2}\\right\\rbrace\\newline \\leq\u0026amp;\\text{Pr}\\left\\lbrace \\max _{q+1\\leq j\\leq q}\\sup _{\\Vert \\beta _1 - \\beta _{01}\\Vert\\leq \\Delta\\sqrt{q/n}}\\left|n^{-1}\\sum _{i=1}^n W _iY _iX _{ij}[I(1 - Y _iZ _i^T\\beta _1\\geq 0) - I(1 - Y _iZ _i^T\\beta _{01}\\geq 0)\\right.\\right.\\newline \u0026amp;\\left.\\left.\\phantom{\\max _{q+1\\leq j\\leq q}\\sup _{\\Vert \\beta _1 - \\beta _{01}\\Vert\\leq \\Delta\\sqrt{q/n}}\\sum _{i=1}^n W _iY _iX _{ij}nnnnnnnnn}-\\text{Pr}\\lbrace 1 - Y _iZ _i^T\\beta _1\\geq 0\\rbrace + \\text{Pr}\\lbrace 1 - Y _iZ _i^T\\beta _{01}\\geq 0\\rbrace]\\right| \u0026gt; \\frac{\\lambda}{4}\\right\\rbrace\\newline \u0026amp;+\\text{Pr}\\left\\lbrace \\max _{q+1\\leq j\\leq q}\\sup _{\\Vert \\beta _1 - \\beta _{01}\\Vert\\leq \\Delta\\sqrt{q/n}}\\left|n^{-1}\\sum _{i=1}^n W _iY _iX _{ij}[\\text{Pr}\\lbrace 1 - Y _iZ _i^T\\beta _1\\geq 0\\rbrace - \\text{Pr}\\lbrace 1 - Y _iZ _i^T\\beta _{01}\\geq 0\\rbrace]\\right| \u0026gt; \\frac{\\lambda}{4}\\right\\rbrace \\end{split} \\qquad (8) $$\n根据引理3，式（8）中不等式右侧的第一项为 $o_p(1)$。因此，我只需界定第二项即可。注意到\n$$ \\begin{split} |\\text{Pr}\\lbrace 1 - Y_iZ_i^T\\beta_1\\geq 0\\rbrace - \\text{Pr}\\lbrace 1 - Y_iZ_i^T\\beta_{01}\\geq 0\\rbrace| \\leq \u0026amp;|F_i(1 + Z_i^T(\\beta_1 - \\beta_{01})) - F_i(1)|\\text{Pr}(Y_i = 1)\\newline \u0026amp;+ |G_i(-1 + Z_i^T(\\beta_1 - \\beta_{01})) - G_i(-1)|\\text{Pr}(Y_i = -1) \\end{split} $$\n则有\n$$ \\begin{split} \u0026amp;\\max_{q+1\\leq j\\leq q}\\sup_{\\Vert \\beta_1 - \\beta_{01}\\Vert\\leq \\Delta\\sqrt{q/n}}\\left|n^{-1}\\sum_{i=1}^n W_iY_iX_{ij}[\\text{Pr}\\lbrace 1 - Y_iZ_i^T\\beta_1\\geq 0\\rbrace - \\text{Pr}\\lbrace 1 - Y_iZ_i^T\\beta_{01}\\geq 0\\rbrace]\\right|\\newline \\leq\u0026amp; C\\max_{i,j}|X_{ij}|\\sup_{\\Vert \\beta_1 - \\beta_{01}\\Vert\\leq \\Delta\\sqrt{q/n}} n^{-1}\\sum_{i=1}^n \\Vert Z_i\\Vert\\cdot \\Vert \\beta_1 - \\beta_{01}\\Vert\\newline =\u0026amp;O(\\sqrt{\\log(pn)})O(\\sqrt{q/n})O(\\sqrt{q\\log(n)})\\newline =\u0026amp;o_p(1) \\end{split} $$\n则\n$$ \\text{Pr}\\left\\lbrace \\max_{q+1\\leq j\\leq q}\\sup_{\\Vert \\beta_1 - \\beta_{01}\\Vert\\leq \\Delta\\sqrt{q/n}}\\left|n^{-1}\\sum_{i=1}^n W_iY_iX_{ij}[\\text{Pr}\\lbrace 1 - Y_iZ_i^T\\beta_1\\geq 0\\rbrace - \\text{Pr}\\lbrace 1 - Y_iZ_i^T\\beta_{01}\\geq 0\\rbrace]\\right| \u0026gt; \\frac{\\lambda}{4}\\right\\rbrace = o_p(1) $$\n综上，也就证明了定理1的结论。$\\blacksquare$\n定理1主要刻画了hinge损失函数在Oracle估计量处次梯度的行为特征。具体而言，在通常的设定下，依概率1满足，对应重要变量的次梯度为零；对应无关变量的次梯度离零不会很远。定理1对证明Oracle性质有重要的启发作用。\n3.3 局部Oracle性质 在证明LASSO的变量选择一致性时，我们先找到LASSO模型和估计之间联结的桥梁——KKT条件，再分析稀疏解对应的KKT条件成立的概率，最后论证LASSO的变量选择一致性。与之类似，我们证明非凸惩罚SVM的局部Oracle性质，也要找到类似KKT条件联结非凸惩罚SVM和估计之间的条件，再讨论其成立的概率。与LASSO不同，非凸惩罚SVM是非凸优化，我们在此要研究针对DC优化（difference convex programming）的充分局部最优性条件（sufficient local optimality condition），而不是KKT条件。\n注意到，虽然非凸惩罚SVM（3）是非凸优化问题，但是它可以写成两个凸函数的差，即\n$$ Q(\\beta) = g(\\beta) - h(\\beta) $$\n其中\n$$ g(\\beta) = \\frac 1n \\sum _{i=1}^n W _i(1 - Y _iX _i^T\\beta) _+ + \\lambda \\sum _{i=1}^p |\\beta _j| $$\n以及\n$$ h(\\beta) = \\lambda \\sum _{i=1}^p |\\beta _j| - \\sum _{i=1}^p p _{\\lambda}(|\\beta _j|) = \\sum _{i=1}^p H _{\\lambda}(\\beta _j) $$\n其中 $H_{\\lambda}(|\\beta_j|)$ 和罚函数有关。对于SCAD罚项，我们有\n$$ H_{\\lambda}(\\beta_j) = \\frac{\\beta_j^2 - 2\\lambda|\\beta_j| + \\lambda^2}{2(a - 1)}I(\\lambda\\leq |\\beta_j|\\leq a\\lambda) + \\left\\lbrace \\lambda|\\beta_j| + \\frac{(a+1)\\lambda^2}{2}\\right\\rbrace I(|\\beta_j| \u0026gt; a\\lambda) $$\n以上关于 $Q(\\beta)$ 的凸函数的差分解，表明非凸惩罚SVM（3）满足DC算法（difference of convex function algorithm，An and Tao（2005））的形式要求，可以用相关理论进行分析。我们具体阐述基于次梯度的充分局部最优性条件。Tao and An（1997）中推论（Corollary）1表明，若在点 $x^ *$ 附近有一个领域 $U$ 使得 $\\partial h(x) \\cap\\partial g(x^ *)\\neq \\emptyset, \\forall x\\in U\\cup \\text{dom}(g)$，则 $x^ *$ 是 $g(x) - h(x)$ 的一个局部最小值点（local minimizer）。基于该结论，我们来证明局部Oracle性质。\n首先，我们计算 $g(\\beta)$ 和 $h(\\beta)$ 的次梯度。对 $g(\\beta)$，我们有\n$$ \\partial g(\\beta) = \\left\\lbrace \\xi\\in\\mathbb{R}^{p+1}: \\xi_j = -\\frac 1n \\sum_{i=1}^n W_iY_iX_{ij}I(1 - Y_iX_i^T\\hat{\\beta}\u0026gt;0) - \\frac 1n \\sum_{i=1}^n W_iY_iX_{ij}v_i + \\lambda l_j, j = 0, 1,\\ldots, p\\right\\rbrace $$\n其中 $l_0 = 0, l_j = \\text{sgn}(\\beta_j), \\beta_j\\neq 0$ 以及 $l_j \\in [-1,1]$ 其他情况；$v_i \\in [-1,0], 1 - Y_iX_i^T\\hat{\\beta} = 0$ 以及 $v_i = 0$ 其他情况。\n根据假设2，对非凸罚项，我们有 $\\lim_{t\\rightarrow 0+} H_{\\lambda}'(t) = \\lim_{t\\rightarrow 0-} H_{\\lambda}'(t) = \\lambda\\text{sgn}(t) - \\lambda\\text{sgn}(t) = 0$，所以 $h(\\beta)$ 处处可导，所以\n$$ \\partial h(\\beta) = \\left\\lbrace \\mu\\in \\mathbb{P}^{p+1}: \\mu_j = \\frac{\\beta_j - \\lambda\\text{sgn}(\\beta_j)}{a - 1}I(\\lambda \\leq |\\beta_j|\\leq a\\lambda) + \\lambda\\text{sgn}(\\beta_j)I(|\\beta_j|\u0026gt;a\\lambda)\\right\\rbrace $$\n上述结果对SCAD罚项成立，其他罚项如MCP可自行推导。\n结合充分局部最优性条件和定理1中的结论，我们应当表明依概率1满足，对任意在以Oracle估计量 $\\hat{\\beta}$ 为中心、半径为 $\\lambda/2$ 的球域中的 $\\beta$，存在一个次梯度 $\\xi\\in \\partial g(\\hat{\\beta})$ 使得 $\\partial h(\\beta)/\\partial \\beta_j = \\xi_j, j = 0, 1, \\ldots, p$。这就能推出Oracle估计量 $\\hat{\\beta}$ 自己就是非凸惩罚SVM（3）的一个局部最小值点。我们把这个结论用定理2来描述。\n【定理2】假定C1-C8都满足。令 $B_n(\\lambda)$ 是非凸优化问题中基于参数 $\\lambda$ 的 $Q(\\beta)$ 的局部最小值集合。若 $\\lambda = o(n^{-(1 - c_1)/2})$ 和 $\\log(p)q\\log(n)n^{-1/2} = o(\\lambda)$，则Oracle估计量 $\\hat{\\beta} = (\\hat{\\beta}_1^T, \\mathbf{0}^T)^T$ 满足\n$$ \\text{Pr}\\lbrace \\hat{\\beta}\\in B_n(\\lambda)\\rbrace \\rightarrow 1, \\quad n\\rightarrow\\infty. $$\n【证明】我们应当表明通过写成 $Q(\\beta) = g(\\beta) - g(\\beta)$ 来说明 $\\hat{\\beta}$ 是 $Q(\\beta)$ 的局部最小值点。\n根据定理1，我们有 $\\text{Pr}\\lbrace \\mathscr{G}\\subseteq\\partial g(\\hat{\\beta})\\rbrace$，其中\n$$ \\mathscr{G} = \\lbrace \\xi: \\xi_0 = 0; \\xi_j = \\lambda\\text{sgn}(\\hat{\\beta}_j), j = 1, \\ldots, q; \\xi_j = s_j(\\hat{\\beta}) + \\lambda l_j, j = q+1, \\ldots, p\\rbrace $$\n考虑任意在以Oracle估计量 $\\hat{\\beta}$ 为中心、半径为 $\\lambda/2$ 的球域中的 $\\beta$，我们可以证明存在 $\\xi^ * \\in \\mathscr{G}$ 使得 $\\text{Pr}\\lbrace \\xi_j^ * = \\partial h(\\beta)/\\partial \\beta_j\\rbrace \\rightarrow 1$ 在 $n\\rightarrow \\infty$ 时成立。\n由于 $\\partial h(\\beta)/\\partial \\beta_0 = 0$，我们有 $\\xi_0^ * = \\partial h(\\beta)/\\partial \\beta_0$。\n对 $j = 1, \\ldots, q$，根据定理1，我们有 $\\min _{1\\leq j\\leq q}|{\\beta} _j| \\geq \\min _{1\\leq j\\leq q}|\\hat{\\beta} _j| - \\max _{1\\leq j\\leq q}|\\hat{\\beta} _j - \\beta _j| \\geq (a + 1/2)\\lambda - \\lambda/2 = a\\lambda$ 依概率1成立。根据针对非凸罚项的假设2，有 $\\text{Pr}\\lbrace \\partial h(\\beta)/\\partial \\beta _j = \\lambda\\text{sgn}(\\beta _j)\\rbrace\\rightarrow 1, j = 1, \\ldots, q$。根据引理1和定理1，对充分大的 $n$，有 $\\text{sgn}(\\beta _j) = \\text{sgn}(\\hat{\\beta} _j)$。所以，当 $n\\rightarrow \\infty$ 时，我们有 $\\text{Pr}\\lbrace \\xi _j^ * = \\partial h(\\beta)/\\partial \\beta _j\\rbrace\\rightarrow 1, j = 1, \\ldots, q$。\n对 $j = q+1, \\ldots, p$，根据定理1，有 $\\text{Pr}\\lbrace |\\beta _j| \\leq |\\hat{\\beta} _j| + |\\beta _j - \\hat{\\beta} _j|\\leq \\lambda\\rbrace\\rightarrow 1$。所以，对SCAD罚项有 $\\text{Pr}\\lbrace \\partial h(\\beta)/\\partial \\beta_j = 0\\rbrace\\rightarrow 1$。注意到针对非凸罚项的假设2，有 $\\text{Pr}\\lbrace |\\partial h(\\beta)/\\partial \\beta_j| \\leq \\lambda\\rbrace\\rightarrow 1$。根据定理1，有 $\\text{Pr}\\lbrace |s_j(\\hat{\\beta})|\\leq \\lambda\\rbrace\\rightarrow 1, j = q+1, \\ldots, p$。所以，我们总是可以找到 $l_j\\in [-1, 1]$ 使得 $\\text{Pr}\\lbrace \\xi_j^ * = s_j(\\hat{\\beta}) + \\lambda l_j = \\partial h(\\beta)/\\partial \\beta_j\\rbrace \\rightarrow 1, j = q+1, \\ldots, p$ 成立。\n这样就证明了定理2的结论。$\\blacksquare$\n可以证明，如果取 $\\lambda = n6{-1/2 + \\delta}, c_1\u0026lt;\\delta\u0026lt;c_2/2$，则局部Oracle性质对 $p = o(\\exp(n^{(\\delta - c_1)/2}))$ 也成立。这表明非凸惩罚SVM的局部Oracle性质在变量个数随着样本量呈指数增长时也成立。\n4 结语 传统的机器学习理论主要关注模型的泛化误差、Bayes渐近性质等，而较少关注模型参数的大样本性质。本文借鉴统计中关于高维稀疏化模型的研究，探讨了高维情形下非凸惩罚SVM的系数的变量选择一致性。相关文献还不多见，值得进一步跟踪研究。\n","id":1,"section":"posts","summary":"传统的支持向量机模型（support vector machine，SVM）可以适用统计中的正则化模型框架，即损失+罚项的结构，其中SVM采用的是hing","tags":["机器学习","变量选择","高维数据分析"],"title":"高维情形下非凸惩罚SVM的参数一致性","uri":"https://qkai-stat.github.io/2022/08/svm-with-non-convex-penalization/","year":"2022"},{"content":"交替最小化 (alternating minimization) 算法在很多领域中有着广泛的应用场景。本文主要讨论凸优化情形下AM算法的收敛性，内容来源于对Beck（2015）的翻译。\n1 问题的引入与基本假设 1.1 问题的引入 在本文中，AM算法考虑如下的优化问题：\n$$ \\min_{y\\in\\mathbb{R}^{n_1}, z\\in\\mathbb{R}^{n_2}} {H(y, z) \\equiv f(y, z) + g_1(y) + g_2(z)} \\qquad (1) $$\n其中 $f, g_1, g_2$ 被假定满足如下的条件：\n【A】$g_1: \\mathbb{R}^{n_1}\\mapsto (\\infty, \\infty]$ 和 $g_2: \\mathbb{R}^{n_2}\\mapsto (\\infty, \\infty]$ 都是闭函数、真凸函数 (proper convex functions)，且在各自定义域内被假定为次可微的。\n【B】$f$ 是在 $\\mathrm{dom}g_1\\times \\mathrm{dom}g_2$ 上连续、可导的凸函数。\n 真函数 (proper functions)，是在 $[-\\infty, +\\infty] = \\mathbb{R}\\cup{\\pm\\infty}$ 上扩展定义的实值函数，满足：定义域非空集、在 $-\\infty$ 处从不取值以及不恒等于 $+\\infty$。\n 针对优化问题 (1)，交替最小化 (alternating minimization，AM) 算法的一般流程如下所述：\n第一步：初始化 $y_0\\in \\mathrm{dom}g_1, z_0\\in \\mathrm{dom}g_2$ 使得\n$$ z_0 \\in arg\\min_{z\\in\\mathbb{R}^{n_2}} f(y_0, z) + g_2(z) $$\n第二步：循环迭代以下问题 $(k=0, 1, 2, \\ldots)$：\n$$ \\begin{split} y_{k+1} \u0026amp;\\in arg\\min_{y\\in\\mathbb{R}^{n_1}} f(y, z_{k}) + g_1(y)\\newline z_{k+1} \u0026amp;\\in arg\\min_{z\\in\\mathbb{R}^{n_2}} f(y_{k}, z) + g_2(z)\\newline \\end{split} $$\n1.2 基本假设 下面给出建立问题（1）下AM算法的收敛性理论的一些基本假设。为记号方便，令 $x = (y, z)$ 为优化变量，则 $f(y, z)$ 可以记成 $f(x)$，$g_1(y)+g_2(z)$ 可以记成 $g(x)$。令 $\\nabla_1 f(x)$ 为 $f(x)$ 关于 $y$ 的梯度，$\\nabla_2 f(x)$ 为 $f(x)$ 关于 $z$ 的梯度。基于这些符号，我们假定如下条件成立：\n【C】$f$ 关于 $y$ 的梯度 $\\nabla_1 f(x)$ 在 $\\mathrm{dom}g_1$ 上是关于常数 $L_1\\in (0, \\infty)$ Lipschitz连续的，即\n$$ \\Vert \\nabla_1 f(y + d_1, z) - \\nabla_1 f(y, z)\\Vert \\leq L_1\\Vert d_1\\Vert $$\n对任意 $y\\in \\mathrm{dom}g_1, z\\in \\mathrm{dom}g_2$，$d_1\\in \\mathbb{R}^{n_1}$ 且 $y+d_1 \\in \\mathrm{dom}g_1$ 都成立。\n在某些情形中，我们也假设 $f$ 关于 $z$ 的梯度是Lipschitz连续的：\n【D】$f$ 关于 $z$ 的梯度 $\\nabla_2 f(x)$ 在 $\\mathrm{dom}g_2$ 上是关于常数 $L_2\\in (0, \\infty]$ Lipschitz连续的，即\n$$ \\Vert \\nabla_2 f(y, z + d_2) - \\nabla_2 f(y, z)\\Vert \\leq L_2\\Vert d_2\\Vert $$\n对任意 $y\\in \\mathrm{dom}g_1, z\\in \\mathrm{dom}g_2$，$d_2\\in \\mathbb{R}^{n_2}$ 且 $z+d_2 \\in \\mathrm{dom}g_2$ 都成立。\n在后续的收敛性分析中，我们总是假定【A】-【D】成立，且允许 $L_2$ 取 $\\infty$——这意味着如非特别说明，目标函数对于优化变量的第二块 (即 $z$) 可以是非Lipschtiz连续的。\n当然，除了假设【A】-【D】外，为了保证优化问题（1）的良好定义，我们还始终默认如下的假设：\n【E】令问题（1）的最优解集非空，记为 $X ^ *$，对应的最优目标函数值记为 $H ^ *$。此外，对任意的 $\\tilde{y}\\in \\mathrm{dom}g_1$ 和 $\\tilde{z}\\in \\mathrm{dom}g_2$，问题\n$$ \\min_{z\\in\\mathbb{R}^{n_2}} f(\\tilde{y}, z) + g_2(z), \\quad \\min_{y\\in\\mathbb{R}^{n_1}} f(y, \\tilde{z}) + g_1(y) $$\n存在最小值。\n2 预备知识 为了方便后续进行收敛性分析，我们在此给出相关的一些数学背景知识，主要包括近端算子（proximal operator）和梯度映射（gradient mapping）。此外，我们还将给出两个与收敛性分析密切相关的结论，即块下降引理（block decent lemma）和充分下降性质（sufficient decrease property）。\n2.1 近端算子 对于一个给定的闭的、真扩展实值凸函数 $h: \\mathbb{R}^n\\mapsto (-\\infty, \\infty]$，其近端算子（proximal operator）定义为\n$$ \\text{prox}_h (x) = arg\\min _{u} \\left\\lbrace h(u) + \\frac12 \\Vert u - x\\Vert^2 \\right\\rbrace $$\n其中 $\\Vert\\cdot\\Vert^2$ 是定义在 $\\mathbb{R}^n$ 上的 $l_2$ 范数。\n 根据近端算子的定义不难看出，$h$ 在点 $x$ 处的近端值是一个点（$\\in\\mathbb{R}^n$），即 $x$ 和 $h$ 最小值处变量取值之间的折衷点。因此，称 $\\text{prox}_h(x)$ 是 $x$ 相对于 $h$ 的近点。\n 下面探讨近端算子和梯度之间的关系。根据定义，若$h$ 可微，则 $\\text{prox}_h(x)$ 满足一阶微分条件，即\n$$ \\nabla h(u) + u - x = 0 \\Rightarrow u \\approx x - \\lambda\\nabla h(x) $$\n当 $\\lambda$ 很小时成立。这表明，近端算子和梯度方法之间有密切的联系，其中 $\\lambda$ 类似于梯度方法中的步长。近端算子的详细讨论参见Parikh and Boyd（2014）。\n一个重要且有用的识别近端算子的结论由以下引理给出：\n【引理1】令 $h: \\mathbb{R}^n \\mapsto (-\\infty, \\infty]$ 是一个闭的、真凸函数，再令 $M\u0026gt;0$，则\n$$ w = \\text{prox}_{\\frac 1M h}(x) $$\n当且仅当 $\\forall u\\in \\text{dom}h$，满足\n$$ h(u) \\geq h(w) + M \\langle x - w, u - w \\rangle $$\n2.2 梯度映射 另一个重要的概念是梯度映射（gradient mapping）。给定 $M\u0026gt;0$，和问题（1）相关的近端-梯度（prox-grad）映射定义为\n$$ T_M(x) = \\text{prox}_{\\frac1M g} \\left( x - \\frac1M \\nabla f(x)\\right) $$\n根据梯度方法，$x - \\frac1M \\nabla f(x)$ 是基于 $x$ 以步长 $\\frac1M$ 得到的 $f$ 的下降点。根据近端算子，$T_M(x)$ 是基于 $x - \\frac1M \\nabla f(x)$ 得到的 $g$ 的下降点。所以，$T_M(x)$ 是基于 $x$ 得到的 $f$ 和 $g$ 同时下降的点。\n类似的，我们可以定义分别关于 $y$ 和 $z$ 的近端-梯度算子：\n$$ \\begin{split} T_M^1(x) \u0026amp;= \\text{prox} _{\\frac 1M g_1} \\left( y - \\frac 1M \\nabla _1 f(y,z)\\right)\\newline T_M^2(x) \u0026amp;= \\text{prox} _{\\frac 1M g_2} \\left( z - \\frac 1M \\nabla _2 f(y, z)\\right) \\end{split} $$\n显然，我们有\n$$ T_M(x) = (T_M^1(x), T_M^2(x)) $$\n现在，我们给出梯度映射的定义（对任意 $M\u0026gt;0$）：\n$$ G_M(x) = M(x - T_M(x)) = M\\left(x - \\text{prox}_{\\frac1M g} \\left( x - \\frac1M \\nabla f(x)\\right) \\right) $$\n它将 $x$ 映射成问题（1）目标函数的梯度。根据2.1中近端算子和梯度的关系，可知\n$$ G_M(x) = M\\left(x - \\left(\\left(x - \\frac1M \\nabla f(x)\\right) - \\nabla g(x)\\right)\\right) = \\nabla f(x) + \\nabla g(x) = \\nabla H(x) $$\n也即是问题（1）目标函数在 $x$ 处的梯度。\n类似的，我们定义关于 $y$ 和 $z$ 的偏梯度映射：\n$$ \\begin{split} G_M^1(x) \u0026amp;= M\\left(y - \\text{prox} _{\\frac1M g_1} \\left( y - \\frac1M \\nabla f(y, z)\\right) \\right)\\newline G_M^2(x) \u0026amp;= M\\left(z - \\text{prox} _{\\frac1M g_2} \\left( z - \\frac1M \\nabla f(y, z)\\right) \\right) \\end{split} $$\n显然，我们有\n$$ G_M(x) = (G_M^1(x), G_M^2(x)) $$\n梯度映射可以用来衡量解的最优性。注意到，$G_M(x)=0$ 对某个 $M\u0026gt;0$ 成立当且仅当 $x$ 是问题（1）的最优解；$x$ 是问题（1）的最优解当且仅当 $G_{M_1}^1(x)=0$ 和 $G_{M_2}^2(x)=0$ 对某个 $M_1,M_2\u0026gt;0$ 成立。此外，若 $G_M(x)=0$ 对某个 $M\u0026gt;0$ 成立，则$G_M(x)=0$ 对所有 $M\u0026gt;0$ 成立。如果我们移除 $f$ 凸的约束，那 $G_M(x)=0$ 就表明 $x$ 是平稳点（stationary point）。\n2.3 块下降引理 根据性质【C】和【D】中描述的块-Lipschitz（block-Lipschitz）假设，我们可以给出如下块版本的下降引理（block decent lemma）：\n【引理2】（块下降引理）令$y\\in \\text{dom}g_1$ 和 $z\\in \\text{dom}g_2$，并令 $h\\in \\mathbb{R}^{n_1}$ 使得 $y+h\\in \\text{dom}g_1$，则对任意的 $M \\geq L_1$有\n$$ f(y+h, z) \\leq f(y, z) + \\langle \\nabla_1 f(y, z), h \\rangle + \\frac M2 \\Vert h\\Vert^2 $$\n以及，令$y\\in \\text{dom}g_1$ 和 $z\\in \\text{dom}g_2$，并令 $h\\in \\mathbb{R}^{n_2}$ 使得 $z+h\\in \\text{dom}g_2$，则若 $L_2\u0026lt;\\infty$，对任意的 $M \\geq L_2$有\n$$ f(y, z+h) \\leq f(y, z) + \\langle \\nabla_2 f(y, z), h \\rangle + \\frac M2 \\Vert h\\Vert^2 $$\n2.4 充分下降性质 下面，我们介绍一个重要的结论，即充分下降性质（sufficient decrease property），它和后续的收敛性分析密切相关。设 $s: \\mathbb{R}^p\\mapsto \\mathbb{R}$ 是一个连续、可导且具有基于常数 $L$ 的Lipschtiz梯度的函数，$h: \\mathbb{R}^p \\mapsto (-\\infty, \\infty]$ 是一个闭的、真凸函数。根据Beck和Teboulle（2011）中的引理2.6，对 $S(x) = s(x) + h(x)$，有如下的充分下降性质：\n$$ S(x) - S\\left(\\text{prox}_h\\left(x - \\frac1L \\nabla s(x)\\right)\\right) \\geq \\frac{1}{2L}\\left\\Vert L\\left(x - \\text{prox}_h\\left(x - \\frac1L \\nabla s(x)\\right)\\right) \\right\\Vert^2 $$\n 充分下降性质主要刻画了，当前点 $x$ 和基于 $x$ 利用近端算子找到的下降点 $\\text{prox}_h\\left(x - \\frac1L \\nabla s(x)\\right)$ 的目标函数值的下降值与两点之间的欧氏距离的大小关系。\n 注意到，$\\forall z\\in \\text{dom}g_2$，$f(\\cdot, z)$ 是一个连续、可导且具有基于常数 $L_1$ 的Lipschtiz梯度的函数，则结合偏梯度映射和充分下降性质，有\n$$ H(y, z) - H(T_{L_1}^1(x), z) \\geq \\frac{1}{2L_1}\\Vert G_{L_1}^1(y, z)\\Vert^2 \\qquad (2) $$\n若 $L_2\u0026lt;\\infty$，类似的，我们有\n$$ H(y, z) - H(y, T_{L_2}^2(x)) \\geq \\frac{1}{2L_2}\\Vert G_{L_2}^2(y, z)\\Vert^2 \\qquad (3) $$\n3 收敛性分析 令第 $k$ 次的迭代值为 $x_k = (y_k, z_k)$，“半次”迭代的结果记为\n$$ x_{x+\\frac12} = (y_{k+1}, z_k) $$\n根据AM算法，我们很容易得到\n$$ H(x_0)\\geq H(x_{\\frac12})\\geq H(x_1)\\geq H(x_{\\frac32})\\geq \\cdots $$\n这只是目标函数下降，具体的算法收敛性是怎样的，以下我们将从目标函数非凸和凸两种情形来展开分析讨论。\n3.1 非凸情形 本小节讨论 $f$ 非凸的情形。根据2.3中的充分下降性质，有如下引理：\n【引理3】令 ${x_k}_{k\\geq0}$ 是AM算法产生的序列，则对任意 $k\\geq0$ 成立着以下不等式：\n$$ H(x_{k}) - H(x_{k+\\frac12}) \\geq \\frac{1}{2L_1}\\Vert G_{L_1}^1(x_k)\\Vert^2 \\qquad (4) $$\n此外，若 $L_2\u0026lt;\\infty$，则\n$$ H(x_{k+\\frac12}) - H(x_{k+1}) \\geq \\frac{1}{2L_2}\\Vert G_{L_2}^2(x_{k+\\frac12})\\Vert^2 $$\n【证明】将 $y = y_k$ 和 $z = z_k$ 带入到式（2）中得\n$$ H(y_k, z_k) - H(T_{L_1}^1(x_k), z_k) \\geq \\frac{1}{2L_1}\\Vert G_{L_1}^1(y_k, z_k)\\Vert^2 $$\n根据 $T_{L_1}^1$ 的定义，其是固定 $z=z_k$ 让 $H(y, z_k)$ 下降的点，那么必然有\n$$ H(T_{L_1}^1(x_k), z_k) \\geq H(y_{k+\\frac12}, z_k) $$\n两者结合就可以得到引理2中的第一个结论。类似的，不难得到第二个结论。$\\blacksquare$\n接着，我们给出非凸情形的主要收敛结论，即偏梯度的范数区域零的收敛速率。\n【定理1】（偏梯度映射的收敛速率）令 ${x_k}_{k\\geq0}$ 是AM算法产生的序列，则对任意的 $n\\geq1$ 有\n$$ \\min_{k=0,1,\\ldots,n} \\Vert G_{L_1}^1(x_k)\\Vert \\leq \\frac{\\sqrt{2L_1(H(x_0) - H^* )}}{\\sqrt{n+1}} $$\n此外，此外，若 $L_2\u0026lt;\\infty$，则\n$$ \\min_{k=0,1,\\ldots,n} \\Vert G_{L_2}^2(x_k)\\Vert \\leq \\frac{\\sqrt{2L_2(H(x_0) - H^* )}}{\\sqrt{n+1}} $$\n【证明】根据引理2，有\n$$ H(x_k) - H(x_{k+1}) \\geq H(H_{k}) - H(x_{k+\\frac12}) \\geq \\frac{1}{2L_1} \\Vert G_{L_1}^1(x_k)\\Vert^2 $$\n将不等式两侧从 $k=0, 1, \\ldots, n$ 相加，得到\n$$ H(x_0) - H(x_{n+1}) \\geq \\frac{1}{2L_1} \\sum_{k=0}^n \\Vert G_{L_1}^1(x_k)\\Vert^2 $$\n由于\n$$ \\sum_{k=0}^n \\Vert G_{L_1}^1(x_k)\\Vert^2 \\geq (n+1) \\min_{k=0,1,\\ldots,n} \\Vert G_{L_1}^1(x_k)\\Vert^2 $$\n以及 $H(x_{n+1}) \\geq H^*$，可以推出\n$$ \\min_{k=0,1,\\ldots,n} \\Vert G_{L_1}^1(x_k)\\Vert \\leq \\frac{\\sqrt{2L_1(H(x_0) - H^* )}}{\\sqrt{n+1}} $$\n类似的，我们也能推出第二个结论。$\\blacksquare$\n 根据定理1，AM算法得到的序列对应的梯度至少以 $O(1/\\sqrt{n})$ 的速率趋于零，即收敛到平稳点。具体的收敛结论如引理4所述。\n 【引理4】令 ${x_k}_{k\\geq0}$ 是AM算法产生的序列，则 ${x_k}$ 的聚点是问题（1）的平稳点（stationary point）。\n【证明】设 $\\tilde{x}$ 是 ${x_k}$ 的聚点，则存在一个子序列 ${x_{k_n}}$ 收敛到 $\\tilde{x}$。由于 $x_{k+1}$ 是固定 $y=y_{k+\\frac12}$ 优化 $z$ 得到的，因此我们总是有 $G_{L_2}^2(x_{k_n})=0$。于是，根据 $G_{L_2}^2$ 的连续性，可推出 $G_{L_2}^2(\\tilde{x})=0$。另一方面，由于 ${H(x_{\\frac k2})}$ 是单调非增有下界，故存在极限，则 $H(x_k) - H(x_{k+\\frac12})\\rightarrow 0$，根据式（4），易知当 $n\\rightarrow \\infty$ 时 $G_{L_1}^1(x_{k_n})\\rightarrow 0$。根据 $G_{L_1}^1$ 的连续性，可推出 $G_{L_1}^1(\\tilde{x})=0$。综述，因为 $G_{L_1}^1(\\tilde{x})=0$ 和 $G_{L_2}^2(\\tilde{x})=0$，所以可知 $\\tilde{x}$ 是平稳点。$\\blacksquare$\n 在证明的过程中要从子列的角度来，是因为序列 ${x_k}_{k\\geq0}$ 并不是逐步逼近聚点 $\\tilde{x}$的，那么后续用函数的连续性来推出聚点处的梯度为零就不可行。因此，根据聚点的定义，寻找收敛到聚点的子列，以此来论证就能够衔接上了。\n 3.2 凸情形 现在，我们回到 $f$ 是凸函数的情形。我们主要的目的是证明函数序列的收敛速率。首先，我们给出如下具有启发性的引理：\n【引理5】令 ${x_k}_{k\\geq0}$ 是AM算法产生的序列，则对任意 $k \\geq 0$ 有\n$$ H(x_{k + \\frac 12}) - H(x ^ *) \\leq \\Vert G_{L_1}^1(x_k) \\Vert \\cdot \\Vert x_k - x ^ * \\Vert $$\n此外，若 $L_2\u0026lt;\\infty$，则对任意 $k \\geq 0$ 有\n$$ H(x_{k + 1}) - H(x ^ *) \\leq \\Vert G_{L_2}^2(x_{x + \\frac 12}) \\Vert \\cdot \\Vert x_{x + \\frac 12} - x ^ * \\Vert $$\n【证明】注意到\n$$ T_{L_1}(x_k) = ( T_{L_1}^1(x_k), T_{L_1}^2(x_k)) = \\left(T_{L_1}^1(x_k), z_k - \\frac{1}{L_1} G_{L_1}^2(x_k) \\right) = (T_{L_1}^1(x_k), z_k) $$\n上述最后一个等式成立是因为AM算法的序列产生机制表明 $G_{M}^2(x_k) = 0$ 对任意 $M\u0026gt;0, k = 0, 1, \\ldots$ 都成立。基于此，结合块下降引理（引理2）有\n$$ \\begin{split} f(T_{L_1}(x_k)) - f(x ^ *) \u0026amp;\\leq f(x_k) + \\langle \\nabla_1 f(x_k), T_{L_1}^1(x_k) - y_k \\rangle + \\frac{L_1}{2} \\Vert T_{L_1}^1(x_k) - y_k \\Vert^2 - f(x ^ *)\\newline \u0026amp;= f(x_k) + \\langle \\nabla f(x_k), T_{L_1}(x_k) - x_k \\rangle + \\frac{L_1}{2} \\Vert T_{L_1}^1(x_k) - y_k \\Vert^2 - f(x ^ *) \\end{split} $$\n由于 $f$ 是凸函数，则 $f(x_k) - f(x ^ *) \\leq \\langle \\nabla f(x_k), x_k - x ^ * \\rangle$，结合上式得\n$$ f(T_{L_1}(x_k)) - f(x ^ *) \\leq \\langle \\nabla f(x_k), T_{L_1}(x_k) - x ^ * \\rangle + \\frac{L_1}{2} \\Vert T_{L_1}^1(x_k) - y _k \\Vert^2 \\qquad (5) $$\n另一方面，由于\n$$ T_{L_1}(x_k) = \\text{prox}_{\\frac {1}{L_1} g} \\left( x_k - \\frac {1}{L_1} \\nabla f(x_k)\\right) $$\n根据引理1，令 $h = g$，$M = L_1$，$x = x_k - \\frac {1}{L1} \\nabla f(x_k)$，$w = T_{L_1}(x_k)$ 和 $u = x ^ *$，则\n$$ g(x ^ *) \\geq g(T_{L_1}(x_k)) + L_1 \\left\\langle x_k - \\frac {1}{L1} \\nabla f(x_k) - T_{L_1}(x_k), x ^ * - T_{L_1}(x_k) \\right\\rangle \\qquad (6) $$\n由于 $H(x_{x + \\frac 12}) \\leq H(T_{L_1}(x_k))$，结合式（5）和式（6）可知\n$$ \\begin{split} H(x_{k + \\frac 12}) - H(x ^ *) \u0026amp;\\leq H(T_{L_1}(x_k)) - H(x ^ *) = f(T_{L_1}(x_k)) + g(T_{L_1}(x_k)) - f(x ^ *) - g(x ^ *)\\newline \u0026amp;\\leq \\langle \\nabla f(x_k), T_{L_1}(x_k) - x ^ * \\rangle + \\frac{L_1}{2} \\Vert T_{L_1}^1(x_k) - y_k \\Vert^2 + L_1 \\left\\langle x_k - \\frac {1}{L1} \\nabla f(x_k) - T_{L_1}(x_k), T_{L_1}(x_k) - x ^ * \\right\\rangle \\newline \u0026amp;= L_1 \\left\\langle x_k - T_{L_1}(x_k), T_{L_1}(x_k) - x ^ * \\right\\rangle + \\frac{L_1}{2} \\Vert T_{L_1}^1(x_k) - y_k \\Vert^2 \\newline \u0026amp;= \\left\\langle G_{L_1}(x_k), T_{L_1}(x_k) - x ^ * \\right\\rangle + \\frac{1}{2L_1} \\Vert G_{L_1}^1(x_k) \\Vert^2 \\newline \u0026amp;= -\\frac{1}{L_1} \\Vert G_{L_1}(x_k) \\Vert^2 + \\left\\langle G_{L_1}(x_k), x_k - x ^ * \\right\\rangle + \\frac{1}{2L_1} \\Vert G_{L_1}^1(x_k) \\Vert^2 \\newline \u0026amp;\\leq \\left\\langle G_{L_1}(x_k), x_k - x ^ * \\right\\rangle \\newline \u0026amp;\\leq \\Vert G_{L_1}(x_k) \\Vert \\cdot \\Vert x_k - x ^ * \\Vert \\newline \u0026amp;= \\Vert G_{L_1}^1(x_k) \\Vert \\cdot \\Vert x_k - x ^ * \\Vert \\end{split} $$\n也就是引理5中得第一个结论。若 $L_2 \u0026lt; \\infty$，通过定义函数\n$$ \\tilde{f}(z, y) \\equiv f(y, z) $$\n和初始点 $(z^0, y^1)$，我们按照上述分析可以得到引理中得第二个结论。$\\blacksquare$\n 引理5中得第二个结论要求 $L_2\u0026lt;\\infty$。然而，若函数 $z \\mapsto f(y_{k+1}, z)$ 具有常数为 $M(y_{k+1})$ 的Lipschitz梯度，则准确的结论为 $$ H(x_{k + 1}) - H(x ^ *) \\leq \\Vert G_{M(y_{k+1})}(x_{x + \\frac 12}) \\Vert \\cdot \\Vert x_{x + \\frac 12} - x ^ * \\Vert $$ 该结论对 $L_2 = \\infty$ 同样成立。\n 下面引入一些概念以便于后续的分析。假定水平集（level set）\n$$ S = {x \\in \\text{dom}g_1 \\times g_2: H(x) \\leq H(x_0)} $$\n是紧集，且用 $R$ 记如下的“直径”：\n$$ R = \\max_{x\\in \\mathbb{R}^{n_1 \\times n_2}} \\max_{x ^ * \\in X ^ *} {\\Vert x - x ^ * \\Vert: H(x) \\leq H(x_0)} $$\n特别地，根据 ${H_{(\\frac k2)}}_{k\\geq0}$ 的单调性可知\n$$ \\Vert x_k - x ^ * \\Vert, \\Vert x_{k + \\frac 12} - x ^ * \\Vert \\leq R, \\quad k = 0, 1, \\ldots \\qquad (7) $$\n基于上述的符号，我们可以给出如下的AM算法产生的序列对应的函数值的递归不等式。\n【引理6】令 ${x_k}_{k\\geq0}$ 是AM算法产生的序列，则\n$$ H(x_k) - H(x_{k+1}) \\geq \\frac{1}{2\\min {L_1, L_2}R^2} (H(x_{k+1}) - H ^ *)^2 \\qquad (8) $$\n对所有的 $k \\geq 0$ 都成立。\n【证明】根据引理5和式（7）可得\n$$ H(x_{k + \\frac 12}) - H(x ^ *) \\leq \\Vert G_{L_1}^1(x_k) \\Vert R $$\n再根据引理3，得\n$$ \\begin{split} H(x_k) - H(x_{k+1}) \u0026amp;\\geq H(x_k) - H(x_{k+\\frac 12}) \\newline \u0026amp;\\geq \\frac{1}{2L_1}\\Vert G_{L_1}^1(x_k)\\Vert^2 \\newline \u0026amp;\\geq \\frac{(H(x_{k+\\frac 12}) - H ^ *)^2}{2L_1 R^2} \\newline \u0026amp;\\geq \\frac{(H(x_{k+1}) - H ^ *)^2}{2L_1 R^2} \\end{split} $$\n如果 $L_2 = \\infty$，则式（8）必然成立；如果 $L_2 \u0026lt; \\infty$，则根据引理5和式（7）可得\n$$ H(x_{k + 1}) - H(x ^ *) \\leq \\Vert G_{L_2}^2(x_{k+\\frac 12}) \\Vert R $$\n则\n$$ H(x_k) - H(x_{k+1}) \\geq H(x_{k+\\frac 12}) - H(x_{k+1}) \\geq \\frac{1}{2L_2}\\Vert G_{L_2}^2(x_{k+\\frac 12}) \\Vert \\geq \\frac{(H(x_{k+1}) - H ^ *)^2}{2L_2 R^2} $$\n也就证明了结论。$\\blacksquare$\n接下来，我们证明AM算法产生的函数序列具有次线性收敛速率（sublinear rate of convergence）。为了得到该结论，我们还需要如下简单的引理：\n【引理7】令 ${A_k}_{k\\geq 0}$ 是一个非负实数序列，满足\n$$ A_k - A_{k+1} \\geq \\gamma A_{k+1}^2, \\quad k = 0, 1, \\ldots, $$\n和\n$$ A_1 \\leq \\frac{1.5}{\\gamma}, A_2 \\leq \\frac{1.5}{2\\gamma} $$\n其中 $\\gamma$ 是某个正数。那么，\n$$ A_k \\leq \\frac{1.5}{\\gamma}\\frac {1}{k}, \\quad k = 0, 1, \\ldots, $$\n【证明】我们用归纳法来证明上述结论。假设对 $A_k, k \\geq 3$ 成立以上结论，则\n$$ \\gamma A_{k+1}^2 + A_{k+1} \\leq A_k \\leq \\frac{1.5}{\\gamma}\\frac {1}{k} $$\n根据二次函数的性质，易知\n$$ A_{k+1} \\leq \\frac{-1 + \\sqrt{1 + 4\\cdot 1.5/k}}{2 \\gamma} \\leq \\frac{1.5}{\\gamma}\\frac {1}{k+1} $$\n【定理2】令 ${x_k}_{k\\geq0}$ 是AM算法产生的序列，则对任意的 $k \\geq 1$ 有\n$$ H(x_k) - H ^ * \\leq \\frac{3\\max {H(x_0) - H ^ *, \\min {L_1, L_2}R^2}}{k} \\qquad (9) $$\n【证明】令 $A_k = H(x_k) - H ^ *$ 和 $\\tilde{\\gamma} = \\frac{1}{2\\min{L_1, L_2}R^2}$，根据引理6得\n$$ A_k - A_{k+1} \\geq \\tilde{\\gamma} A_{k+1}^2 $$\n显然\n$$ \\begin{split} A_1 \u0026amp;= H(x_1) - H ^ * \\leq H(x_0) - H ^ *\\newline A_2 \u0026amp;\\leq H(x_0) - H ^ * \\end{split} $$\n故令\n$$ \\gamma = \\frac 12 \\min \\left\\lbrace \\frac{1}{H(x_0) - H ^ *}, \\frac{1}{\\min{L_1, L_2}R^2} \\right\\rbrace $$\n则有\n$$ A_1 \\leq \\frac{1.5}{\\gamma}, A_2 \\leq \\frac{1.5}{2\\gamma} $$\n由于 $\\tilde{\\gamma}\\leq \\gamma$，可以得到\n$$ A_k - A_{k+1} \\geq \\gamma A_{k+1}^2 $$\n对所有的 $k \\geq 1$ 都成立，则根据引理7对任意的 $k \\geq 1$ 有\n$$ A_k \\leq \\frac{1.5}{\\gamma}\\cdot \\frac{1}{k} $$\n也即证明了定理的结论。$\\blacksquare$\n 定理2表明，AM算法的函数序列可以达到一个次线性收敛的速率。根据式（9），收敛效率依赖于块Lipschitz常数 $L_1, L_2$ 的最小值，而不是最大值或者全局块Lipschitz常数。这表明AM算法对于更加平滑的函数，也即是有较小的块Lipschitz常数，具有更好的表现。注意到，式（9）依赖于 $H(x_0) - H ^ *$，意味着函数的全局Lipschitz常数对收敛效率也有一个明显的影响，这显然是一个缺陷。然而，在后续的证明中可以看出，对 $H(x_0) - H ^ *$ 依赖很小，对给定收敛误差精度下所需的迭代次数的影响很小。为了得到这个结果，我们需要满足引理7不等式更深入、细致的分析结论，即如下的引理。\n 【引理8】令 ${A_k}_{k\\geq 0}$ 是一个非负实数序列，满足\n$$ A_k - A_{k+1} \\geq \\gamma A_{k+1}^2, \\quad k = 0, 1, \\ldots, $$\n则对任意 $n \\geq 2$ 有\n$$ A_n \\leq \\max \\left\\lbrace \\left( \\frac 12 \\right)^{(n-1)/2} A_0, \\frac{4}{\\gamma (n-1)} \\right\\rbrace $$\n此外，对任意的 $\\epsilon \u0026gt; 0$，若\n$$ n \\geq \\max \\left\\lbrace \\frac{2}{\\ln (2)}(\\ln (A_0) + \\ln (1/\\epsilon)), \\frac {4}{\\gamma \\epsilon} \\right\\rbrace $$\n则 $A_n \\leq \\epsilon$。\n【证明】注意到，对任意的 $k\\geq 0$ 有\n$$ \\frac{1}{A_{k+1}} - \\frac{1}{A_{k}} = \\frac{A_{k} - A_{k+1}}{A_{k}A_{k+1}} \\geq \\gamma\\frac{A_{k+1}}{A_k} $$\n对每一个 $k$，有两种可能：\n $A_{k+1} \\leq \\frac 12 A_{k}$， $A_{k+1} \u0026gt; \\frac 12 A_{k}$。  在第2种可能下，我们有\n$$ \\frac{1}{A_{k+1}} - \\frac{1}{A_{k}} \\geq \\frac{\\gamma}{2} $$\n现在假设 $n$ 是偶数。如果第2种可能发生了至少 $\\frac 12$ 次，那么\n$$ \\frac{1}{A_n} \\geq \\frac{\\gamma n}{4} $$\n即\n$$ A_n \\leq \\frac{4}{\\gamma n} $$\n另一方面，如果第1种可能发生了至少 $\\frac 12$ 次，结合 $A_{k}$ 的单调递减性，那么\n$$ A_n \\leq \\left( \\frac 12\\right)^{n/2} A_0 $$\n因此，我们可以得到\n$$ A_n \\leq \\max \\left\\lbrace \\left( \\frac 12\\right)^{n/2} A_0, \\frac{4}{\\gamma n} \\right\\rbrace \\qquad (10) $$\n如果 $n$ 是奇数，我们可以得到、\n$$ A_n \\leq A_{n-1} \\leq \\max \\left\\lbrace \\left( \\frac 12\\right)^{(n-1)/2} A_0, \\frac{4}{\\gamma (n-1)} \\right\\rbrace \\qquad (11) $$\n比较易知，式（10）的不等式右侧比式（11）不等式右侧小，故引理8的第一个结论成立。为了保证 $A_n \\leq \\epsilon$，只需要求\n$$ \\max \\left\\lbrace \\left( \\frac 12\\right)^{(n-1)/2} A_0, \\frac{4}{\\gamma (n-1)} \\right\\rbrace \\leq \\epsilon $$\n即\n$$ \\left( \\frac 12\\right)^{(n-1)/2} A_0 \\leq \\epsilon, \\frac{4}{\\gamma (n-1)} \\leq \\epsilon $$\n即\n$$ n \\geq \\frac{2}{\\ln (2)} (\\ln (A_0) + \\ln (1/\\epsilon)) + 1, n \\geq \\frac{4}{\\gamma \\epsilon} + 1 $$\n因此，若\n$$ n \\geq \\max \\left\\lbrace \\frac{2}{\\ln (2)} (\\ln (A_0) + \\ln (1/\\epsilon)), \\frac{4}{\\gamma \\epsilon} \\right\\rbrace + 1 $$\n成立，则不等式 $A_n \\leq \\epsilon$ 成立。\n基于引理8，我们现在可以立马得到相较于定理2更加细致的收敛性结果，如定理3所述。\n【定理3】令 ${x_k}_{k\\geq0}$ 是AM算法产生的序列，则对任意的 $k \\geq 2$ 有\n$$ H(x_k) - H ^ * \\leq \\max \\left\\lbrace \\left( \\frac 12\\right)^{(k-1)/2} (H(x_0) - H ^ *), \\frac{8\\min {L_1, L_2}R^2}{\\gamma (k-1)} \\right\\rbrace $$\n此外，要得到 $\\epsilon$-最优解，需要的迭代次数至多为\n$$ \\max \\left\\lbrace \\frac{2}{\\ln (2)} (\\ln (H(x_0) - H ^ *) + \\ln (1/\\epsilon)), \\frac{8\\min {L_1, L_2}R^2}{\\epsilon} \\right\\rbrace + 2 $$\n【证明】根据引理7，我们有\n$$ A_{k} - A_{k+1} \\geq \\gamma A_{k+1}^2 $$\n其中 $A_k = H(x_k) - H ^ *$ 和 $\\gamma = \\frac{1}{2\\min {L_1, L_2}R^2}$。则根据引理8立即可得上述结论。\n 根据定理3，由于 $H(x_0) - H ^ *$ 前是指数型系数，可知 $H(x_0) - H ^ *$ 对于收敛效率的影响很小，即全局Lipschitz常数的影响很小，主要还是块Lipschitz常数的最小值影响大。从 $\\epsilon$-最优解的最大迭代次数也能看出，全局Lipschitz常数的影响很小，远没有块Lipschitz常数的最小值影响大。\n 4 结语 本文主要讨论了凸优化框架下，两块（two-block）变量的交替最小化算法的收敛性。分析表明，在满足某些常见条件下，AM算法可以收敛到最优值，且达到次线性收敛速率。此外，收敛效率受块Lipschitz常数的最小值的影响较大；全局Lipschitz常数对收敛效率也有影响，但很小。总之，AM算法在统计学、机器学习等领域中应用广泛，了解它的收敛理论也很有必要。\n","id":2,"section":"posts","summary":"交替最小化 (alternating minimization) 算法在很多领域中有着广泛的应用场景。本文主要讨论凸优化情形下AM算法的收敛性，内容来源于对Beck（2015）的翻译。 1 问题的","tags":["经典算法理论","优化算法"],"title":"交替最小化：凸优化下的收敛性分析","uri":"https://qkai-stat.github.io/2022/06/am-algorithm-and-its-convergence-analysis/","year":"2022"},{"content":"高维稀疏化技术是大数据分析的重要方法之一。以LASSO为代表的模型选择方法，在过去的20多年间（20世纪90年代以来）得到了众多学者的关注并进行了深入的研究。目前，LASSO类方法的统计理论和求解算法的相关成果较为丰富，仍有大量学者进行相关的研究。这篇博客与其他几篇博客围绕LASSO方法进行系列的介绍，主要包括LASSO的基本概念、变量选择一致性、参数估计的界、主流求解算法等统计理论和优化算法方面的重要结果。\n本文探讨了Zhao和Yu（2006）关于LASSO模型一致性所提出的不可表示性条件（Irrepresentable Condition）。在固定维和发散维的情形下，作者证明了不可表示性条件是LASSO随着样本量增大而选出正确模型的几乎必要和充分条件。本文相关的证明技巧和研究结果都很经典，故学习该理论在研究稀疏化技术中非常必要。\n1 背景介绍 1.1 问题的提出 考虑如下的线性模型： $$ Y = X\\beta^* + \\epsilon \\qquad (1) $$ 其中 $Y = (y_1, \\ldots, y_n)^T$​​ 是 $n \\times 1$​​ 的响应变量，$X = (X_1, \\ldots, X_p) = (x_1^T; \\ldots; x_n^T)$​​ 是 $n\\times p$​​ 的设计矩阵，这里 $X_j$​​ 是第 $j$​​ 个变量，$ x_i $​​ 是第 $i$​​ 个样本。$\\beta^ * = (\\beta_1^ * , \\ldots, \\beta_p^ * )^T$​​ 是回归系数。$\\epsilon = (\\epsilon_1, \\ldots, \\epsilon_n)^T$​​ 均值为0、方差为 $\\sigma ^ 2$​​​ 的独立同分布的随机变量。\n由于实际问题中数据可能包含冗余变量，利用全部变量构建的模型将会存在解释性差的问题。对此，我们需要考虑变量选择，关于这个话题可参见我先前博客的相关介绍。本文主要探讨的是经典的变量选择方法——LASSO——的统计性质，即变量选择一致性。\n假设 $ \\hat{\\beta}^ * $ 是如下LASSO模型 $$ \\begin{equation} \\hat{\\beta}^ * = arg\\min_{\\beta} \\Vert Y - X\\beta\\Vert_2^2 + \\lambda\\Vert \\beta\\Vert_1 \\qquad (2) \\end{equation} $$ 的估计。其中 $\\lambda\u0026gt;0$​ 是调节参数，控制着系数的压缩程度。\n LASSO可以用来进行变量选择，那么它实际模型选择的能力究竟怎样呢？\n 上述的问题便是我们本文所要介绍的LASSO模型一致性。具体而言，对于给定的数据，LASSO是否可以始终以某一非零的概率挑选出正确的模型？如是，这个概率又是否会随着样本量增大而趋于1（一致性）呢？\n1.2 早期工作概述 针对LASSO的参数估计一致性，Knight和Fu（2000）在研究固定维下Bridge估计（比LASSO更一般的估计）的渐近性质（asymptotic properties）时指出，基于一些正则化条件（regularity conditions），当 $\\lambda_n = o(n)$ 时，LASSO估计满足 $\\hat{\\beta}^ * \\rightarrow_p \\beta^ * $，即参数估计一致性。进一步，若 $\\lambda_n = O(n^{\\frac12})$，则LASSO估计正确把零系数压缩到零的概率总是为正。然而对他们的结果进行仔细的计算发现，LASSO估计在零系数处的极限概率小于1（Zhang和Huang，2008），表明在没有恰当条件的保证下，LASSO估计没有变量选择一致性。\n针对LASSO的变量选择一致性，Meinshausen和Buhlmann（2006）在研究高维图模型时证明了，基于某些条件，LASSO可以一致地估计出高斯随机变量之间的相关性，该结果对高维情形也成立。值得一提的是，Meinshausen和Buhlmann（2006）也独立给出了类似不可表示条件的结果，但是他们仅在高斯假设下进行了探讨，而Zhao和Yu（2006）中均值0、方差 $\\sigma^2$​ 的独立同分布的假设更为一般。\nZhao和Yu（2006）主要关注的是，能否在某个确定的条件下，LASSO估计具有变量选择一致性？此外，对于有限样本情形，能否在该确定的条件下，存在一个正确的压缩量（调节参数）使得LASSO估计可以挑选出正确的模型？换句话说，他们所关心的不仅是LASSO估计变量选择的渐近性质，还包括有限样本的具体表现。\n1.3 若干记号与定义 为了后续表述方便，在模型（1）稀疏的前提下，不失一般性，我们进一步假设 $ \\beta^ * $​ 的前 $q$​ 个系数非零，后 $p-q$​ 个系数为零，并令 $ \\beta^ * (1) = (\\beta_1^ * , \\ldots, \\beta_q^ * )^T $​ 和 $ \\beta^ * (2) = (\\beta_{q+1}^ * , \\ldots, \\beta_p^ * )^T $​。相应的，我们再令 $ X(1) $​ 为 $X$​ 的前 $q$​ 列，$ X(2) $​ 为 $X$​ 的后 $p-q$​ 列。此外，我们还令 $ C_n = \\frac1n X^TX $，则 $ C_n $ 有如下的分块矩阵形式： $$ C_n = \\begin{pmatrix} C_n^{11} \u0026amp; C_n^{12}\\newline C_n^{21} \u0026amp; C_n^{22}\\newline \\end{pmatrix} $$ 其中 $ C_n^{11} $ 被假设成可逆的。\n变量选择一致性通常仅关注模型正确识别重要和冗余变量的能力。Zhao和Yu（2006）则考虑了一种更强的变量选择一致性，即符号一致性（sign consistency）。符号一致性指的是，模型正确将正系数估计为正、负系数估计为负以及零系数估计为零的能力。关于符号一致性，我们作如下定义：\n【定义1】（符号一致性）一个估计量 $ \\hat{\\beta}^ * $ 和真实值 $ {\\beta}^ * $ 的符号一致记为 $ \\hat{\\beta}^ * =_{s} \\beta^ * $，当且仅当 $ \\textrm{sign}(\\hat{\\beta}^ * ) = \\textrm{sign}(\\beta^ * ) $ 成立，其中 $\\textrm{sign}(\\cdot)$ 是符号函数。\n【定义2】（强符号一致性）若存在 $ \\lambda_n = f(n) $，使得 $$ \\begin{equation} \\lim_{n\\rightarrow\\infty}P(\\hat{\\beta}^ * (\\lambda_n) =_s \\beta^ * ) = 1 \\end{equation} $$ 则LASSO估计具有强符号一致性（strongly sign consistency），其中 $ f(n) $ 是关于 $ n $ 和 $ Y $ 与 $ X $ 独立的函数。\n【定义3】（弱符号一致性）如果 $$ \\begin{equation} \\lim_{n\\rightarrow\\infty}P(\\exists \\lambda\\geq0, \\hat{\\beta}^* (\\lambda) =_s \\beta^* ) = 1 \\end{equation} $$ 则LASSO估计具有弱符号一致性（general sign consistency）。\n根据定义2，如果强符号一致性成立，则对每个有限的样本，LASSO始终能找到一个相应的调节参数，使得LASSO估计的符号一致性得到一个概率（与样本量 $n$​ 相关，随 $n$​ 增大而趋于1）保证。这说明此时LASSO不仅具有良好的渐近性质，还有较好的有限样本的表现。反观定义3，此时LASSO只有在样本量充分大时才能找到某个合适的调节参数保证符号一致性成立，说明有限样本的表现未知。因此，不难看出“强”、“弱”二字的区别。\n下面给出不可表示性条件的定义：\n【定义4】（强不可表示性条件）存在一个正的常数向量 $ \\eta $​，使得 $$ \\begin{equation} \\vert C_n^{21}(C_n^{11})^{-1}\\text{sign}(\\beta^*(1))\\vert \\leq \\mathbf{1} - \\eta \\end{equation} $$ 其中 $ \\mathbf{1} $ 是长度 $ p-q $ 元素皆为1的向量，上述不等式对每个分量成立。\n【定义5】（弱不可表示性条件） $$ \\begin{equation} \\vert C_n^{21}(C_n^{11})^{-1}\\text{sign}(\\beta^*(1))\\vert \u0026lt; \\mathbf{1}, \\end{equation} $$ 上述不等式对每个分量成立。\n2 证明思路和一个引理 2.1 证明思路 这里所谓的证明思路只是个人体会，不是作者真实的证明思路。通过勾勒整个证明的流程，宏观上把握证明的思路，希望有助于读者理解该证明，而不至于困于一处技术细节，或在看完整个证明后仍感到茫然。\n首先，确定证明的目标——LASSO的符号一致性。将该目标具体表述出来，即在有限样本下，给出LASSO估计符号一致的概率或概率下界（和样本量有关），并证明该值随样本量增大而趋于1。为此，我们自然先要找到LASSO估计符号一致的成立条件：若条件充要且概率易于计算并趋于1，就解决了我们的问题。实际上这样的充要条件一般很难找到，但我们只用找到成立的充分条件即可——这基于如下的事实：若条件2是条件1的充分条件，即条件2 $\\Rightarrow $ 条件1，则 $$ P(\\text{条件}1) \\geq P(\\text{条件}2) $$ 换句话说，若某条件1 $\\Rightarrow $ LASSO估计符号一致，则 $$ P(\\hat{\\beta}^ * =_s \\beta^ * ) \\geq P(\\text{条件}1) $$ 这样将 $P(\\text{条件}1)$ 或其下界计算出便可传递给 $P(\\hat{\\beta}^ * =_s \\beta^ * )$，就解决了上述的问题。若 $P(\\text{条件}1)$ 和其下界都难以计算，则按照上面的讨论，可以通过不断加强条件，直到可以进行计算分析。\n接着，我们根据上述思路迈出第一步：寻找LASSO估计符号一致的成立条件。设想，若 $\\hat{\\beta}^ * =_s \\beta^ * $​ 成立，则 $\\hat{\\beta}^ * $​ 必先满足变量选择一致，即 $\\hat{\\beta}^ * $​ 必然要估计成 $ (\\hat{\\beta}^ * (1)^T, \\mathbf{0}^T)^T $​ 这种形式，其中 $ \\hat{\\beta}^ * (1) $​ 非零。联系LASSO估计和KKT条件的等价性，不难想到，满足符号一致的LASSO估计必然满足 $ \\hat{\\beta}^ * = (\\hat{\\beta}^ * (1)^T, \\mathbf{0}^T)^T$​ 所对应的KKT条件，即 $$ \\begin{gather} X(1)^TX(1)\\hat{\\beta}^ * (1) - X(1)^TY = -\\frac{\\lambda_n}{2}\\text{sign}(\\hat{\\beta}^ * (1)) \\qquad (3)\\newline \\vert X(2)^TX(1)\\hat{\\beta}^ * (1) - X(2)^TY\\vert \\leq \\frac{\\lambda_n}{2}\\mathbf{1} \\qquad (4) \\end{gather} $$ 当然，仅满足KKT条件（3）和（4），只能表明LASSO估计 $\\hat{\\beta}^ * $ 具有变量选择一致性而非符号一致性。为此，我们需要加强KKT条件（3）和（4），使得可以推出 $\\hat{\\beta}^ * (1) =_s \\beta^ * (1) $，即 $\\hat{\\beta}^ * =_s \\beta^ * $ 成立。\n注意到，若 $$ \\vert \\hat{\\beta}^* (1) - \\beta^* (1) \\vert \u0026lt; \\vert \\beta^* (1) \\vert \\qquad (5) $$ 则一定成立 $\\hat{\\beta}^ * (1) =_s \\beta^ * (1) $，即KKT条件（3）和（4）加上不等式（5）便可推出 $\\hat{\\beta}^ * =_s \\beta^ * $ 成立。\n 不难看出式（5）很强，例如，$\\hat{\\beta}_j^ * (1) = 5, \\beta_j^ * (1) = 2$，但不满足（5）式。不过考虑到 $\\hat{\\beta}^ * $ 是 ${\\beta}^ * $ 的估计，$ \\vert \\hat{\\beta}^* (1) - \\beta^* (1) \\vert $ 通常不会很大；且线性关系强时，$\\beta^* (1)$ 一般也不会太小，因此不等式（5）放在这里仍然可以接受。\n 最后，我们对条件（3）、（4）和（5）进行分析计算，从而传递给出 $P(\\hat{\\beta}^ * =_s \\beta^ * )$ 的下界。\n以上就是整个证明的大致脉络。基于（3）、（4）和（5）与 $P(\\hat{\\beta}^ * =_s \\beta^ * )$ 建立的大小关系就是2.2节所给出的引理。对条件（3）、（4）和（5）分析其成立的概率就是后续各个情形下符号一致性定理所探讨的内容。下面，我们先给出符号一致的概率下界的引理。\n2.2 符号一致的概率下界 【引理1】 假定强不可表示性条件对某个 $ \\eta\u0026gt;0 $ 成立，则\n$$ P(\\hat{\\beta}^* (\\lambda_n) =_s \\beta^* ) \\geq P(A_n \\cap B_n) $$\n这里\n$$ \\begin{split} A_n \u0026amp;= \\left\\lbrace \\vert (C_n^{11})^{-1}W_n(1)\\vert \u0026lt; \\sqrt{n}\\left(\\vert \\beta^* (1)\\vert - \\frac{\\lambda_n}{2n}\\vert (C_n^{11})^{-1}\\text{sign}(\\beta^* (1))\\vert \\right) \\right\\rbrace \\newline B_n \u0026amp;= \\left\\lbrace \\vert C_n^{21}(C_n^{11})^{-1}W_n(1) - W_n(2)\\vert \\leq \\frac{\\lambda_n}{2\\sqrt{n}}\\eta \\right\\rbrace \\end{split} $$\n其中 $ W_n(1) = \\frac{1}{\\sqrt{n}}X(1)^T\\epsilon $ 和 $ W_n(2) = \\frac{1}{\\sqrt{n}}X(2)^T\\epsilon $。\n【证明】 根据2.1节的讨论，在条件（3）、（4）和（5）下，LASSO估计 $\\hat{\\beta}^ * $ 具有符号一致性。根据1.3节的记号，我们对（3）和（4）进行等价变换有\n$$ \\begin{gather} \\sqrt{n}C_n^{11}(\\hat{\\beta}^* (1) - \\beta^ * (1)) - W_n(1) = -\\frac{\\lambda_n}{2\\sqrt{n}}\\text{sign}(\\hat{\\beta}^ * (1)) \\qquad (6)\\newline \\vert \\sqrt{n}C_n^{21}(\\hat{\\beta}^*(1) - \\beta^ * (1)) - W_n(2)\\vert \\leq \\frac{\\lambda_n}{2\\sqrt{n}}\\mathbf{1} \\qquad (7) \\end{gather} $$\n其中用到 $ Y = X\\beta^* + \\epsilon = X(1)\\beta^* (1) + \\epsilon $ 的关系。\n由于 $C_n^{11}$ 可逆，我们将 $(\\hat{\\beta}^ * (1) - \\beta^ * (1))$​ 从（6）中求解出来，即\n$$ (\\hat{\\beta}^* (1) - \\beta^* (1)) = \\frac{1}{\\sqrt{n}}(C_n^{11})^{-1}\\left(W_n(1)-\\frac{\\lambda_n}{2\\sqrt{n}}\\text{sign}(\\hat{\\beta}^* (1))\\right) \\qquad (8) $$\n将（8）与（5）结合便可得到LASSO估计的非零系数部分符号一致成立的条件：\n$$ \\left\\vert (C_n^{11})^{-1}\\left(W_n(1)-\\frac{\\lambda_n}{2\\sqrt{n}}\\text{sign}(\\hat{\\beta}^* (1))\\right)\\right\\vert \\leq \\sqrt{n}\\vert \\beta^* (1)\\vert \\qquad (9) $$\n对（9），根据三角不等式我们有\n$$ \\left\\vert (C_n^{11})^{-1}\\left(W_n(1)-\\frac{\\lambda_n}{2\\sqrt{n}}\\text{sign}(\\hat{\\beta}^* (1))\\right)\\right\\vert \\leq \\vert (C_n^{11})^{-1}W_n(1)\\vert + \\frac{\\lambda_n}{2\\sqrt{n}}\\vert (C_n^{11})^{-1}\\text{sign}(\\hat{\\beta}^* (1))\\vert $$\n显然，若\n$$ \\vert (C_n^{11})^{-1}W_n(1)\\vert + \\frac{\\lambda_n}{2\\sqrt{n}}\\vert (C_n^{11})^{-1}\\text{sign}(\\hat{\\beta}^* (1))\\vert \\leq \\sqrt{n}\\vert \\beta^* (1)\\vert \\qquad (10) $$\n成立，我们必然可以推出（9）成立，即LASSO估计符号一致性。将（10）稍作整理，得到\n$$ \\vert (C_n^{11})^{-1}W_n(1)\\vert \\leq \\sqrt{n}\\left(\\vert \\beta^* (1)\\vert - \\frac{\\lambda_n}{2n}\\vert (C_n^{11})^{-1}\\text{sign}(\\beta^* (1)\\vert\\right) \\qquad (11) $$\n在（11）中，我们把 $\\text{sign}(\\hat{\\beta}^ * (1))$ 替换成 $\\text{sign}({\\beta}^ * (1))$，因为此时符号一致性已经成立。注意到，（11）的结果正是引理1中 $A_n$ 的定义内容。换句话说，$A_n$ 成立即可推出LASSO估计和非零系数符号一致。\n给定 $A_n$，我们有 $\\hat{\\beta}^ * (1) =_s \\beta^ * (1) $，此时将（8）带入到（7）中，得到 $$ \\left\\vert C_n^{21}(C_n^{11})^{-1}\\left(W_n(1)-\\frac{\\lambda_n}{2\\sqrt{n}}\\text{sign}({\\beta}^ * (1))\\right) - W_n(2)\\right\\vert \\leq \\frac{\\lambda_n}{2\\sqrt{n}}\\mathbf{1} $$ 利用三角不等式易知，若 $$ \\vert C_n^{21}(C_n^{11})^{-1}W_n(1) - W_n(2)\\vert \\leq \\frac{\\lambda_n}{2n}\\left(\\mathbf{1} - \\vert C_n^{21}(C_n^{11})^{-1}\\text{sign}({\\beta}^ * (1))\\vert \\right) \\qquad (12) $$ 成立，则LASSO估计可将 $\\hat{\\beta}^ * (2)$ 压缩到零。对（12）式进一步考虑，若强不可表示性条件对某个 $\\eta \\geq \\mathbf{0}$ 成立，则有 $\\mathbf{1} - \\vert C_n^{21}(C_n^{11})^{-1}\\text{sign}({\\beta}^ * (1))\\vert \\geq \\eta$，那么若 $$ \\vert C_n^{21}(C_n^{11})^{-1}W_n(1) - W_n(2)\\vert \\leq \\frac{\\lambda_n}{2n} \\eta \\qquad (13) $$ 成立，则必然成立（12）式。注意到（13）就是引理1中 $B_n$ 所定义的内容，也就是说，$B_n$ 在 $A_n$ 给定时对应LASSO正确将系数压缩到零的事件。\n综上所述，当强不可表示性条件对某个 $\\eta \\geq \\mathbf{0}$ 成立时， $A_n\\cap B_n \\Rightarrow \\hat{\\beta}^ * (\\lambda_n) = _ s \\beta^ * $，必有 $P(\\hat{\\beta}^ * (\\lambda_n) = _ s \\beta^ * ) \\geq P(A_n \\cap B_n)$，也就证明了引理1的结论。$\\blacksquare$\n3 固定维的LASSO符号一致性 在第2节中，我们探讨了LASSO估计符号一致性的证明思路，即先寻找 $\\hat{\\beta}^ * (\\lambda_n) =_s \\beta^ * $ 成立的充分条件，再通过计算充分条件成立的概率（概率下界）传递得到 $P(\\hat{\\beta}^ *(\\lambda_n) =_s \\beta^ * ) $ 的下界。特别的，我们寻找到了LASSO估计符号一致性成立的充分条件，即 $A_n\\cap B_n$。本节，我们将利用引理1作为桥梁，得到固定维的LASSO估计符号一致性。在陈述主要结论之前，我们先给出将用到的两个正则化条件：\nRC1：$ \\lim_{n\\rightarrow \\infty} C_n = C $，其中 是正定的；\nRC2：$ \\lim_{n\\rightarrow \\infty} \\frac1n \\max_{1\\leq i\\leq n}(x_i^Tx_i) = 0$；\n上面的两个正则化条件在Knight和Fu（2000）中已引入。在低维情形下，设计阵通常列满秩，因此RC1很容易满足。RC2要求每个样本的 $l_2$ 范数有限，因为是固定维情形且设计阵为常数，所以RC2也很容易满足。当然，若考虑随机设计阵，在样本都满足i.i.d.的有限二阶矩的分布，则RC1和RC2同样很容易满足。\n3.1 强不可表示性条件与强符号一致性 在1.2小节中我们说过，在没有适当的条件保证下，LASSO估计不具备变量选择一致性。下面的定理表明，当强不可表示性条件成立时，LASSO估计一定满足符号一致性，且有限维下符号一致成立的概率随着样本量 $n$ 的增大呈指数级增大到1。\n【定理1】 固定 $q, p$ 和 $\\beta^* $，且RC1和RC2同时满足，如果强不可表示性条件成立，则LASSO估计具有强符号一致性。具体而言，当强不可表示性条件成立，$\\forall \\lambda_n$ 满足 $\\lambda_n/n \\rightarrow 0$ 和 $\\lambda_n/ n^{\\frac{1+c}{2}} \\rightarrow \\infty$，其中 $0\\leq c\\leq 1$，我们有\n$$ P(\\hat{\\beta}^* (\\lambda_n) =_s \\beta^* ) = 1 - o(e^{-n^{c}}) $$\n【证明】 为讨论方便，现作如下符号规定：\n$$ \\begin{split} z_n \u0026amp;= (z_n^1, \\ldots, z_n^q)^T = (C_n^{11})^{-1}W_n(1) \\newline \\zeta_n \u0026amp;= (\\zeta_n^1, \\ldots, \\zeta_n^{p-1})^T = C_n^{21}(C_n^{11})^{-1}W_n(1)-W_n(2) \\newline b_n \u0026amp;= (b_n^1, \\ldots, b_n^n)^T = (C_n^{11})^{-1}\\text{sign}(\\beta^*(1)) \\end{split} $$\n结合第2节的讨论，我们有\n$$ P(\\hat{\\beta}^* (\\lambda_n) =_s \\beta^* ) \\geq P(A_n \\cap B_n) $$\n根据德·摩根定律，易知 $(A_n\\cap B_n)^c = A_n^c \\cup B_n^c$，则\n$$ \\begin{split} 1 - P(A_n\\cap B_n) \u0026amp;= P((A_n\\cap B_n)^c) = P(A_n^c \\cup B_n^c) \\newline \u0026amp;\\leq P(A_n^c) + P(B_n^c) \\newline \u0026amp;= \\sum_{i=1}^{q}P\\left(\\vert z_n^i\\vert \\geq \\sqrt{n}\\left(\\vert \\beta_i^*\\vert - \\frac{\\lambda_n}{2n} \\vert b_n^i\\vert \\right)\\right) + \\sum_{i=1}^{p-q}P\\left(\\vert \\zeta_n^i\\vert \\geq \\frac{\\lambda_n}{2\\sqrt{n}}\\eta_i\\right) \\end{split} $$\nKnight和Fu（2000）证明了，在RC1和RC2满足下，我们有下面的极限结论：\n$$ (C_n^{11})^{-1}W_n(1) \\rightarrow_d N(\\mathbf{0}, (C^{11})^{-1}) $$\n和\n$$ C_n^{21}(C_n^{11})^{-1}W_n(1)-W_n(2) \\rightarrow_d N(\\mathbf{0}, C^{22} - C^{21}(C^{11})^{-1}C^{12}) $$\n也即是所有的随机变量 $ z_n^i $ 和 $ \\zeta_n^i $ 都依分布收敛于均值0和有限方差 $ E(z_n^i)^2, E(\\zeta_n^i)^2 \\leq s^2 $ 的正态随机变量，其中 $ s \u0026gt;0 $ 是某个常数。基于上述渐近分布的结果，我们可以分析 $P(A_n \\cap B_n)$​ 成立的概率。在分析之前，我们先给出下面关于正态分布尾部概率的不等式：\n$$ 1 - \\Phi(t) \u0026lt; t^{-1}e^{-\\frac12 t^2} $$\n对上述结果我们作一个简单的说明。根据分部积分法，易推得下面的结果\n$$ \\begin{split} 1 - \\Phi(t) \u0026amp;= \\int_{t}^{+\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac12 x^2}\\text{d}x \\leq \\int_{t}^{+\\infty} \\frac{1}{x}\\cdot xe^{-\\frac12 x^2}\\text{d}x\\newline \u0026amp;= \\left.\\left(-\\frac{1}{x}e^{-\\frac12 x^2}\\right)\\right\\vert_{x=t}^{+\\infty} - \\int_{t}^{+\\infty} \\left(-\\frac{1}{x^2}\\right)\\cdot (-e^{-\\frac12 x^2})\\text{d}x\\newline \u0026amp;= t^{-1}e^{-\\frac12 t^2} - \\int_{t}^{+\\infty} \\frac{1}{x^2}\\cdot e^{-\\frac12 x^2}\\text{d}x\\newline \u0026amp;\\leq t^{-1}e^{-\\frac12 t^2} \\end{split} $$\n当 $ \\frac{\\lambda_n}{n}\\rightarrow 0 $，$ \\frac{\\lambda_n}{n^{\\frac{1+c}{2}}} \\rightarrow \\infty $ 且 $ 0 \\leq c \u0026lt; 1 $，以及 $q, p$​ 固定时，我们有\n$$ \\begin{split} \\sum_{i=1}^{q}P\\left(\\vert z_n^i\\vert \\geq \\sqrt{n}\\left(\\vert \\beta_i^ * \\vert - \\frac{\\lambda_n}{2n}\\vert b_n^i\\vert\\right)\\right) \u0026amp;= \\sum_{i=1}^{q}P\\left( \\vert z_n^i\\vert \\geq \\sqrt{n}\\vert \\beta_i^ * \\vert\\left(1 + \\frac{\\lambda_n}{2n}\\vert\\frac{b_n^i}{\\beta_i^ * }\\vert \\right)\\right)\\newline \u0026amp;\\leq \\sum_{i=1}^{q}P\\left( z_n^i \\geq \\sqrt{n}\\vert \\beta_i^ * \\vert\\left(1 + o(1)\\right)\\right)\\newline \u0026amp;= \\sum_{i=1}^{q} \\left( 1 - \\Phi\\left(\\frac{\\sqrt{n}}{\\bar{s}_i}\\vert \\beta_i^ * \\vert(1 + o(1))\\right) \\right) \\end{split} $$\n其中 $ E(z_n^i)^2 = \\bar{s}_i^2 \u0026lt; s^2, i=1, \\ldots, q $。对上式，进一步可得\n$$ \\begin{split} \\left( 1 - \\Phi\\left(\\frac{\\sqrt{n}}{\\bar{s}_i}\\vert \\beta_i^ * \\vert(1 + o(1))\\right) \\right) \u0026amp;\\leq \\frac{1}{\\frac{\\sqrt{n}}{\\bar{s}_i}\\vert \\beta_i^ * \\vert(1 + o(1))} e^{-\\frac12 \\frac{n}{\\bar{s}_i^2}\\vert \\beta_i^ * \\vert^2(1 + o(1))^2}\\newline \u0026amp;= O(n^{-\\frac12})\\cdot O(e^{-n})\\newline \u0026amp;= O(n^{-\\frac12}e^{-n})\\newline \u0026amp;= o(e^{-n^c}) \\end{split} $$\n因此，我们有\n$$ \\sum_{i=1}^{q}P\\left(\\vert z_n^i\\vert \\geq \\sqrt{n}\\left(\\vert \\beta_i^*\\vert - \\frac{\\lambda_n}{2n}\\vert b_n^i\\vert\\right)\\right) \\leq o(e^{-n^c}) \\qquad (14) $$\n类似的，令 $ E(\\zeta_n^i)^2 = \\tilde{s}_i^2 \u0026lt; s^2, i=1, \\ldots, q $，则有\n$$ \\begin{split} \\sum_{i=1}^{p-q}P\\left(\\vert \\zeta_n^i\\vert \\geq \\frac{\\lambda_n}{2\\sqrt{n}}\\eta_i\\right) \u0026amp;= \\sum_{i=1}^{p-q}\\left(1 - \\Phi\\left(\\frac{1}{\\tilde{s}_i} \\frac{\\lambda_n}{2\\sqrt{n}}\\eta_i\\right)\\right)\\newline \u0026amp;\\leq \\frac{1}{\\frac{1}{\\tilde{s}_i} \\frac{\\lambda_n}{2\\sqrt{n}}\\eta_i}e^{-\\frac12 \\frac{1}{\\tilde{s}_i^2} \\frac{\\lambda_n^2}{4n}\\eta_i^2}\\newline \u0026amp;= o(n^{-\\frac{c}{2}})\\cdot o(e^{-n^c})\\newline \u0026amp;= o(e^{-n^c}) \\end{split} \\qquad (15) $$\n结合（14）和（15），最终可得\n$$ P(\\hat{\\beta}^* (\\lambda_n) =_s \\beta^* ) = 1 - o(e^{-n^c}) $$\n即定理1所陈述的结论。$\\blacksquare$\n3.2 弱不可表示性条件与弱符号一致性 定理1表明，在强不可表示性条件的保证下，LASSO估计具有强符号一致性。值得注意的是，强不可表示性条件只是一个充分性条件。那么，不可表示性条件和LASSO估计的符号一致性还有什么更深的联系呢？下面的定理表明，弱不可表示性条件是LASSO估计满足弱符号一致性（自然也是满足强符号一致性）的必要条件。\n【定理2】 固定 $q, p$ 和 $\\beta^* $，且RC1和RC2同时满足，若LASSO估计满足弱符号一致性，则必存在某个整数 $ N $，使得当 $ n\u0026gt;N $ 时，弱不可表示性条件一定成立。\n【证明】 考虑事件集合 $ F_n^1 $，在该集合中存在某个 $ \\lambda_n $​ 使得\n$$ \\text{sign}(\\hat{\\beta}^ * (1)) = \\text{sign}(\\beta^ *(1))\\quad \\text{and}\\quad \\hat{\\beta}^ * (2) = \\mathbf{0} $$\n那么，LASSO估计的弱符号一致性表明，当 $ n \\rightarrow \\infty $ 时，$ P(F_n^1)\\rightarrow 1 $​ 成立。根据引理1，下面两式\n$$ \\begin{align} C_n^{11}(\\sqrt{n}(\\hat{\\beta}^* (1) - \\beta^ * (1)) - W_n(1)) \u0026amp;= -\\frac{\\lambda_n}{2\\sqrt{n}}\\text{sign}(\\beta^ * (1)) \\qquad (16)\\newline \\vert C_n^{21}(\\sqrt{n}(\\hat{\\beta}^ * (1) - \\beta^ * (1)) - W_n(2)) \\vert \u0026amp;\\leq \\frac{\\lambda_n}{2\\sqrt{n}}\\mathbf{1} \\qquad (17) \\end{align} $$\n在集合 $ F_n^1 $ 上成立。\n由于 $ C_n^{11} $ 可逆，我们可以将 $ (\\hat{\\beta}^* (1) - \\beta^* (1)) $​ 从（16）式中求解出来，将结果带入到（17）式中，得\n$$ F_n^1 \\subset F_n^2 = \\left\\lbrace \\frac{\\lambda_n}{2\\sqrt{n}}L_n \\leq C_n^{21}(C_n^{11})^{-1}W_n(1) - W_n(2) \\leq \\frac{\\lambda_n}{2\\sqrt{n}}U_n \\right\\rbrace $$\n其中\n$$ \\begin{split} L_n \u0026amp;= -\\mathbf{1} + C_n^{21}(C_n^{11})^{-1}\\text{sign}(\\beta^* (1))\\newline U_n \u0026amp;= +\\mathbf{1} + C_n^{21}(C_n^{11})^{-1}\\text{sign}(\\beta^* (1)) \\end{split} $$\n下用反证法证明定理2的结论。假定弱不可表示性条件不成立，则 $\\forall N \u0026gt; 0$ 都存在 $n \u0026gt; N$，使得\n$$ \\vert C_n^{21}(C_n^{11})^{-1}\\text{sign}(\\beta^* (1))\\vert \\geq \\mathbf{1} $$\n成立。不失一般性，假定上述不等式左侧项的第一个元素大于1，即 $ C_n^{21}(C_n^{11})^{-1}\\text{sign}(\\beta^* (1)) \\geq 1 $，则\n$$ [\\frac{\\lambda_n}{2\\sqrt{n}}(L_n)_1, \\frac{\\lambda_n}{2\\sqrt{n}}(U_n)_1] \\subset [0, +\\infty) $$\n对任意 $ \\lambda_n \u0026gt; 0 $ 都成立。由于\n$$ C_n^{21}(C_n^{11})^{-1}W_n(1) - W_n(2) \\rightarrow_d N(\\mathbf{0}, C^{22} - C^{21}(C^{11})^{-1}C^{12}) $$\n说明第一个元素为负的概率始终非零，而集合 $F_n^2$ 定义项的第一个元素要求非负，这表明 $F_n^2$ 成立的概率不会达到1，所以\n$$ \\lim\\inf P(F_n^1) \\leq \\lim\\inf P(F_n^2) \u0026lt; 1 $$\n这和LASSO估计弱符号一致性矛盾，故而假设不成立。因此，弱不可表示性条件就是弱符号一致性的必要条件。$\\blacksquare$\n3.3 若干讨论 3.3.1 定理中一些条件的探讨 由于正则化条件在低（固定）维下较易满足，这里不再讨论。在定理1中，我们用到的两个额外条件：一是不等式（5），保证非零系数符号一致；二是作者所提出的强不可表示性条件，保证LASSO把零系数正确压缩到零。在定理2中，我们用到的一个额外条件是弱不可表示性条件，而没有不等式（5）。这是因为在证明过程中，假设弱不可表示性条件失效，即可推出一般的变量选择一致性不成立，则较之更强的符号一致性必然也不成立。因此，不用考虑不等式（5）。\n在2.1小节中已说过，不等式（5）其实很强，且回顾不等式（5）在定理1的证明中的作用，它对一般变量选择一致性的成立并无影响。为了说明这个问题，我们首先回顾Knight和Fu（2000）的工作。在低（固定）维情形下，当RC1和RC2满足时，Knight和Fu（2000）首先通过LASSO的KKT条件指出，LASSO估计在零系数处为零的概率始终为正。根据（6）和（7），可知LASSO估计在零系数处为零的充要条件是\n$$ \\left\\vert C_n^{21}(C_n^{11})^{-1}\\left(W_n(1)-\\frac{\\lambda_n}{2\\sqrt{n}}\\text{sign}(\\hat{\\beta}^* (1))\\right) - W_n(2)\\right\\vert \\leq \\frac{\\lambda_n}{2\\sqrt{n}}\\mathbf{1} \\qquad (18) $$\n一方面 $P(\\hat{\\beta}^* (2) = \\mathbf{0}) \\geq P(\\hat{\\beta}^* = \\mathbf{0})$，而由上式知 $\\hat{\\beta}^* = \\mathbf{0}$​ 等价于\n$$ \\vert W_n\\vert \\leq \\frac{\\lambda_n}{2\\sqrt{n}}\\mathbf{1} $$\n另一方面，$W_n = \\frac{1}{\\sqrt{n}}X^T\\epsilon \\sim N(\\mathbf{0}, \\sigma^2 C_n)$，则上面的概率恒为正（非零），说明 $P(\\hat{\\beta}^*(2) = \\mathbf{0}) \u0026gt; 0$ 必定成立。这就说明了LASSO的稀疏化能力。\n不过，我们仔细推算（18）式成立的概率，发现它可能比1小，也就是说LASSO没有变量选择一致性。这个结论已经由定理2所证实，即弱不可表示性条件不成立的话，肯定不会成立LASSO的变量选择一致性。 其中的关键就在于，$\\mathbf{1} - \\vert C_n^{21}(C_n^{11})^{-1}\\text{sign}(\\hat{\\beta}^ * (1))\\vert$ 的符号不确定性将导致LASSO把零系数压缩到零的这部分概率无法控制。这也是为什么要强行令 $\\mathbf{1} - \\vert C_n^{21}(C_n^{11})^{-1}\\text{sign}(\\hat{\\beta}^ * (1))\\vert \\geq \\eta$，$\\eta$ 为某个正的向量，就是为了控制相应的概率。从这里可以看出，强不可表示性条件是为了控制LASSO把零系数压缩到零的这部分概率而引入的，不等式（5）则在变量选择一致性的证明中没有起到作用。\n不可表示性条件的引入虽然是为了理论的构建，但是其自身也有一些意义。不妨将冗余变量作为响应变量，重要变量作为自变量，构建两者之间的线性回归模型。那么，对 $X(2)$ 的第 $j$ 列 $X(2)_j$，它关于 $X(1)$ 的回归系数为\n$$ (X(1)^TX(1))^{-1}X(1)^TX(2)_j = (C_n^{11})^{-1}(C_n^{12})_j $$\n其中 $(C_n^{12})_j$ 是 $C_n^{12}$ 的第 $j$ 列。这样，$\\vert C_n^{21}(C_n^{11})^{-1}\\text{sign}(\\hat{\\beta}^ * (1))\\vert$ 的每个分量就是每个冗余变量对 $X(1)$ 的回归系数关于 $\\text{sign}(\\beta^ * (1))$ 的加权和的绝对值。回归系数某种程度可以反映因变量和自变量之间的相关性，要求整体比1小则等价于控制冗余变量和重要变量之间的相关性不能太大。换句话说，冗余变量不能由重要变量表示出来——这也是不可表示性条件名称的由来。当然，为什么对系数求和时要关于 $\\text{sign}(\\beta^ * (1))$ 加权？这\u0026hellip;..就难以解释了~\nKnight和Fu（2000）指出LASSO的稀疏性质后，Meinshausen和Buhlmann（2006）首次给出强不可表示性条件，并在高斯随机变量的情形下证明了LASSO的变量选择一致性，该结论对高维情形也成立。Meinshausen和Buhlmann（2006）这篇文章早在2004年5月就投到AOS上，从时间上来说早于Zhao和Yu（2006）的工作。虽然Zhao和Yu（2006）考虑均值为0、方差为 $\\sigma^2$ 更为一般的随机变量情形，但是这并未对整个证明引起新的困难或提出新的见解。所以，此时再看不等式（5）的加入，它使得变量选择一致性有了新的认识，在证明中也有新的考量（虽然也没难度）。这或许就是不等式（5）的价值吧。\n综上所述，强不可表示性条件是保证LASSO变量选择一致性的关键，不等式（5）则对变量选择一致性提出新的认识。\n3.3.2 关于收敛速率 Knight和Fu（2000）已经指出（文中的定理2），当 $\\lambda_n = O(\\sqrt{n})$ 时，LASSO估计具有 $\\sqrt{n}$-一致性，这里指的是参数估计一致性。然而，根据本文定理1的陈述，LASSO估计的变量选择一致性基于$\\lambda_n/ n^{\\frac{1+c}{2}} \\rightarrow \\infty$，其中 $0\\leq c\\leq 1$。显然，从调节参数 $\\lambda_n$ 的角度看，LASSO参数估计一致性和变量选择一致性不相容。\n事实上，Zou（2006）表明，当 $\\lambda_n/n \\rightarrow 0$ 和 $\\lambda_n/\\sqrt{n}\\rightarrow \\infty$ 时，$\\frac{n}{\\lambda_n}(\\hat{\\beta}^* (\\lambda_n) - \\beta^* )\\rightarrow_p arg\\min (V) $​，其中\n$$ V(u) = u^T C_n u + \\sum_{j=1}^p\\left( u_j\\text{sign}(\\beta_j^* )I(\\beta_j^* \\neq 0) + \\vert u_j\\vert I(\\beta_j^* = 0)\\right) $$\n上面的 $I(\\cdot)$ 是示性函数。这表明，当LASSO保证变量选择一致性时，所对应的估计收敛速率小于 $\\sqrt{n}$。\n通过上面的讨论，一个自然而然的问题就是，有办法使LASSO的 $\\sqrt{n}$-一致性和变量选择一致性同时达到吗？这是可行的，但不在本文的探讨中。感兴趣的读者可以参考Zou（2006）关于自适应（adaptive LASSO）的文章。\n3.3.3 再看OLS估计 结合前面关于LASSO稀疏性的分析可知，LASSO之所以具有稀疏性，是因为 $l_1$ 惩罚衍生出的KKT条件在最优解处具有保证LASSO估计直接为零的条件，即（4）式。OLS也是凸优化，其解自然也有相应的KKT条件，可惜并没有保证估计直接为零的条件。不过，OLS估计优势之一是具有显式解 $(X^TX)^{-1}X^Ty$。套用前文LASSO的分析，$\\hat{\\beta}^{ols} = (X^TX)^{-1}X^Ty \\sim N(\\beta^*, \\sigma^2 (X^TX)^{-1})$，而连续分布在一点处取零（某个值）的概率为零。换句话说，OLS估计的某个元素恰好为零的概率为零，即OLS估计没有稀疏性。\n虽然OLS没有稀疏性，但是OLS通常不会将零系数估计的很大。为了说明这点，不妨考虑正交设计阵情形，则 $\\hat{\\beta}_j^{ols} \\sim N(\\beta_j^* , \\sigma^2)$。当 $\\beta_j^* = 0$ 时，根据正态随机变量的“3 $\\sigma$ 准则”，OLS估计值基本集中在零点附近。\n进一步，上述的分析建立在误差项为连续分布的前提下。假如人为设定误差项在零点处具有某个正的概率，那么根据上面的逻辑，OLS将零系数估计为零的概率将大于零。为了验证这个观点，不妨假设设计阵正交，考虑 $$ \\begin{array}{} X \\in \\mathbb{R}^{1000\\times8}, X^TX = I, \\beta^* = (1,-1,1,-1,0,0,0,0)^T \\newline P(\\epsilon = 0) = p, P(\\epsilon \\sim N(0,1)) = 1 - p \\end{array} $$ 其中 $p$ 是随机误差在零点处的概率。利用R，我们编写下面的程序来查看OLS估计结果。\n# 随机误差产生函数 epsfun \u0026lt;- function(p){ if(runif(1) \u0026lt;= p){ return(0) }else{ return(rnorm(1)) } } samp \u0026lt;- 8 samn \u0026lt;- 1000 beta \u0026lt;- c(rep(c(1, -1), 2), rep(0, 4)) x \u0026lt;- matrix(rnorm(samp*samn), ncol = samp) x_qr \u0026lt;- qr.Q(qr(x)) ols0 \u0026lt;- function(){# 不同的p对应的零系数估计值的绝对均值 coef0_est \u0026lt;- function(p){# 概率p对应的零系数估计值的绝对均值 y \u0026lt;- x_qr%*%beta + unlist(lapply(rep(p, samn), epsfun)) mydata \u0026lt;- data.frame(x_qr, y) colnames(mydata) \u0026lt;- c(paste0(\u0026quot;x\u0026quot;, 1:samp), \u0026quot;y\u0026quot;) lmfit \u0026lt;- lm(y ~ .+0, data = mydata) beta_hat \u0026lt;- coef(lmfit) abssum0 \u0026lt;- mean(abs(beta_hat[5:8])) return(abssum0) } return(unlist(lapply(c(0, 0.1, 0.5, 0.9, 0.99, 0.999, 0.9999), coef0_est))) }  以上 $p = (0, 0.1, 0.5, 0.9, 0.99, 0.999, 0.9999)^T$，我们对每个 $p$ 重复实验500次，将不同的 $p$ 对应的OLS在零系数处估计的绝对均值（后四个系数估计的绝对均值）画出箱线图，如下所示：\n可见，随着误差在零点处的概率增大，OLS在零系数处估计逐渐趋于零，这几乎印证了上面的假设。不过，由于OLS没有LASSO可以保证系数严格压缩到零的条件，OLS估计实际无法（很难）得到严格零解。另一方面，误差项也不会集中分布在零点处。总之，OLS估计没有稀疏性。\n现在我们来讨论OLS估计的符号一致性。通常，我们对OLS估计的直观感受是，OLS没有稀疏性，但似乎对非零系数的符号估计地较准确。那么，OLS估计符号一致性具体怎样呢？不妨再次假设设计阵正交，根据不等式（5），若某个系数 $\\hat{\\beta}_j^{ols} (1)$ 满足符号一致的概率，则\n$$ \\begin{split} P(\\hat{\\beta}_j^{ols}(1) =_s \\beta_j^ * (1)) \u0026amp;\\geq P(\\vert \\hat{\\beta}_j^{ols}(1) - \\beta_j^ * (1) \\vert \u0026lt; \\vert \\beta_j^ * (1)\\vert) \\newline \u0026amp; = 1 - 2\\Phi(-\\vert \\beta_j^ * (1)\\vert/\\sigma) \\end{split} $$ 上式说明，当真实系数绝对值相对随机误差的标准差越大时，OLS估计的符号一致性越容易满足。\n下面我们用前文的模型来进行验证。当 $\\beta^* = (3,-3,3,-3,3,-3,3,-3)^T$ 时，OLS的估计（此时误差服从标准正态分布，下同）结果为\nCoefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) x1 3.536 1.010 3.501 0.000485 *** x2 -4.359 1.010 -4.314 1.76e-05 *** x3 3.740 1.010 3.702 0.000226 *** x4 -3.672 1.010 -3.634 0.000293 *** x5 2.725 1.010 2.697 0.007108 ** x6 -3.351 1.010 -3.317 0.000942 *** x7 2.851 1.010 2.822 0.004865 ** x8 -2.827 1.010 -2.798 0.005234 **  可见OLS估计符号一致性满足的很好，而且每个系数估计的显著性也很高。\n当 $\\beta^* = (1,-1,1,-1,1,-1,1,-1)^T$​ 时，OLS的估计结果为\nCoefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) x1 1.42236 1.01404 1.403 0.1610 x2 0.02772 1.01404 0.027 0.9782 x3 0.44146 1.01404 0.435 0.6634 x4 -0.33551 1.01404 -0.331 0.7408 x5 1.69184 1.01404 1.668 0.0955 . x6 -0.25667 1.01404 -0.253 0.8002 x7 1.67494 1.01404 1.652 0.0989 . x8 -0.55520 1.01404 -0.548 0.5841  其中 $x_2$ 的系数符号就出现了错误，OLS估计的显著性下降很多。\n当 $\\beta^* = 0.1\\times(1,-1,1,-1,1,-1,1,-1)^T$ 时，OLS的估计结果为\nCoefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) x1 -0.5021 0.9898 -0.507 0.612 x2 -0.8301 0.9898 -0.839 0.402 x3 -0.2553 0.9898 -0.258 0.796 x4 0.7577 0.9898 0.765 0.444 x5 1.1240 0.9898 1.136 0.256 x6 -0.1385 0.9898 -0.140 0.889 x7 0.5300 0.9898 0.535 0.592 x8 -0.9893 0.9898 -0.999 0.318  此时很多系数的符号没有估计对，OLS估计都不显著。\n综上所述，OLS估计的符号一致性并不总能够保证地很好，具体和真实系数绝对值相对随机误差标准差的大小有关。\n4 发散维的LASSO符号一致性 4.1 发散维的假设条件 前面的讨论都是固定维情形，即当样本量 $n\\rightarrow\\infty$ 时，维数 $p$ 和非零系数的个数 $q$ 保持不变。本节我们讨论LASSO估计量在发散维情形下的变量选择一致性。所谓发散维，即当样本量 $n\\rightarrow\\infty$ 时，维数 $p$ 和非零系数的个数 $q$ 也随之增长。因此，前面关于模型的假设不再适用，我们转而考虑下面的假设：\n存在 $0\\leq c_1\u0026lt; c_2\\leq 1$ 和 $M_1, M_2, M_3, M_4 \u0026gt;0$​，使得下面的式子成立：\n$$ \\begin{array}{r} \\frac{1}{n} X_i^T X_i \\leq M_1,, \\forall i, \\qquad (19) \\newline \\alpha^T C_n^{11} \\alpha \\geq M_2,, \\forall \\Vert\\alpha\\Vert_2^2 = 1, \\qquad(20) \\newline q = O(n^{c_1}), \\qquad (21) \\newline n^{\\frac{1 - c_2}{2}} \\underset{i=1, \\ldots, q}{\\min} \\vert \\beta_i\\vert \\geq M_3,, \\qquad (22) \\end{array} $$\n实际中常常对数据进行标准化处理，每列（变量）均值为零、方差为1，因此条件（19）很容易满足。（20）式则表明，重要变量对应的Gram矩阵具有良好的可逆性。条件（21）表明，随着样本量 $n$ 的增大，非零系数的个数以 $n^{c_1}$ 的同阶速率增长，这和固定维情形截然不同。我们知道，OLS估计具有理想的收敛速率，即 $\\sqrt{n}$-收敛。对OLS估计，我们有\n$$ \\hat{\\beta}^{ols} = (X^TX)^{-1}X^Ty = \\beta^* + \\frac{1}{\\sqrt{n}}W_n \\overset{n\\rightarrow\\infty}{\\longrightarrow_p} \\beta^* $$\n换句话说，估计的残差按照 $n^{-\\frac12}$ 的速率收敛到零。 根据前面的讨论，LASSO的非零估计部分也和 $W_n$ 密切相关。因此，式（22）表明非零系数绝对值最小的项趋于零的速率与 $\\sqrt{n}$ 之间差了一个 $n^{c_2}$，即与残差有一定的区分度才能让LASSO正确识别出。联系（21）式进一步可以看出，非零系数绝对值最小的项与残差趋于零的速率差 $n^{c_2}$ 要比非零系数个数的增长速率的 $\\sqrt{q}$ 要大，这可以在控制模型参数的同时控制LASSO估计的偏度。\n4.2 有限 $2k$ 阶矩噪声的情形 基于4.1小节的假设，我们有如下的定理：\n【定理3】 假设 $\\epsilon_i$ 是iid的随机噪声，并具有有限的 $2k$ 阶矩，即 $E(\\epsilon_i^{2k}) \u0026lt; \\infty$ 对某个整数 $k\u0026gt;0$ 成立。若（19）-（22）都成立，则强不可表示性条件可以推出LASSO在 $p = o(n^{(c_2 - c_1)k})$ 时的强符号一致性。特别的，对任意 $\\lambda_n$ 满足 $\\frac{\\lambda_n}{\\sqrt{n}}=o(n^{\\frac{c_2 - c_1}{2}})$ 和 $\\frac{1}{p}(\\frac{\\lambda_n}{\\sqrt{n}})^{2k}\\rightarrow \\infty$​，我们有 $$ P(\\hat{\\beta}^* (\\lambda_n) =_s \\beta^* ) \\geq 1 - O\\left(\\frac{pn^k}{\\lambda_n^{2k}} \\right)\\rightarrow 1 $$\n【证明】 类似定理1的证明，我们有\n$$ \\begin{split} 1 - P(A_n\\cap B_n) \u0026amp;= P((A_n\\cap B_n)^c) = P(A_n^c \\cup B_n^c) \\newline \u0026amp;\\leq P(A_n^c) + P(B_n^c) \\newline \u0026amp;= \\sum_{i=1}^{q}P\\left(\\vert z_n^i\\vert \\geq \\sqrt{n}\\left(\\vert \\beta_i^* \\vert - \\frac{\\lambda_n}{2n} \\vert b_n^i\\vert \\right)\\right) + \\sum_{i=1}^{p-q}P\\left(\\vert \\zeta_n^i\\vert \\geq \\frac{\\lambda_n}{2\\sqrt{n}}\\eta_i\\right) \\end{split} $$\n其中\n$$ \\begin{split} z_n \u0026amp;= (z_n^1, \\ldots, z_n^q)^T = (C_n^{11})^{-1}W_n(1) \\newline \\zeta_n \u0026amp;= (\\zeta_n^1, \\ldots, \\zeta_n^{p-1})^T = C_n^{21}(C_n^{11})^{-1}W_n(1)-W_n(2) \\newline b_n \u0026amp;= (b_n^1, \\ldots, b_n^n)^T = (C_n^{11})^{-1}\\text{sign}(\\beta^* (1)) \\end{split} $$\n现在，我们令 $z_n^i = H_A^T \\epsilon$，其中 $H_A^T = (h_a^1, \\ldots, h_a^q)^T = (C_n^{11})^{-1}(n^{-\\frac12}X(1)^T)$​，则\n$$ H_A^T H_A = (C_n^{11})^{-1}(n^{-\\frac12}X(1))((C_n^{11})^{-1}(n^{-\\frac12}X(1)))^T = (C_n^{11})^{-1} $$\n考虑坐标向量 $e_i = (0, \\ldots, 1, \\ldots, 0)^T\\in \\mathbb{R}^q$，其中第 $i$ 个元素为1、其余为零，则\n$$ e_i^T H_A^T H_A e_i = e_i^T (C_n^{11})^{-1} e_i \\leq \\frac{1}{M_2}, \\forall i=1, \\ldots, q $$\n因此，我们有 $z_n^i = (h_a^i)^T\\epsilon$​，其中\n$$ \\Vert h_a^i\\Vert_2^2 \\leq \\frac{1}{M_2}, \\forall i=1, \\ldots, q \\qquad (23) $$\n类似的，我们令 $\\zeta_n^i = H_B^T\\epsilon$，其中 $H_B^T = C_n^{21}(C_n^{11})^{-1}(n^{-\\frac12}X(1)^T) - n^{-\\frac12}X(2)^T$​，则\n$$ H_B^TH_B = \\frac1nX(2)^T(I - X(1)(X(1)^TX(1))^{-1}X(1)^T)X(2) $$\n由于 $I - X(1)(X(1)^TX(1))^{-1}X(1)^T$ 是投影矩阵，则其特征值为0或1，因此\n$$ \\Vert h_b^i\\Vert_2^2 \\leq M_1, \\forall i=1, \\ldots, q \\qquad (24) $$\n同时注意到\n$$ \\vert \\frac{\\lambda_n}{n}b_n\\vert = \\frac{\\lambda_n}{n}\\vert (C_n^{11})^{-1}\\text{sign}(\\beta^* (1))\\vert \\leq \\frac{\\lambda_n}{nM_2}\\Vert \\text{sign}(\\beta^* (1))\\Vert_2 = \\frac{\\lambda_n}{nM_2}\\sqrt{q} \\qquad (25) $$\n结合（23）和（24），当 $E(\\epsilon_i^{2k}) \u0026lt;\\infty$ 时，可以推出 $E(z_n^i)^{2k}\u0026lt;\\infty$ 和 $E(\\zeta_n^i)^{2k}\u0026lt;\\infty$ 成立。此外，当随机变量具有有限的 $2k$​ 阶矩时，我们可推出它的尾部概率满足下面的界：\n$$ P(z_n^i \u0026gt; t) = O(t^{-2k}) $$\n因此，对 $\\lambda_n /\\sqrt{n} = o(n^{\\frac{c_2 - c_1}{2}})$，利用（25），我们可以得到\n$$ \\sum_{i=1}^{q}P\\left(\\vert z_n^i\\vert \\geq \\sqrt{n}\\left(\\vert \\beta_i^ * \\vert - \\frac{\\lambda_n}{2n} \\vert b_n^i\\vert \\right)\\right) = qO\\left( \\left(\\sqrt{n}\\left(\\vert \\beta_i^ * \\vert - \\frac{\\lambda_n}{2n} \\vert b_n^i\\vert \\right) \\right)^{-2k} \\right) $$\n而\n$$ \\begin{split} n^{\\frac{1-c_2}{2}}\\frac{\\lambda_n}{2n} \\vert b_n^i\\vert \u0026amp;\\leq n^{\\frac{1-c_2}{2}} \\frac{\\lambda_n}{nM_2}\\sqrt{q} \\newline \u0026amp;= \\frac{\\lambda_n}{n^{\\frac{1 + c_2}{2}}M_2}O(n^{\\frac{c_1}{2}}) \\newline \u0026amp;= O\\left(\\frac{\\lambda_n}{n^{\\frac12} n^{c_2 - c_1}} \\right) \\newline \u0026amp;= o(1) \\end{split} $$\n因此，可以推出\n$$ \\begin{split} \\sum_{i=1}^{q}P\\left(\\vert z_n^i\\vert \\geq \\sqrt{n}\\left(\\vert \\beta_i^ * \\vert - \\frac{\\lambda_n}{2n} \\vert b_n^i\\vert \\right)\\right) \u0026amp;= q O\\left( \\left(\\sqrt{n}\\left(\\vert \\beta_i^ * \\vert - \\frac{\\lambda_n}{2n} \\vert b_n^i\\vert \\right) \\right)^{-2k} \\right) \\newline \u0026amp;= q O\\left( \\left( n^{\\frac{c_2}{2}}\\left(n^{\\frac{1-c_2}{2}}\\vert \\beta_i^ * \\vert - n^{\\frac{1-c_2}{2}}\\frac{\\lambda_n}{2n} \\vert b_n^i\\vert \\right) \\right)^{-2k} \\right) \\newline \u0026amp;= q O\\left(\\left(n^{\\frac{c_2}{2}}\\left(n^{\\frac{1-c_2}{2}}\\vert \\beta_i^ * \\vert + o(1) \\right)\\right)^{-2k} \\right) \\newline \u0026amp;= q O(n^{-kc_2}) \\newline \u0026amp;= o\\left(\\frac{pn^k}{\\lambda_n^{2k}} \\right) \\end{split} $$\n类似的，我们可以推出\n$$ \\sum_{i=1}^{p-q}P\\left(\\vert \\zeta_n^i\\vert \\geq \\frac{\\lambda_n}{2\\sqrt{n}}\\eta_i\\right) = (p-q)O\\left(\\frac{n^k}{\\lambda^{2k}} \\right) = O\\left(\\frac{pn^k}{\\lambda^{2k}} \\right) $$\n注意到 $p = o(n^{k(c_2 - c_1)})$，则存在 $\\lambda_n \u0026gt; 0 $ 使得 $\\frac{\\lambda_n}{\\sqrt{n}}=o(n^{\\frac{c_2 - c_1}{2}})$ 和 $\\frac{1}{p}(\\frac{\\lambda_n}{\\sqrt{n}})^{2k}\\rightarrow \\infty$ 成立。至此，定理3证明完毕。$\\blacksquare$\n定理3表明，当强不可表示性条件成立以及噪声具有有限 $2k$ 阶矩时，LASSO可以一致地选择出正确的模型。例如，当噪声具有有限的2阶矩时，根据定理3可知，数据的维度可以以低于 $n^{c_2 - c_1}$ 的速率（可能比 $n$ 要小，即仍是低维）增长，而此时LASSO在强不可表示性条件的保证下可以一致地选择出真实模型。如果噪声的所有 $2k$ 阶矩都有限，那么此时数据的维度可以以任意多项式的速率增长。特别地，当噪声是高斯噪声时，我们可以得到一个更强结论，具体如下一小节所述。\n4.3 高斯噪声的情形 当噪声是高斯噪声时，我们有下面的定理：\n【定理3】 假设 $\\epsilon_i$ 是iid的随机噪声，且（19）-（22）都成立。如果存在 $0\\leq c_3\u0026lt; c_2 - c_1$ 使得 $p = O(e^{n^{c_3}})$ 成立，则强不可表示性条件可以推出LASSO的强符号一致性。特别的，对 $\\lambda_n \\propto n^{\\frac{1+c_4}{2}}$ 其中 $c_3 \u0026lt; c_4 \u0026lt; c_2 - c_1$，\n$$ P(\\hat{\\beta}^* (\\lambda_n) =_s \\beta^* ) \\geq 1 - o(e^{-n^{c_3}}) \\rightarrow 1 $$\n【证明】 由于 $\\lambda_n \\propto n^{\\frac{1+c_4}{2}}$ 和 $c_3 \u0026lt; c_4 \u0026lt; c_2 - c_1$，根据（25）式立即可推出\n$$ \\begin{split} n^{\\frac{1-c_2}{2}}\\frac{\\lambda_n}{2n} \\vert b_n^i\\vert \u0026amp;\\leq n^{\\frac{1-c_2}{2}} \\frac{\\lambda_n}{nM_2}\\sqrt{q} \\newline \u0026amp;= n^{\\frac{1-c_2}{2}}\\frac{O(n^{\\frac{1+c_4}{2}})}{nM_2}O(n^{\\frac{c_1}{2}}) \\newline \u0026amp;= \\frac{O(n^{\\frac{2+c_4 - (c_2 - c_1)}{2}})}{nM_2} \\newline \u0026amp;= o(1) \\end{split} $$\n则根据定理1中正态随机变量尾部概率的界和（23）式可知\n$$ \\begin{split} \\sum_{i=1}^{q}P\\left(\\vert z_n^i\\vert \\geq \\sqrt{n}\\left(\\vert \\beta_i^* \\vert - \\frac{\\lambda_n}{2n} \\vert b_n^i\\vert \\right)\\right) \u0026amp;= \\sum_{i=1}^{q}P\\left(\\vert z_n^i\\vert \\geq n^{\\frac{c_2}{2}}\\left(n^{\\frac{1-c_2}{2}}\\vert \\beta_i^* \\vert - n^{\\frac{1-c_2}{2}}\\frac{\\lambda_n}{2n} \\vert b_n^i\\vert \\right) \\right) \\newline \u0026amp;\\leq \\sum_{i=1}^{q}P\\left( \\vert z_n^i\\vert \\geq n^{\\frac{c_2}{2}}\\left(n^{\\frac{1-c_2}{2}}\\vert \\beta_i^* \\vert + o(1) \\right) \\right) \\newline \u0026amp;\\leq \\sum_{i=1}^{q}P\\left( \\vert z_n^i\\vert \\geq n^{\\frac{c_2}{2}}\\left(M_3 + o(1) \\right) \\right) \\newline \u0026amp;\\leq \\sum_{i=1}^{q}\\left( 1 - \\Phi(M_2n^{\\frac{c_2}{2}}(M_3 + o(1)))\\right) \\newline \u0026amp;= qO(1 - \\Phi(M_2M_3n^{\\frac{c_2}{2}})) \\newline \u0026amp;= o(e^{-n^{c_3}}) \\end{split} $$\n类似的，可以推出\n$$ \\sum_{i=1}^{p-q}P\\left(\\vert \\zeta_n^i\\vert \\geq \\frac{\\lambda_n}{2\\sqrt{n}}\\eta_i\\right) = (p-q)O\\left(1 - \\Phi\\left(\\frac{1}{M_1}\\frac{\\lambda_n}{\\sqrt{n}}\\eta \\right) \\right) = o(e^{-n^{c_3}}) $$\n这样就证明了定理4的结论。$\\blacksquare$\n5 结语 本篇博客主要以Zhao和Yu（2006）所提出的“不可表示性条件”为主介绍了LASSO的变量选择一致性。通过前文的分析和探讨，我们知道LASSO具有稀疏属性，但其变量选择一致性则需要额外的条件来保证。此外，当维度发散时，LASSO估计仍然可以在若干条件下保证良好的稀疏性，其中维数的变动特征和噪声有关。当噪声为高斯噪声时，LASSO可以在高维甚至超高维情形下达到良好的变量选择一致性。\n6 参考文献 [1] Fu W, Knight K. Asymptotics for lasso-type estimators[J]. The Annals of statistics, 2000, 28(5): 1356-1378.\n[2] Meinshausen N, Bühlmann P. High-dimensional graphs and variable selection with the lasso[J]. The annals of statistics, 2006, 34(3): 1436-1462.\n[3] Zhao P, Yu B. On model selection consistency of Lasso[J]. The Journal of Machine Learning Research, 2006, 7: 2541-2563.\n[4] Zhang CH, Huang J. The sparsity and bias of the lasso selection in high-dimensional linear regression[J]. The Annals of Statistics, 2008, 36(4): 1567-1594.\n[5] Zou H. The adaptive lasso and its oracle properties[J]. Journal of the American statistical association, 2006, 101(476): 1418-1429.\n","id":3,"section":"posts","summary":"高维稀疏化技术是大数据分析的重要方法之一。以LASSO为代表的模型选择方法，在过去的20多年间（20世纪90年代以来）得到了众多学者的关注并","tags":["多元统计","变量选择","高维数据分析"],"title":"LASSO的变量选择一致性之不可表示性条件","uri":"https://qkai-stat.github.io/2021/10/irrepresentable-condition/","year":"2021"},{"content":"数更寒雨今方休，天凉月高怕登楼。\n湖边垂緌声切切，陌上枰仲影悠悠。\n谁家俦侣吹芦管，何处羁人醉离愁。\n原道西风恁无情，玉壶光转白人头。\n","id":4,"section":"posts","summary":"数更寒雨今方休，天凉月高怕登楼。 湖边垂緌声切切，陌上枰仲影悠悠。 谁家俦侣吹芦管，何处羁人醉离愁。 原道西风恁无情，玉壶光转白人头。","tags":["虎溪岁月","诗文"],"title":"中秋","uri":"https://qkai-stat.github.io/2021/09/zq2021/","year":"2021"},{"content":" 凸优化是优化问题的重要分支之一，也是研究机器学习的重要工具之一。本文和其他几篇以“凸优化”为主题的博客都是我在学习Boyd所著《Convex Optimization》这本书时的笔记。因此，这些博客的内容结构和书中第5、9-11章节的框架相似，我在这里只不过罗列了书中的一些要点和个人的一点思考。\n 带有等式约束、不等式约束的凸优化问题可以通过一些技巧转化成无约束问题。因此，研究无约束凸优化问题是学习凸优化算法的基础。本文主要介绍无约束优化问题的基本概念，如下水平集、条件数等。了解和掌握这些概念对认识和分析无约束凸优化问题有重要的帮助。\n1 无约束优化问题 我们研究下面形式的无约束优化问题 $$ \\min f(x) $$ 其中 $f：\\mathbb{R}^n\\rightarrow \\mathbb{R}$ 是二次可微凸函数。我们假定该问题存在最优解 $x^*$，对应最优值 $p^ * =\\inf_x f(x)=f(x^ *)$。既然 $f$ 是可微凸函数，那么最优点应当满足最优性一阶微分条件，即 $$ \\nabla f(x^ *)=0 $$ 这样，最小化 $f$​​ 就变成求解 $n$​​ 个变量 $x_1,\\ldots,x_n$​​ 的 $n$​​ 个方程问题了。除了一些特殊情况，我们通常很难解析的求解上述一阶微分问题。实际中，我们一般采用迭代算法进行求解，也即是构造点列 $x^{(0)}$​​，$x^{(1)}$​​，$\\ldots$​​ $\\in \\textrm{dom} f$​​，使得 $k\\rightarrow\\infty$​​ 时 $f(x^{(k)})\\rightarrow p^ *$​​。这样的点列称之为优化问题的极小化点列。实际计算时通常给定容许误差 $\\epsilon$​​，当 $f(x^{(k)})-p^ *\u0026lt;\\epsilon$​​​​ 时算法终止。\n2 下水平集 函数 $f$ 的 $\\alpha$-下水平集（sublevel sets） $C_{\\alpha}$ 定义成 $$ C_{\\alpha} = \\lbrace x\\in \\textrm{dom}f | f(x)\\leq \\alpha \\rbrace $$ 根据定义，$\\alpha$-下水平集描述的是那些让 $f$ 取值小于等于 $\\alpha$ 的 $x$ 所构成的集合，即定义域 $\\textrm{dom}f$ 的一个子集。关于凸函数的$\\alpha$-下水平集，我们有如下的结论：\n【定理】 凸函数的 $\\alpha$-下水平集仍然是凸集，反之则不成立。\n【证明】 根据凸集的定义证明即可。假设 $f$ 是凸函数，考虑其 $\\alpha$-下水平集 $C_{\\alpha}$ 中的任意两点 $x_1$ 和 $x_2$，则 $$ f(x_1)\\leq \\alpha,\\quad f(x_2)\\leq \\alpha $$ 结合函数的凸性不难得到 $$ \\begin{split} f(\\theta x_1 + (1 - \\theta x_2)) \u0026amp;\\leq \\theta f(x_1) + (1-\\theta)f(x_2)\\newline \u0026amp;\\leq \\theta \\alpha + (1-\\theta)\\alpha\\newline \u0026amp;= \\alpha \\end{split} $$ 表明 $C_{\\alpha}$ 确实是凸集。\n反之不成立。我们考虑函数 $f = -e^{x}$，如下图所示\n可见，对于任意的 $\\alpha$，$f$ 的 $\\alpha$-下水平集 $C_{\\alpha}$ 都是凸集，但是 $f$ 显然是严格凹的。$\\blacksquare$\n我们后续介绍的求解无约束问题的方法需要一个适当的初始点 $x^{(0)}\\in \\textrm{dom}f$，且其下水平集 $$ S = \\lbrace x\\in \\textrm{dom}f | f(x)\\leq f(x_0) \\rbrace $$ 必须是闭集。此外，在下面的内容中我们还将看到下水平集和优化问题之间的重要联系。\n3 强凸性及其含义 3.1 强凸函数的次优性条件 关于无约束优化问题，我们大多数情况都假设目标函数在 $S$ 上是强凸的，也即是存在 $m\u0026gt;0$，使得 $$ \\nabla^2 f(x) \\succeq m I \\qquad (1) $$ 对任意的 $x\\in S$ 都成立。\n强凸性可以推出若干有意义的结果。例如，对 $x, y \\in S$，我们有 $$ f(y) = f(x) + \\nabla f(x)^T(y - x) + \\frac12 (y - x)^T\\nabla^2 f(z)(y - x) \\qquad (2) $$ 其中 $z$ 属于线段 $[x, y]$。此时，根据强凸性式（1）的结果，式（2）的最后一项不会小于 $1/2\\Vert y-x\\Vert_2^2$，因此 $$ f(y) \\geq f(x) + \\nabla f(x)^T(y - x) + \\frac m2 \\Vert y - x\\Vert_2^2 \\qquad (3) $$ 对 $ S $ 中任意的 $x$ 和 $y$ 都成立。当 $m = 0$ 时，式（3）就变回描述凸性的基本不等式；当 $m \u0026gt; 0$ 时，对 $f(y)$ 的下界我们可以比单独用凸性更好的结果。\n现在，我们说明可以利用式（3）给出 $f(x) - p^* $，所得上界结果可表明 $x$ 是其目标值和最优目标值的偏差正比于 $\\Vert \\nabla f(x)\\Vert_2 $ 的次优解。对于任意固定的 $x$，式（3）的不等式右侧是关于 $y$ 的二次函数，它在 $\\bar{y} = x - (1/m)\\nabla f(x)$ 处取得最小值，即 $$ \\begin{split} f(y) \u0026amp;\\geq f(x) + \\nabla f(x)^T(y - x) + \\frac m2 \\Vert y - x\\Vert_2^2\\newline \u0026amp;\\leq f(x) + \\nabla f(x)^T(\\bar{y} - x) + \\frac m2 \\Vert \\bar{y} - x\\Vert_2^2\\newline \u0026amp;= f(x) - \\frac{1}{2m}\\Vert\\nabla f(x)\\Vert_2^2 \\end{split} $$\n既然该式对任意的 $y\\in S$ 都成立，那么可以得到 $$ p^ * \\geq f(x) - \\frac{1}{2m} \\Vert \\nabla f(x)\\Vert_2^2 \\qquad (4) $$\n由此可见，任何梯度足够小的点都是近似最优解。不等式（4）是最优性条件的推广。由于 $$ \\Vert \\nabla f(x)\\Vert_2 \\leq (2m\\epsilon)^{1/2} \\Rightarrow f(x) - p^* \\leq \\epsilon \\qquad (5) $$\n我们可以将其解释成次优性条件。\n3.2 关于 $\\nabla^2 f(x)$ 的上界 不等式（3）表明，$ S $ 所包含的所有下水平集都有界，因此，$ S $ 本身作为一个下水平集也有界。由于 $ \\nabla^2 f(x) $ 的最大特征值是 $ x $ 在 $ S $ 上的连续函数，所以它在 $ S $ 上有界，即存在常数 $ M $ 使得 $$ \\nabla^2 f(x) \\preceq MI \\qquad (6) $$ 对所有 $x\\in S$ 都成立。这意味着，对所有的 $x, y\\in S$， $$ f(y) \\leq f(x) + \\nabla f(x)^T (y - x) + \\frac{M}{2}\\Vert y - x\\Vert_2^2 \\qquad (7) $$ 类似的，对右边关于 $y$ 求极小，又可以得到 $$ p^* \\leq f(x) - \\frac{1}{2M}\\Vert \\nabla f(x)\\Vert_2^2 \\qquad (8) $$ 这是和式（4）相对应的不等式。\n3.3 下水平集条件数 根据强凸性，我们得到了关于Hessian矩阵的不等式（1）和（6），即对任意的 $x\\in S$ 都成立 $$ mI \\preceq \\nabla^2 f(x) \\preceq MI $$ 因此，比值 $\\kappa = M/m$ 是矩阵 $\\nabla^2 f(x)$ 的条件数（其最大特征值和最小特征值之比）的上界。下面我们基于 $f$ 的下水平集给出一个几何解释。\n对于任意满足 $\\Vert q\\Vert_2 = 1$ 的方向向量 $q$，我们定义凸集 $C\\subseteq R^n$ 的宽度如下 $$ W(C, q) = \\sup_{z\\in C} q^Tz - \\inf_{z\\in C} q^Tz $$ 再定义 $C$ 的最小宽度和最大宽度 $$ W_{\\min} = \\inf_{\\Vert q\\Vert_2 = 1} W(C, q),\\quad W_{\\max} = \\sup_{\\Vert q\\Vert_2 = 1} W(C, q) $$ 于是，凸集 $C$ 的条件数可以表示成 $$ \\textrm{cond}(C) = \\frac{W_{\\max}^2}{W_{\\min}^2} $$\n即最大宽度和最小宽度的平方比值。该式表明，$C$ 的条件数给出了各向异性或离心率的一种测度。换句话说，如果 $C$ 的条件数小（接近1），表明集合在所有方向上的宽度近似相等，即几乎是一个球体。如果 $C$ 的条件数大，表明集合在某些方向上的宽度远比其他一些方向的宽度大。\n下面，我们来研究 $f$ 的下水平集和最优性的关系。假定 $f$ 满足 $mI \\preceq \\nabla^2 f(x) \\preceq MI$。令 $C_{\\alpha} = \\lbrace x| f(x)\\leq \\alpha\\rbrace$，其中 $p^* \u0026lt;\\alpha \\leq f(x_0)$，以及下面两个集合 $$ \\begin{split} B_{inner} \u0026amp;= \\lbrace y | \\Vert y - x^* \\Vert_2 \\leq (2(\\alpha - p^* )/M)^{1/2}\\rbrace \\newline B_{outer} \u0026amp;= \\lbrace y | \\Vert y - x^* \\Vert_2 \\leq (2(\\alpha - p^* )/m)^{1/2}\\rbrace \\end{split} $$\n将 $x = x^* $ 带入到式（3）和式（7）中得到 $$ p^* + \\frac{M}{2}\\Vert y - x^* \\Vert_2^2 \\geq f(y) \\geq p^* + \\frac{m}{2}\\Vert y - x^* \\Vert_2^2 $$\n这意味着 $B_{inner} \\subseteq C_{\\alpha} \\subseteq B_{outer}$ 成立（可以根据集合包含关系的定义来证明）。根据 $B_{inner}$ 和 $B_{outer}$ 的定义，两者分别表示半径为 $2(\\alpha - p^* )/M$ 和 $2(\\alpha-p^* )/m$ 的球体。那么， $B_{inner}$ 和 $B_{outer}$ 的半径平方之比就给出了下水平集 $C_{\\alpha}$ 的条件数的一个上界： $$ \\textrm{cond}(C_{\\alpha}) \\leq \\frac{M}{m} $$ 我们也可以对最优解处Hessian矩阵的条件数 $\\kappa(\\nabla^2 f(x^* ))$ 给出一种几何解释。将 $f$ 在 $x^* $ 处进行Taylor展开 $$ f(y) = p^* + \\frac12 (y-x^* )^T\\nabla^2 f(x^* )(y - x^* ) $$ 可以看出，对于充分靠近 $p^* $ 的 $\\alpha$ 有 $$ C_{\\alpha} \\approx \\lbrace y |(y-x^* )^T\\nabla^2 f(x^* )(y - x^* )\\leq 2(\\alpha - p^* )\\rbrace $$ 即中心为 $x^* $ 的椭球可以很好的逼近下水平集。因此 $$ \\lim_{\\alpha\\rightarrow p^* } \\textrm{cond}(C_{\\alpha}) = \\kappa (\\nabla^2 f(x^* )) $$\n我们将发现，对于一些常用的无约束优化算法，$f$ 的下水平集的条件数是影响其计算效率的重要因素 。\n4 结语 本文主要介绍了无约束凸优化问题的一些重要概念。我们着重讨论了强凸性在优化问题中的含义，重点揭示了 $f$ 的下水平集和最优解之间的联系。这些概念将在具体的无约束优化算法的讨论中体现其含义和价值。\n","id":5,"section":"posts","summary":"凸优化是优化问题的重要分支之一，也是研究机器学习的重要工具之一。本文和其他几篇以“凸优化”为主题的博客都是我在学习Boyd所著《Convex","tags":["机器学习","优化算法","凸优化"],"title":"无约束优化：基本概念总结","uri":"https://qkai-stat.github.io/2021/07/unconstrained-optimization/","year":"2021"},{"content":"时清日长蛩音噪，\n沉香燎尽暑难消。\n昏昏酣睡方初醒，\n楫舟梦寻故乡遥。\n","id":6,"section":"posts","summary":"时清日长蛩音噪， 沉香燎尽暑难消。 昏昏酣睡方初醒， 楫舟梦寻故乡遥。","tags":["虎溪岁月","诗文"],"title":"端午","uri":"https://qkai-stat.github.io/2021/06/dw/","year":"2021"},{"content":"在往期的博客中，我和大家分享了仅含有R代码的R包制作的简易教程。实际中，我们的函数文件可能既包含R代码又包含C++代码，那么此时的R包制作会有哪些异同呢？本期我们一起来学习含有C++代码的R包制作流程。\n1 准备工作 我们仍然默认大家已经安装了R和RStudio，配置R包开发的基础环境可以参考我往期的博客——R包制作简易教程1。此外，我们还要准备好两件事。一是安装Rcpp程序包，它是R中辅助开发C++程序的重要R包，关于Rcpp程序包的介绍可参考我往期的博客——效率瓶颈突破：Rcpp简介。二是安装Rtools套件，这是CRAN镜像网络为Windows系统下R开发C++代码提供的环境支持。Rtools套件需要单独下载进行安装，其官方下载地址为\nhttps://cran.r-project.org/bin/windows/Rtools/  根据自己的情况选择合适的版本即可。安装完毕后，需要将下面几个目录加入到系统的环境变量中，否则R检测不到相关程序就无法完成C++代码的编译：\n# 假设Rtools的安装目录为 D:\\rtools40 # 需要放进环境变量的目录分别是 D:\\rtools40 D:\\rtools40\\usr\\bin D:\\rtools40\\mingw32\\bin D:\\rtools40\\mingw64\\bin  关于Windows下更改环境变量的方法，可以参考如下的百度经验帖：\nhttps://jingyan.baidu.com/article/9113f81b5c2b322b3214c7a0.html  准备工作完毕，我们介绍本次制作含有C++代码的R包所用到的函数代码：\n1. awensvm.R 2. clipDCDMain.cpp 3. gaussian_kernel.R  其中代码1是主程序，用来实现我一篇论文中的AWENSVM模型；代码2是求解模型的核心算法程序，是一个C++代码；代码3则是用来求解非线性AWENSVM时要用到。此外，函数awensvm和函数gaussian_kernel要在载入程序包后可以用户查询和使用，而函数clipDCDMain则可以隐藏。\n 由于篇幅限制，我们不介绍R中C++函数的构建和编译过程而是假定C++代码已经调试成功并可以在R中调用执行。\n 下面，我们正式介绍含有C++代码的R包制作方式。\n2 RStudio可视化制作 首先我们还是来看RStudio中的可视化操作方式。与R代码的R包制作方式类似，我们通过RStudio菜单栏依次选择File \u0026gt; New Project创建一个项目，然后选择New Directory \u0026gt; R package，得到R包制作的初始界面：\n与R代码程序包不同的是，这里我们选择Package w/cpp，然后填上R包的名称，并通过Add按钮添加所需的函数代码。最后，我们把生成的R包文件夹的路径填成桌面，点击Create Project即可在桌面创建名为AWENSVMcpp的文件夹。\n与仅含R代码生成的目录不同之处在于，这里多了一个src文件夹，而src文件夹正是存放我们C++代码的文件夹。其他文件夹和文档的含义请读者阅读往期文章——R包制作简易教程1。下面，我们重点介绍修改文件和仅含R代码时的不同之处。\nDESCRIPTION文档的修改。这里要注意多出的Imports和LinkingTo两个参数。由于R中C++代码的正常运行是基于Rcpp包的，所以我们要在Imports参数后添加Rcpp，后面括号的数字表示支持本R包所需Rcpp的最低版本——如果考虑到R包的适用性，可以适当降低版本号。此外，我这里的C++代码用到了RcppEigen包。因此，我们需要在LinkingTo参数后额外加上RcppEigen，表示链接到该包。\nPackage: AWENSVMcpp Type: Package Title: What the Package Does (Title Case) Version: 0.1.0 Author: Who wrote it Maintainer: The package maintainer \u0026lt;yourself@somewhere.net\u0026gt; Description: More about what it does (maybe more than one line) Use four spaces when indenting paragraphs within the Description. License: What license is it under? Encoding: UTF-8 LazyData: true Imports: Rcpp (\u0026gt;= 1.0.6) LinkingTo: Rcpp, RcppEigen  NAMESPACE文档的修改。除了R代码制作时所需的前两行命令外，我们还要加上如下的后两行命令，表示R包要导入Rcpp包来进行C++代码运算，以及R包生成的动态链接库会被生成的R包使用。\nexportPattern(\u0026quot;^[[:alpha:]]+\u0026quot;) export(\u0026quot;awensvm\u0026quot;, \u0026quot;gaussian_kernel\u0026quot;) importFrom(Rcpp, evalCpp) useDynLib(AWENSVMcpp, .registration = TRUE)  其余的操作和R代码制作程序包相同，如光标放在需要说明的函数所在代码的函数名处使用Ctrl+Shift+Alt+R进行特殊的注释，再运行devtools::document()生成Rd说明文档等，这里不在赘述。\n最后，我们通过RStudio菜单栏中的Build \u0026gt; Check Package来检查R包是否可以成功编译，通过RStudio菜单栏中的Build \u0026gt; Build Source Package来生成可供他人安装的.tar.gz文件。\n3 Rcpp.package.skeleton制作 Rcpp包提供了一个和R中package.skeleton类似的命令Rcpp.package.skeleton来制作含有C++代码的程序包。Rcpp.package.skeleton的使用方式和package.skeleton基本一致，读者可以参考R包制作简易教程1中关于package.skeleton的介绍。由于生成的程序包文件夹和第2节中的一样，我们就不在赘述具体的制作方式了。\n4 结语 本期我们主要介绍了含有C++代码的R包制作简易教程，这部分的内容和仅含R代码的程序包制作方式类似。在实际应用中，制作含有C++代码的程序包有时不可避免。例如，我们希望通过C++编写函数的核心程序来减少时间成本。然而，如果我们打算进一步结合foreach来并行计算，就发现会报错。此时，通过编写程序包将我们的R代码和C++代码打包编译后再结合foreach并行就可以成功运行。\n","id":7,"section":"posts","summary":"在往期的博客中，我和大家分享了仅含有R代码的R包制作的简易教程。实际中，我们的函数文件可能既包含R代码又包含C++代码，那么此时的R包制作会","tags":["R语言","算法"],"title":"R包制作简易教程2","uri":"https://qkai-stat.github.io/2021/05/rpackagemake2/","year":"2021"},{"content":"在使用R时，最常见的操作就是通过 install.packages() 安装某个R包以及 library()载入并使用该R包。这些R包的开发者很多来自于活跃在统计、数据分析、机器学习等领域的学者和教授。有时，某个新模型被提出后，作者就已经开发出相应的R包供他人使用。因此，R包的持续更新和开发使得R用户可以站在众多领域的最前沿。\n 诚然，正是得益于大家的分享，才铸就了R的强大。\n 普通人虽然难以开发出专业的R包分享给他人使用，但是掌握R包开发的基本方法仍然是有益的——可以把自己的代码集中管理，以便调用。本文便是R包制作的一个简易教程。\n1 准备工作 在R包开始制作前，我们要为其配置相关的环境。首先，我已经默认大家已经安装了R和RStudio两个软件。R包的开发需要用到一系列程序包，可以用下面的命令安装\ninstall.packages(c(\u0026quot;devtools\u0026quot;, \u0026quot;roxygen2\u0026quot;, \u0026quot;testthat\u0026quot;, \u0026quot;knitr\u0026quot;)) # devtools 可以自动化一些开发工作，减少R包开发的痛苦 # roxygen2 将R包的帮助文档写作变得便捷 # testthat 用来构建测试 # knitr 则是一个动态文档成工具  我们可以使用下面的命令检测是否所有的工具都已安装：\nlibrary(devtools) # 载入需要的程辑包：usethis has_devel() # Your system is ready to build packages!  环境配置好后，接着我们介绍本文制作R包所用到的3个R函数代码：\n1. awensvm.R 2. clipDCD_Main.R 3. gaussian_kernel.R  其中代码1是主功能函数，用来实现我在2019年提出的AWENSVM模型；代码2是用来求解该模型的核心程序；代码3则用来计算高斯核内积，在构建非线性AWENSVM时要用到。需要说明的是，函数awensvm和函数gaussian_kernel在载入包后要能被用户调用，而clipDCD_Main则可以隐藏。\n一切就绪后，下面我们正式进入R包制作环节。\n2 RStudio可视化制作 2.1 建立R包项目 基于RStudio，我们可以可视化的构建R包。首先，通过RStudio菜单栏的File \u0026gt; New Project创建一个项目：\n在出现的上述界面中选择New Directory \u0026gt; R package，得到R包创建初始化界面：\n在Package name下填上自己R包的名称，这里是AWENSVM；通过Add按钮添加R包所需的R函数代码，这里依次添加了上述的三个函数的R代码；接着选择将该项目（构建R包）放在哪个目录下，这里是电脑桌面；最后点击Create Project即可建立本次R包开发的项目。\n2.2 更改包文件信息 点击Create Project之后，在桌面上就出现一个名为AWENSVM的文件夹，里面包含了下述文件/目录：\n为了顺利开发R包以及增强R包的使用性，我们需要对该目录的一些文档进行自定义修改，主要涉及DESCRIPTION文件、NAMESPACE文件、R文件夹和man文件夹。下面逐一介绍各自的作用和修改方式。\nDESCRIPTION描述了所建R包的信息，我们可以在RStudio中打开并编辑它。如果要分享自己的R包，填上相关信息是必要且有意义的。\nPackage: AWENSVM Type: Package Title: What the Package Does (Title Case) Version: 0.1.0 Author: Who wrote it Maintainer: The package maintainer \u0026lt;yourself@somewhere.net\u0026gt; ...(其他内容省略)  上述的选项容易理解。我们选择其他几个重要的选项给予解释：一是Imports，它声明了保证本R包正常运行所必须安装的包；一是Suggests，它声明了本R包可以使用的包，但不是必需的；一是LinkingTo，它声明了本R包正常运行所必须的另一个包中的C或C++代码（本文不考虑含有C++代码的R包制作）。由于awensvm函数中的示例代码需要ggplot2和\tMASS包但非必需，我们添加如下代码：\nSuggests: ggplot2, MASS  NAMESPACE描述了R包需要导出供用户调用的函数名，第一行代码表示按照函数的字母名称进行调用。我要导出awensvm和gaussian_kernel，在下面用export()输出，编辑后保存文档即可。\nexportPattern(\u0026quot;^[[:alpha:]]+\u0026quot;) export(\u0026quot;awensvm\u0026quot;, \u0026quot;gaussian_kernel\u0026quot;)  R文件夹下放置了所需的R代码文件，也即是开始添加的三个函数的R代码。如果想为R包再加入R函数，直接将相应的R代码文件拖入其中即可。\nman文件夹放置了R包的帮助文档，后缀名为.Rd。刚创建时，man文件夹下为空。以创建awensvm函数帮助文档为例，首先打开R文件夹下awensvm的R代码，然后将光标放在函数名上，按快捷键Ctrl+Shift+Alt+R生成特殊的注释，根据@key录入相关信息即可。可以通过@添加新的内容，下表列举了一些常见的信息：\n   命令 含义     @param 给函数中的变量添加描述   @export 描述函数输出值的信息   @author 函数代码的作者   @reference 参考文献信息   @example 函数的使用示例代码    关于awensvm函数的注释如下所示，其中示例代码过长这里省略：\n#' @title Adaptive Weighted Elastic Net SVM #' #' @param x the data matrix #' @param y the label vector #' @param lam1 the nonnegative tuning parameter #' @param lam2 the nonnegative tuning parameter #' @param weightMat the weight matrix #' @param kernel kernel choice #' @param ... Gaussian kernel parameter #' #' @return w the normal vector #' @return b the intercept #' @return u the estimated Lagrangian multipliers #' #' @export #' #' @references \\url{https://linkinghub.elsevier.com/retrieve/pii/S0950705119303764} #' awensvm \u0026lt;- function(x, y, lam1, lam2, weightMat = \u0026quot;identity\u0026quot;, kernel = \u0026quot;linear\u0026quot;, ...){ # 函数主体(省略) ... }  保存后，运行devtools::document()即可在man中生成awensvm函数的帮助文档awensvm.Rd。可以直接在RStudio中打开该.Rd文档\n然后点击Preview即可在RStudio中预览该文档\n这和我们通常使用?函数名查看函数帮助文档的界面相同。需要注意的是，每次修改描述注释后，都要重新运行devtools::document()才能刷新帮助文档。\n2.3 生成R包 在RStudio菜单栏中选择Build \u0026gt; Check Package来检查R包是否可以成功编译。运行结束后，在工作目录生成AWENSVM.Rcheck文件夹，若其中的日志文件显示*DONE，则表示R包没有问题。此时已经可以在RStudio使用library()载入AWENSVM包，并用?awensvm查看说明文档。然而，此时AWENSVM包并没有真的安装在R的library目录下，我们下面介绍如何生成R包源文件并安装。\n在RStudio菜单栏中选择Build \u0026gt; Build Source Package进行R包编译，成功后即在工作目录生成R包源文件\n使用如下命令进行安装测试\ninstall.packages(\u0026quot;AWENSVM_0.1.0.tar.gz\u0026quot;, repos = NULL, type = \u0026quot;source\u0026quot;)  出现下面的*DONE结果即表明成功安装所生成的R包，此时在R的library目录下可看到名为AWENSVM的文件夹。\n 将生成的.tar.gz源文件分享给他人，即可按照上述的方式安装开发的R包。\n 3 package.skeleton制作 R为程序包开发设有专门的package.skeleton()命令，其中常见的变量与释义如下所示：\nname: R包名称 path: R包文件将要存放的位置 code_files: 所需R代码的路径 list: 所需R函数名  如果R包需要单个R代码，直接用code_files 指明其所在目录即可；如果需要多个R函数，只要将各个函数source()进来，然后用list指明所需的函数名即可将对应的R代码纳入R包。这里AWENSVM的代码为\n\u0026gt; package.skeleton(name = \u0026quot;AWENSVM\u0026quot;, + path = \u0026quot;C:/Users/jiandan/Desktop\u0026quot;, + list = c(\u0026quot;awensvm\u0026quot;, \u0026quot;gaussian_kernel\u0026quot;)) Creating directories ... Creating DESCRIPTION ... Creating NAMESPACE ... Creating Read-and-delete-me ... Saving functions and data ... Making help files ... Done. Further steps are described in 'C:/Users/jiandan/Desktop/AWENSVM/Read-and-delete-me'.  代码运行结束后会在工作目录中生成名为AWENSVM的文件夹，其中内容和RStudio生成的一致。不同的是，man文件夹下非空，自动生成了awensvm和gaussian_kernel的说明文档。上述list没有clipDCD_Main参数，因为其只需在R包内部调用，AWENSVM文件夹创建后直接将其R代码拖入R文件夹下即可。\n类似的，我们要对其中的相关文档进行编辑。此处可参照第2节的内容，故不赘述。\n修改完成后，我们切换到CMD，并将工作目录设置成AWENSVM文件夹所在的目录，此处即桌面。然后运行下面的命令进行R包的安装测试\nR CMD check AWENSVM  没有问题再运行下面的命令进行R包生成\nR CMD build AWENSVM  最后在桌面生成.tar.gz源文件，在R中安装即可\ninstall.packages(\u0026quot;AWENSVM_1.0.tar.gz\u0026quot;, repos = NULL, type = \u0026quot;source\u0026quot;)  4 AWENSVM包 这个R包是实现我在2019年提出的AWENSVM的算法。AWENSVM是一个处理含有异常值的不平衡二分类问题的自适应加权SVM方法。为了说明方法的有效性，我在AWENSVM包中添加了一个示例代码。这里给出更为详细的实现过程与说明。\n首先，我们生成300个来自二维双峰正态分布的训练样本点\nlibrary(ggplot2) library(MASS) set.seed(112) n \u0026lt;- 300 sig \u0026lt;- diag(c(0.02, 0.06)) xpos1 \u0026lt;- mvrnorm(0.25*n, mu = c(0.4, 0.7), Sigma = sig) xpos2 \u0026lt;- mvrnorm(0.25*n, mu = c(-0.3, 0.7), Sigma = sig) xneg1 \u0026lt;- mvrnorm(0.25*n, mu = c(-0.7, 0.2), Sigma = sig) xneg2 \u0026lt;- mvrnorm(0.25*n, mu = c(0.3, 0.2), Sigma = sig) xpos \u0026lt;- rbind(xpos1, xpos2) xneg \u0026lt;- rbind(xneg1, xneg2) x \u0026lt;- rbind(xpos1, xpos2, xneg1, xneg2) y \u0026lt;- rep(c(1,-1), c(0.5*n, 0.5*n))  然后，我们考虑线性核并用网格法选出最优参数，其中weightMat = \u0026quot;identity\u0026quot;表示对样本不加权\n# 构建参数网格 tunseq \u0026lt;- 2^seq(-8, 8, 2) awen_tuning \u0026lt;- ParameterGrid(list(lam1 = tunseq, lam2 = tunseq)) # CV选择最优参数 # 最优值为 lam1=1, lam2=16 OptPar_awen \u0026lt;- cv_awensvm(x, y, tuning = ensvm_tuning) # 将最优参数带入到模型中计算 awen_noweight \u0026lt;- awensvm(x, y, lam1 = OptPar_awen[2], lam2 = [3], weightMat = \u0026quot;identity\u0026quot;, kernel = \u0026quot;linear\u0026quot;)  查看最优模型的预测准确率有\nprey_noweight \u0026lt;- sign(x%*%awen_noweight$w + awen_noweight$b) round(sum(diag(table(prey_noweight, y)))/n, 4) # [1] 0.8667  根据模型的计算结果，我们画出不加权模型的预测超平面示意图\n接着，我们采用AWENSVM的加权思路对数据进行加权处理。为简便，我们仍然采用前面的最优参数进行模型训练，相关代码为\n# aenl_weight_transform是线性AWENSVM的权重计算函数 # 参数k和g可以用网格法寻优，这里仅以2和0.5作示例 tran_data \u0026lt;- aenl_weight_transform(x, y, k = 2, g = 0.5) tx \u0026lt;- as.matrix(tran_data$x) ty \u0026lt;- as.matrix(tran_data$y) awen_weight \u0026lt;- awensvm(tx, ty, lam1 = 1, lam2 = 16, weightMat =diag(tran_data$weight))  查看最优模型的预测准确率有\nprey_weight \u0026lt;- sign(tx%*%awen_weight$w + awen_weight$b) round(sum(diag(table(prey_weight, ty)))/n, 4) # [1] 0.8733  根据模型的计算结果，我们画出不加权模型的预测超平面示意图\n从预测准确率来看，加权模型的结果更好。从预测超平面示意图来看，加权模型的超平面稍偏下，表明受到上方异常数据点的吸引更小，这也表明了模型的有效性。\n5 结语 本文简单介绍了R包制作的核心流程，感兴趣的读者可以动手实践起来，毕竟R in action！当然，本文的内容非常粗浅，只涉及到了R代码的R包制作，没有涉及与C++代码的混合制作方式，这些内容会在后续的推文中与大家分享。读者可参考Hadley所著的《R Packages》（中文名《R包开发》）了解更全面的知识。\n","id":8,"section":"posts","summary":"在使用R时，最常见的操作就是通过 install.packages() 安装某个R包以及 library()载入并使用该R包。这些R包的开发者很多来自于活跃在统计、数据分析、机器学","tags":["R语言","算法"],"title":"R包制作简易教程1","uri":"https://qkai-stat.github.io/2021/04/rpackagemake1/","year":"2021"},{"content":"在数据分析时，代码的运行速率常常是我们所关心的话题之一。小Q往期的文章中也有介绍通过并行计算提升R运行速率的文章。然而R代码自身的运行速率偏慢，具有某种“先天缺陷”。因此，并行R函数的运行时间有时仍然不尽人意。本期小Q与大家分享R的另一提速方式：Rcpp之效率瓶颈突破。\n1 什么是Rcpp 在往期的文章中曾讲过，我们在R中尽量采用向量化或内建函数运算，避免自编循环带来的额外时间消耗。然而，算法中有时难以避免循环，若循环部分耗时严重，直接并行含有该循环的算法显得力不从心，此时又该如何？稍有编程知识的读者可能会想到，较低级的语言在执行循环时更高效。因此，将算法中耗时严重的循环用较低级的语言重新编写是提速的可行思路。、\n Rcpp程序包便是构建R与C++之间的重要桥梁。有了Rcpp程序包的支持，我们可以很容易地将C++程序接入到R中，也可以在C++中使用R的数据类型甚至函数。\n 其实将R和C++整合的想法由来已久，早在2001年Bates和DebRoy就已尝试过这种思路。Rcpp程序包最早在2005年出现，当时是作为RQuantLib的扩展包。在2006年时，Rcpp有了自己的名字并成为一个CRAN扩展包。不过，很长一段时间内，Rcpp程序包没有较大的更新发展。从2008年开始，Eddelbuettel对其进行重新开发后，Rcpp的更新和扩展有了积极的支持。\n2 Rcpp基本使用 2.1 安装Rcpp 我们在R中直接执行install.packages(\u0026quot;Rcpp\u0026quot;)就可以安装Rcpp程序包。不过，仅仅安装程序包并不能直接在R中调用C++程序，因为我们需要为之搭建C++编译的环境。在Windows系统中，C++程序的编译由Rtools工具包支持。我们可在CRAN上下载并安装Rtools工具包，下载链接为\nhttps://cran.r-project.org/bin/windows/Rtools/  安装尽量选择默认配置，注意要将Rtools的目录放到环境变量中。此外，我们还要注意R自身的安装路径中不能含有空格，否则编译C++程序时会报错。一般注意以上两点，我们的准备工作就可以顺利完成。\n2.2 用cppFunction转换C++函数 函数cppFunction()可以用来在R中调用C++函数。例如，考虑Fibonacci数列的计算问题 $$ f_0 =0,f_1=1,f_n = f_{n-1} + f_{n-2}. $$ 那么，我们可以采用下面R和C++交互的形式来计算给定 $n$ 的Fibonacci数：\nlibrary(Rcpp) cppFunction(code = \u0026quot; int fibonacci(const int n){ if(n \u0026lt; 2){ return n; }else{ return(fibonacci(n-1) + fibonacci(n-2)); } } \u0026quot;)  其中code部分就是一段C++代码。运行上面的命令后，就可以在R中调用函数fibonacci()，例如\nfibonacci(0) # [1] 0 fibonacci(1) # [1] 1 fibonacci(10) # [1] 55  2.3 用sourceCpp转换C++函数 当C++程序代码较多时，直接用cppFunction()来调用不太合适，将其单独保存成.cpp文件并使用sourceCpp()加载调用更为灵活方便。我们在RStudio中选择新建C++ File就可以在工作目录建立一个.cpp文档，RStudio默认会给出一个C++的示例，如下所示：\n#include \u0026lt;Rcpp.h\u0026gt; using namespace Rcpp; // [[Rcpp::export]] NumericVector timesTwo(NumericVector x) { return x * 2; }  上述C++代码有这几个地方值得注意：\n 头文件。代码#include \u0026lt;Rcpp.h\u0026gt;引入了名为Rcpp的头文件，表明代码需要使用到Rcpp中的模板或库等内容。事实上，Rcpp程序包简化了R中编写C++程序的过程，其中的语法糖（Sugar） 可以让用户在保持代码性能的同时使用类似R中的代码进行编程；此外，Rcpp程序包也让C++程序的编译、连接、接口等问题隐式进行，大大降低了代码迁移的负担。 输出。C++程序中编写函数文件若想输入到R中进行调用，须在函数前加上// [[Rcpp::export]]这种特殊的注释；反之，如果不想将函数输出到R中，则可以省略这段代码。   本文并不涉及Rcpp用法的细节，因为这部分的内容实在过于繁多。我们这里更多的展示Rcpp对代码速率的提升效果，为R代码加速提供不同的解决方案，Rcpp细节的内容则需要读者自行学习。\n 我们将该文档保存后，用sourceCpp()加载，等待后台编译结束后就可以使用该文档中C++定义的函数timesTwo()，例如\nsourceCpp(\u0026quot;timesTwo.cpp\u0026quot;) timesTwo(5) # [1] 10  3 Rcpp与R效率对比 回到文章开始的话题，使用Rcpp优化R代码究竟带来多大的效率提高？小Q对此无法给出一个理论的结果。不过，我们可以用具体的案例直观感受一下优化前后的运行时间差异。下面代码都是在我的Matebook X Pro上跑的，CPU为Intel(R) Core(TM) i5-10210U，内存为16GB，R版本为4.0.5。\n考虑一个二分类数据集，其中正、负数据集服从双峰分布，即两个均值向量不同但协方差矩阵相同的多元正态分布。具体来说，正数据集的均值向量分别是\n$$ \\mu_1^+=(-0.3,0.7)^T,\\mu_2^+=(0.4,0.7)^T $$\n负数据集的均值向量分别是\n$$ \\mu_1^-=(-0.7,0.2)^T,\\mu_2^-=(0.3,0.2)^T $$\n协方差矩阵皆为 $V = diag(0.02, 0.06)$。\n我们按照上述设定产生等量的250个训练样本，使用LSVM（Lagrangian SVM）求解分类直线，算法基于坐标下降（Coordinate Decent）法设计。采用纯R编写的函数记为 funR，核心循环用C++重写其余保持不变的函数记为funC。由于LSVM中包含 一个调节参数 $\\lambda_1$，我们分别查看funR和funC在 $\\lambda_1\\in {2^{-8},2^{-7},\\ldots,2^7,2^8}$ 中寻找最优参数的表现，如下所示：\n## OptSearch_funR：funR寻找最优参数的函数 ## OptSearch_funC：funC寻找最优参数的函数 tunseq \u0026lt;- 2^seq(-8, 8, 1) # funR总耗时 system.time(OptPar_funR \u0026lt;- OptSearch_funR(x, y, tunseq)) # 用户 系统 流逝 # 15.10 0.00 15.13 # funC总耗时 system.time(OptPar_funC \u0026lt;- OptSearch_funC(x, y, tunseq)) # 用户 系统 流逝 # 4.93 0.01 4.95 op_funR \u0026lt;- funR(x, y, lam1=OptPar_funR[2], kernel = \u0026quot;linear\u0026quot;) op_funC \u0026lt;- funC(x, y, lam1=OptPar_funC[2], kernel = \u0026quot;linear\u0026quot;) op_funR$w # funR得到的法向量估计 # [,1] # [1,] 0.1588028 # [2,] 2.3881889 op_funC$w # funC得到的法向量估计 # [,1] # [1,] 0.1588028 # [2,] 2.3881889 op_funR$b # funR得到的截距估计 # [1] -1.201495 op_funC$b # funC得到的截距估计 # [1] -1.201495  从上述的运行结果来看，funR寻找最优参数耗时15.10秒，而用C++改良后的funC则只需4.93秒，速率提升较为明显。此外，我们也看到两者找到的最优参数、分类直线的法向量和截距项估计均相同——这是显然的，因为我们仅仅用C++重写了部分代码，相应的精度等设置并未改变，则结果自然与原始函数一致。\n上述的数据和分类直线如上图所示。其中蓝色*表示正数据点，红色o表示负数据点，黑色实线为分类直线。\n5 结语 本文简单介绍了Rcpp程序包的功能，以及展示了使用Rcpp程序包后对原始R代码速率的提升幅度。关于Rcpp的详细使用方法，读者可以阅读Rcpp程序包开发者Dirk Eddelbuettel所著的《Rcpp：R与C++的无缝整合》（英文书名《Seamless R and C++ Integration with Rcpp》）。虽然C++在短时间内难以掌握，但是通过简单地学习一些C++命令和Rcpp程序包的使用方法，完全可以很快上手并优化手中的R代码。\n","id":9,"section":"posts","summary":"在数据分析时，代码的运行速率常常是我们所关心的话题之一。小Q往期的文章中也有介绍通过并行计算提升R运行速率的文章。然而R代码自身的运行速率偏","tags":["R语言","算法"],"title":"效率瓶颈突破：Rcpp简介","uri":"https://qkai-stat.github.io/2021/04/rcpp-synopsis/","year":"2021"},{"content":"有偏估计是统计学中一个重要的概念，与之对应的是无偏估计。从字面意思看，有偏估计意为有偏差的参数估计，而无偏估计则是没有偏差的参数估计。在实际问题中估计未知参数时，我们自然希望估计值和真实值之间的偏差越小越好。如此说来，引入无偏估计的目的何在？研究无偏估计的意义又何在呢？\n本文就和大家一起浅谈线性模型中的有偏估计。\n1 均方误差的概念 为了说明引入无偏估计的目的，我们首先给出均方误差的概念。假设某实际问题中的目标对象（$y$）和与之相关的若干非随机量（$x_1,\\ldots,x_p$）之间为线性关系，这个模型便可以写成 $$ y = \\beta_1 x_1 + \\ldots + \\beta_p x_p + \\epsilon $$ 其中 $\\beta_1,\\ldots,\\beta_p$ 为系数，也就是我们需要估计未知的非随机参数；$\\epsilon$ 是随机扰动（搜集数据时会面临各种各样的误差，这些不确定性因素归结在一起就是随机扰动项）。从上述的模型中不难看出，虽然 $\\beta$ 和 $x_i$ 都是非随机的，但是 $y$ 的求和式中含有随机扰动 $\\epsilon$，则 $y$ 是随机的。\n如果大家有统计理论基础，就知道估计上述的未知系数有很多成熟的方法（参见往期文章）。我们以最小二乘为例，它给出的系数估计如下 $$ \\hat{\\beta}=(X^TX)^{-1}X^TY $$ 其中 $\\hat{\\beta}=(\\hat{\\beta}_1,\\ldots,\\hat{\\beta}_p)$ 是各个系数估计组成的向量，$X\\in \\mathbb{R}^{n\\times p}$ 和 $Y\\in \\mathbb{R}^{n}$ 分别是实际搜集得到的 $n$ 个数据组成的数据矩阵和目标向量。\n最小二乘这个方法究竟好不好呢？一个简单的衡量标准就是把最小二乘估计的结果（$\\hat{\\beta}$）与真实系数（$\\beta$）做比较，差距越小自然就说明这个方法越好。例如，采用每个系数估计差值的平方和衡量这个方法的性能，也就是 $$ \\Vert \\hat{\\beta} - \\beta\\Vert^2 = \\sum_{i=1}^p (\\hat{\\beta}_{i} - \\beta_i)^2 $$\n 采用平方和的一个好处就是计算上（例如求导）非常方便。当然，平方和衡量标准不是唯一的，你大可以采用绝对值的和等合理的标准。\n 这时，一个棘手的问题来了：最小二乘估计中含有 $Y=(y_1,\\ldots,y_n)^T$是个随机量，那么上述的误差平方和就是随机的，这可怎么用来作为衡量标准呢？\n所幸统计中关于随机量是有刻画的，就是各种各样的随机分布。一旦我们假定 $\\epsilon$ 服从某个分布 $F$，就能通过数学方法知道 $y$ 的分布，也就能计算出最小二乘 $\\hat{\\beta}$ 的分布。因此，尽管上面的误差平方和是随机的，我们还是可以从统计上对它进行界定。一般我们采用期望来进行界定： $$ MSE=E\\Vert \\hat{\\beta} - \\beta\\Vert^2 $$ 上述估计误差平方和的期望称之为均方误差（Mean Square Error，MSE）。这里采用期望的意思可以简单理解成，误差平方和 $\\Vert \\hat{\\beta}-\\beta\\Vert^2$ 这个随机量在 MSE 上下“对等”波动。因此，MSE如同 $\\Vert \\hat{\\beta}-\\beta\\Vert^2$ 这个随机量的一个平均大小，用MSE界定还是有道理的。\n2 鱼和熊掌不可兼得 既然期望可以反映一个随机量的性质，那么把 $\\hat{\\beta}$ 的期望 $E\\hat{\\beta}$ 考虑进来会有什么样的结果呢？我们对MSE进行一些数学恒等变换可知 $$ \\begin{split} E\\Vert \\hat{\\beta} - \\beta\\Vert^2 \u0026amp;= E\\Vert \\hat{\\beta} - E\\hat{\\beta} + E\\hat{\\beta} - \\beta\\Vert^2\\newline \u0026amp;=E\\Vert \\hat{\\beta} - E\\hat{\\beta}\\Vert^2 + \\Vert E\\hat{\\beta} - \\beta\\Vert^2 \\end{split} $$ 前者 $E\\Vert \\hat{\\beta} - E\\hat{\\beta}\\Vert^2$ 是估计量 $\\hat{\\beta}$ 各个估计的方差之和；后者 $\\Vert E\\hat{\\beta} - \\beta\\Vert^2$ 就是估计量 $\\hat{\\beta}$ 各个估计偏差的平方和。方差反映了这个估计 $\\hat{\\beta}$ 自身的波动情况，偏差则反映了这个估计 $\\hat{\\beta}$ 的期望与真实值之间的差距。无偏估计意为该估计的偏差为零，有偏估计则意为该估计的偏差非零。\n 小Q唠叨这么多才给出偏差的定义，是希望用例子的形式向非统计专业的朋友传达相应的统计思维。相对于冰冷的数学公式定义，理解整个统计过程和思想才最重要。\n 换句话说，一个估计方法的好坏可以用MSE来衡量，而MSE又可以分解成这个估计自身的方差与偏差平方的和。好的估计方法自然希望方差和偏差都要小。那么，无偏估计的偏差为零，这肯定比有偏估计（偏差大于零）的MSE要小，为什么还要引入有偏估计呢？\n 如果你这么想，只能说too young， too naive了！\n 实际中我们发现，方差和偏差两个东西如同鱼和熊掌，我们在降低方差的同时往往会增加偏差，在降低偏差的同时往往又会增加方差。所以，无偏估计虽然好，但可能会产生一个很大的方差，也即一个很大的MSE，这自然不是我们想要的。\n 无偏但方差大的方法在实际问题中肯定不适用。例如真实值为100，A方法无偏但方差为10000；B方法有偏，估计的期望为101，但是方差为1。由于真实数据存在随机性，当搜集到某些不太好的数据时，A方法估计出了150（方差即为波动），B最差估计为102。此时，大家自然倾向于选择B方法。\n 综上所述，在MSE的准则下，引入和研究有偏估计是非常有意义的！\n3 复共线性对MSE的影响 我们延续第一节的线性回归问题来具体阐述引入有偏估计的意义何在。为分析问题方便，考虑到数据矩阵 $X$ 的Gram矩阵 $X^TX$ 是对称的，则由矩阵理论可知必然存在一个正交方阵 $Q$ 使得 $$ Q^TX^TXQ = \\Lambda $$ 其中 $\\Lambda = diag(\\lambda_1, \\ldots, \\lambda_p)$ 为对角矩阵，对角元素 $\\lambda_1,\\ldots,\\lambda_p$ 为Gram阵的特征值。这时，我们将原来的线性模型做一个恒等变换得到 $$ \\begin{split} y \u0026amp;= XQQ^T\\beta + \\epsilon\\Rightarrow\\\ny \u0026amp;= Z\\eta + \\epsilon \\end{split} $$ 这里 $Z = XQ$ 和 $\\eta = Q^T\\beta$。我们称上述变换后的模型为线性模型的典则（canonical）形式。典则形式下系数的最小二乘估计为 $$ \\hat{\\eta} = (Z^TZ)^{-1}Z^TY = \\Lambda^{-1}Z^TY $$ 根据 $\\hat{\\beta} = Q\\hat{\\eta}$ 便可轻松得到原始系数的估计。\n将原始的线性模型转换成典则形式有什么好处呢？好处就是便于我们分析最小二乘估计的性能。 在线性模型中随机扰动 $\\epsilon$ 通常假设成独立同分布于均值为0方差为 $\\sigma^2$ 的正态分布 $N(0, \\sigma^2)$。于是，典则形式下最小二乘估计的偏差为 $$ \\begin{split} E\\hat{\\eta} \u0026amp;= E((Z^TZ)^{-1}Z^TY)\\newline \u0026amp;=E((Z^TZ)^{-1}Z^T(Z\\eta + \\epsilon))\\newline \u0026amp;=E((Z^TZ)^{-1}Z^TZ\\eta) + E((Z^TZ)^{-1}Z^T\\epsilon)\\newline \u0026amp;=\\eta + 0 = \\eta \\end{split} $$ 所以，最小二乘估计 $E\\hat{\\beta} = E(Q\\hat{\\eta}) = Q\\eta = \\beta$ 是一个无偏估计。再来看它的MSE $$ \\begin{split} MSE(\\hat{\\eta})\u0026amp;=E\\Vert \\hat{\\eta}-\\eta\\Vert^2 = E\\Vert \\hat{\\eta} - E\\hat{\\eta}\\Vert^2\\\n\u0026amp;= E\\Vert(Z^TZ)^{-1}Z^T\\epsilon\\Vert^2\\newline \u0026amp;= E(\\epsilon^TZ\\Lambda^{-2}Z^T\\epsilon)\\newline \u0026amp;= Etr(\\epsilon^TZ\\Lambda^{-2}Z^T\\epsilon)\\newline \u0026amp;= \\sigma^2\\sum_{i=1}^p \\frac{1}{\\lambda_i} \\end{split} $$ 由于 $\\beta = Q\\hat{\\beta}$，$Q$ 是一个正交方阵，则 $$ \\begin{split} MSE(\\hat{\\beta}) \u0026amp;= E\\Vert \\hat{\\beta}-\\beta\\Vert^2= E\\Vert Q\\hat{\\eta}-Q\\eta\\Vert^2\\\n\u0026amp;= E\\Vert Q(\\hat{\\eta}-\\eta)\\Vert^2\\newline \u0026amp;= E\\Vert \\hat{\\eta}-\\eta\\Vert^2\\newline \u0026amp;= \\sigma^2\\sum_{i=1}^p \\frac{1}{\\lambda_i} \\end{split} $$ 换句话说，最小二乘估计的MSE实际上和数据矩阵 $X$ 的Gram阵 $X^TX$ 的特征值密切相关。\n 特别的，当 $X$ 的列之间存在（或近似存在）线性关系（也就是变量 $x_1,\\ldots,x_p$ 之间存在相关性）时，Gram阵奇异，其最小特征值为（或近似）零，MSE此时趋于无穷大！！！\n 数据矩阵 $X$ 列线性相关的这种情形称之为复共线性（multi-collinearity）。当发生复共线性时，根据前面的分析可知，最小二乘估计的MSE会变得非常大，此时它的性能很不稳定。虽然它还是一个无偏估计量，但是在MSE的准则下并不是一个很好的估计方法。\n4 岭估计 我们观察到，之所以在复共线性时最小二乘估计的MSE很大，是因为它的估计中涉及到Gram阵 $X^TX$ 的求逆。因此，一个朴素的想法就是给Gram阵加上某个调整项，使得在求逆时降低奇异带来的影响。考虑下面的的估计 $$ \\tilde{\\beta} = (X^TX + kI)^{-1}X^TY $$ 其中 $k\u0026gt;0$ 是一个事先给定的常数。根据前面的推导，不难求出该估计量的MSE为 $$ MSE(\\tilde{\\beta}) = \\sigma^2\\sum_{i=1}^p \\frac{\\lambda_i}{(\\lambda_i + k)^2} + k^2\\sum_{i=1}^p \\frac{\\beta_i^2}{(\\lambda_i + k)^2} $$ 当 $X$ 存在复共线性时，虽然最小特征值趋于零，但是若取定适当的 $k$ ，便不会发生MSE趋于无穷（或非常大）的结果，也就避免了复共线性带来的影响。注意到， $$ \\begin{split} E(\\tilde{\\beta}) \u0026amp;= E(X^TX + kI)^{-1}X^TY\\newline \u0026amp;= E(X^TX + kI)^{-1}X^T(X\\beta + \\epsilon)\\newline \u0026amp;= (X^TX + kI)^{-1}X^TX\\beta\\newline \u0026amp;= (X^TX + kI)^{-1}(X^TX + kI - kI)\\beta\\newline \u0026amp;= \\beta - k(X^TX + kI)^{-1}\\beta\\newline \u0026amp;\\neq \\beta \\end{split} $$ 显然是一个有偏的估计。也就是说，我们通过牺牲最小二乘的无偏性，换来了复共线性情形下MSE的显著减小。这与第3节阐述的内容一致——鱼和熊掌不可兼得。\n我们称 $\\tilde{\\beta}$ 为参数 $k$ 的岭估计（ridge estimator）。Hoerl和Kennard在1970年研究表示，岭估计可以等价成求解一个带有二次罚项的最小二乘回归问题： $$ \\tilde{\\beta} = \\min \\frac12\\Vert Y-X\\beta\\Vert^2 + \\lambda\\sum_{i=1}^p\\beta_i^2 $$ 其中 $\\lambda\u0026gt;0$ 为模型的调节参数。这个形式和当前流行的lasso类罚函数模型非常相似，感兴趣的朋友可以参见之前关于lasso模型的文章。限于篇幅，就不展开介绍了。\n从前面的分析可以看出，岭估计中的参数 $k$ 的选取十分重要。当 $k=0$ 时，这就是普通的最小二乘，显然不能处理复共线性的问题；当 $k$ 很大时，岭估计可以克服复共线性问题，但此时的偏差变得很大，这也不是我们所希望的。实际使用中，我们可以利用交叉验证（cross validation，CV）、广义交叉验证（generalized cross validation，GCV）等准则来选取岭参数。\n5 一些说明 第一，有偏和无偏不是专门为线性模型而设的概念。这里从线性模型的角度来谈有偏估计，一方面是希望可以帮助理解有偏估计的含义，另一方面是让大家可以看到这个概念在实际问题中的意义。此外，线性模型中发展建立起的有偏估计方法已然成了一门重要的统计理论，因此在线性模型的框架下谈有偏估计更加合适。\n第二，始终都要明确一点：采用有偏估计是无奈之举。如果可以实现无偏的同时MSE也很小，就果断采用无偏估计。因此，没有复共线性问题或普通最小二乘表现很好时，一般没必要采用有偏估计。无偏始终是我们的追求，只有在实现无偏会带来很大的负面影响时，我们才考虑牺牲无偏来换取负面影响的降低。\n第三，岭估计不是唯一的解决复共线性的有偏估计方法。除岭估计外，刘估计、双参数估计等类岭估计，主成分估计等都是解决复共线性问题的可行有偏估计方法。感兴趣的朋友可以自行了解相关的内容。\n6 模拟分析 我们用一个小例子说明复共线性下普通最小二乘的缺陷和岭估计的优势。考虑下面的模型 $$ y = x_1 + 2x_2 + x_3 + \\epsilon,\\epsilon\\sim N(0,1) $$ 其中 $x_1, x_2, x_3$ 存在如下的线性关系： $$ x_3 = x_1 + x_2 + 0.001\\cdot\\xi,\\xi\\sim N(0,1) $$ 我们生成100个数据，首先计算Gram矩阵的三个特征值大小，结果为\nx1 \u0026lt;- rnorm(100) x2 \u0026lt;- rnorm(100) x3 \u0026lt;- x1 + x2 + 0.001*rnorm(100) y \u0026lt;- x1 + 2*x2 + x3 + rnorm(100) x \u0026lt;- cbind(x1, x2, x3) x_eigen \u0026lt;- eigen(t(x)%*%x) x_eigen$values ## [1] 3.527229e+02 1.155482e+02 3.062182e-05  可见最小特征值为 $10^{-5}$ 阶，非常小。再来看普通最小二乘估计的结果\nhat_beta \u0026lt;- solve(t(x)%*%x)%*%t(x)%*%y hat_beta ## [,1] ## x1 105.2502 ## x2 106.5857 ## x3 -103.5417  三个系数估计同真实值 $(1,2,1)$ 差距十分明显，也即是MSE异常大。如果我们利用前面的模型再产生100个新的数据，用上述估计的系数来进行预测，结果如下图所示\n可见，受到复共线性影响的最小二乘估计表现十分糟糕。\n我们使用R中的MASS程序包进行岭估计。载入程序包，我们设置 $[0,1]$ 之间等距100个点为模型岭参数的候选值，运行下面的代码训练岭模型\ndata \u0026lt;- data.frame(x1 = x1, x2 = x2, x3 = x3, y = y) ridge_fit \u0026lt;- lm.ridge(y ~ ., lambda = seq(0, 1, length = 100), data, model = TRUE)  查看三个系数随岭参数增大的关系图（岭迹图）\n可见随着岭参数的增大，系数估计趋于稳定。结合第5节第二点的说明，我们要尽量选择较小的岭参数，在克服复共线性问题的同时尽量减小偏差。直接铜鼓观察图像来选择岭参数十分困难，我们利用程序包自带的函数选择GCV准则下的最优岭参数以以及对应的岭估计\nridge_fit$lambda[which.min(ridge_fit$GCV)] ## [1] 0.3131313 ridge_fit$coef[, which.min(ridge_fit$GCV)] ## x1 x2 x3 ## 0.5302685 2.0396480 1.8136743  这时的岭估计和真实的系数就十分接近了。我们把前面产生的100个新数据用岭估计的系数进行预测，结果如下图所示\n可见预测结果和真实值的差距非常小！这也印证了有偏估计在实际问题中确实可以发挥巨大的威力！\n7 结语 本期我们主要介绍了线性模型中的有偏估计的概念，具体结合岭估计展示了有偏估计在克服复共线性问题时的威力。关于有偏估计的理论这里只是冰山一角，感兴趣的朋友可以查阅相关文献了解更多关于有偏估计的知识。欢迎在下方的评论区一起交流~~\n","id":10,"section":"posts","summary":"有偏估计是统计学中一个重要的概念，与之对应的是无偏估计。从字面意思看，有偏估计意为有偏差的参数估计，而无偏估计则是没有偏差的参数估计。在实际","tags":["多元统计","回归分析"],"title":"有偏估计","uri":"https://qkai-stat.github.io/2020/11/biased-estimate/","year":"2020"},{"content":"主成分分析（principle component analysis，PCA）是经典的降维分析工具之一，在数据挖掘、图像处理、信号分析等众多领域被广泛的研究和应用。PCA最早由现代统计科学的创立者、英国数学家Pearson（1901）针对非随机变量提出，后经Hotelling（1933）推广到随机变量而逐渐完善成熟。PCA的背后有着坚实的数学理论，它将原始变量转换成若干个综合变量，在损失较少信息的前提下，抓住问题的主要矛盾、简化问题从而提升解决问题的效率。\n本期我们主要向大家介绍PCA的基本原理以及如何在R中实现PCA方法。\n1 引言 早在中学时代，我们就已经接触“数形结合”的思想。在很多实际问题中，我们思考如何处理搜集而来的数据之前，一个朴素的想法便是先通过画图观察数据中是否存在某种规律。比如，下面的图（a）就表示某二变量数据的散点图。\n我们之所以将数据对应成图（a）的形式，是因为基于如下的考虑。通常数据中包含很多变量，如研究健康问题中的血氧浓度、肺活量等，这些变量可以看成是对同一问题（健康）的不同角度的描述。那么，我们自然可以将问题中的各个变量与几何图形中的各个维度（如 $x_1, x_2$）进行一一对应：图形上的点在各个维度的取值等价于原始数据在相应变量上的取值。\n这样由原始变量对应的几何维度，我们称之为坐标系。一般情况下，坐标系的各个维度和数据中的原始变量一一对应，每个维度取值得到的交点和数据中的一个观测样本相对应。\n如果你明白了上述“数形结合”的思想，那么现在来思考这样一个问题：\n 坐标系的维度是否非要和原始变量一一对应？\n 也就是说，我们可不可以用其他的维度对相同的数据进行描述刻画，此时数据分析变得更加简单、规律更容易被发现？事实上， 以图（a）为例，我们确实可以找到一个新的坐标系来描述相同的数据，如下所示\n我们按照原始变量建立坐标系之后，可以将坐标系的零点 $O$ 移动到图（b）中的 $O'$ 处，并以 $p_1,p_2$ 为新的维度来描述数据。在新的坐标系下，数据主要在 $p_1$ 方向分布，而 $p_2$ 方向则变动不大。换句话说，我们完全可以只用 $p_1$ 来描述数据。这样，原来要用两个变量（维度）刻画的数据，在新坐标系下只需一个新变量（维度）刻画即可，也即实现的降维的目的。\n进一步观察可以发现：如果事先将原始数据对各个变量进行中心化（每个变量的数据减去其平均值），新坐标和原始坐标的零点就重合了且可以看成原始坐标的一个旋转，如图（c）所示。\n此外，我们也注意到当前新坐标的方向 $p_1,p_2$ 可以写成原始坐标 $x_1,x_2$ 的一个线性组合。这一点对寻找新坐标很有启发意义。\n总之，上述思考表明确实可以通过原始变量建立新变量来描述同一数据，且可能降低分析问题的难度。这些对于问题朴素的思考就蕴含了主成分分析（principle component analysis）的基本思想。\n2 主成分分析（PCA） 假设研究数据中的变量分别为 $x_1,\\ldots,x_p$，由于原始变量繁杂不易于直接分析，主成分分析便希望通过构建原始变量的一系列线性组合找到若干主成分变量进行分析，在简化问题的同时又保留了原始数据的绝大多数信息。\n结合第一节的思考，对于PCA的第一个要求，我想大家应该豁然开朗了：PCA要寻找的主成分变量其实就是坐标变换后的新坐标。因为将原始数据中心化后，新坐标系中的各个方向（新变量）就是原始方向（变量）的线性组合。所以，PCA思想的背后其实就是线性代数中的坐标变换。这也表明PCA背后有坚实的数学理论基础。\n找到构建新变量的思路后，还要思考一个问题——变换到具体哪个坐标才是我们想要的？这要利用PCA的第二个要求：保留原始数据绝大多数信息。\nPCA对于数据信息的描述是基于方差的。方差反映数据的波动范围：方差越大波动越大，方差越小波动越小。波动大，表示该方向上数据有很大的变化，也即是参考这个方向以后对各个数据会有更多的认识。波动小，表示该方向上数据没有显著的改变，也即是参考这个方向以后对各个数据不会有更多的认识。所以，方差大的方向信息丰富，反之则信息较少。\n 以图（c）为例，数据在 $p_1$ 方向的分布范围很大，可以刻画原始数据丰富的细节；而在 $p_2$ 方向分布范围较小，可以忽略。而数据在 $x_1$ 和 $x_2$ 方向的分布范围都很大，因此我们无法忽略哪个。\n 下面，我们以第一主成分为例，介绍求解主成分的基本思路。\n给定原始变量 $x_1,\\ldots,x_p$，我们用他们的一个线性组合表示第一主成分 $$ y_1 = \\alpha_{11} x_1 + \\ldots + \\alpha_{1p} x_p $$ 根据方差最大原理，上述的系数要满足 $$ \\max Var(y_1) $$ 由于对系数没有要求，单纯最大化的得不到最终的解。因此，我们对系数添加一个正则化的约束 $$ \\max Var(y_1) \\quad s.t.\\quad \\Vert \\alpha_1\\Vert=1 $$ 其中 $\\alpha_1 = (\\alpha_{11},\\ldots,\\alpha_{1p})^T$，$\\Vert \\cdot\\Vert$ 表示欧式范数。进一步假设我们针对 原始变量 $x_1,\\ldots,x_p$ 观测到的数据矩阵（已列中心化）为 $X\\in \\mathbb{R}^{n\\times p}$，则上述的优化问题变成 $$ \\max \\alpha_1^T(X^TX)\\alpha_1 \\quad s.t.\\quad \\Vert \\alpha_1\\Vert=1 $$ 它的最优解为Gram矩阵 $X^TX$ 的最大特征值，$\\alpha_1$ 为最大特征值对应的特征向量。\n如果已经求解出前 $q-1$ 个主成分，对于第 $q$ 个主成分 $y_q$ 有 $$ y_q = \\alpha_{q1} x_1 + \\ldots + \\alpha_{qp} x_p $$ 满足 $$ \\max \\alpha_q^T(X^TX)\\alpha_q \\quad \\\ns.t.\\quad \\Vert \\alpha_q\\Vert=1, \\alpha_q^T\\alpha_1=0,\\ldots, \\alpha_q^T\\alpha_{q-1}=0 $$ 这等价于求一个 $p\\times q$ 的列正交矩阵使 $X^TX$ 正交化的问题。当然，PCA还可以通过矩阵的SVD进行求解。具体的思路不再赘述，有兴趣的读者可以参考其他资料。\n从上述的分析中可以看出，PCA的背后是坐标变换，求解的过程利用到了依次方差最大的原则。当我们求解的各个主成分的方差累计贡献率（所求主成分的方差和占总方差的比例）达到一定比例时（一般在70%以上），就可以停止继续求解主成分了。我们对数据的分析就可以基于所求主成分展开。\n3 若干说明 3.1 PCA可行性检验 PCA是数据降维的一个有效手段，可以透过繁杂的原始变量直击问题的主要矛盾，但并不意味着我们可以滥用PCA。从前面的分析可以看出，数据在各个方向上的分布有明星偏倚，才可以考虑采用PCA处理。假如数据的分布没有明显的偏倚，如空间中以球形分布，如图（d）所示，就不适用PCA进行处理了。\n因此，我们在使用PCA之前要对数据进行检测，通常是 KMO和Bartlett球形检验。\n3.2 主成分的含义 通过前面的讨论不难发现，主成分找到后虽然可以解释原始数据的绝大部分信息，但是它是原始变量的线性组合，其自身的含义难以理解。例如，假设图（c）中 $x_1$ 表示血氧浓度，$x_2$ 表示肺活量，$p_1 = 2x_1 + x_2$ 是第一主成分，它包含了原始数据的大部分信息。然而，原始问题的变量 $x_1,x_2$ 含义很清晰，$p_1$ 就很难基于实际问题理解其含义了。因此，PCA得到的主成分的含义解释，往往在问题分析中颇费精力。\n之所以难以理解主成分的含义，是因为主成分和所有的变量都有关。因此，一个提升含义的直接办法就是构建稀疏主成分，也即是主成分和较少的变量有关而在其余变量上稀疏为零或接近零。经典的办法是进行主成分的旋转。不过，旋转主成分丧失了依次方差最大的原则。关于这方面的内容，可以参考相关文献。\n4 R实现PCA 在R中有专门的程序包实现PCA方法。首先我们安装并载入所需的程序包\ninstall.packages(\u0026quot;psych\u0026quot;) library(psych)  数据选取的是某地区高校大学生职业生涯规划现状的问卷，包含10 个变量，其含义如下表所示：\n在做PCA之前，对数据进行KMO和Bartlett球形检验：\n\u0026gt; KMO(as.matrix(mydata)) Kaiser-Meyer-Olkin factor adequacy Call: KMO(r = as.matrix(mydata)) Overall MSA = 0.88 MSA for each item = x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 0.58 0.51 0.65 0.63 0.58 0.91 0.93 0.91 0.91 0.95 \u0026gt; cortest.bartlett(cor(mydata), n = nrow(mydata)) $chisq [1] 7929.074 $p.value [1] 0 $df [1] 45  可见KMO系数 $\u0026gt;0.5$ 且Bartlett检验的P值 $\u0026lt;0.05$，说明数据很适合进行主成分分析。\n程序包psych中进行PCA的函数是principal，它的基本使用方式为\nprincipal(data, nfactors = 1, rotate = \u0026quot;varimax\u0026quot;, scores = T)  其中data是我们要分析的数据的相关系数矩阵，直接输入原始数据亦可。nfactors是保留的主成分个数，需要被实现给定。rotate表示在求解过程中采用哪种旋转方式，默认varimax方式。varimax是一种试图对载荷阵的列进行去噪，使得每个主成分和有限的变量有关，从而提升主成分的可解释性。scores表示是否输出主成分得分矩阵，默认输出。\nfa.parallel(cor(mydata), n.obs = nrow(mydata), fa = \u0026quot;both\u0026quot;, n.iter = 100)  由于principal函数要实现给定主成分的个数，我们先用psych中的fa.parallel进行主成分个数的选取：\n一般我们选择特征值大于1的主成分即可，如上面的碎石图所示，我们选择3个主成分进行后续分析。先看不使用旋转（rotate = \u0026quot;none\u0026quot;）的结果：\n\u0026gt; principal(mydata, nfactors = 3, rotate = \u0026quot;none\u0026quot;, scores = T) Principal Components Analysis Call: principal(r = mydata, nfactors = 3, rotate = \u0026quot;none\u0026quot;, scores = T) Standardized loadings (pattern matrix) based upon correlation matrix PC1 PC2 PC3 h2 u2 com x1 0.32 0.05 -0.85 0.83 0.172 1.3 x2 0.03 0.70 0.21 0.53 0.473 1.2 x3 0.05 0.70 0.14 0.52 0.482 1.1 x4 0.13 0.58 0.29 0.43 0.567 1.6 x5 -0.30 -0.50 0.68 0.81 0.192 2.3 x6 0.98 -0.04 0.09 0.97 0.033 1.0 x7 0.97 -0.03 0.09 0.96 0.044 1.0 x8 0.98 -0.05 0.09 0.96 0.037 1.0 x9 0.98 -0.05 0.08 0.96 0.038 1.0 x10 0.94 -0.14 0.08 0.90 0.100 1.1 PC1 PC2 PC3 SS loadings 4.89 1.60 1.37 Proportion Var 0.49 0.16 0.14 Cumulative Var 0.49 0.65 0.79 Proportion Explained 0.62 0.20 0.17 Cumulative Proportion 0.62 0.83 1.00  上述第一部分的结果描述的提取的主成分变量PC1、PC2和PC3与原始变量的线性关系；第二部分则是每个主成分的方差贡献率和累计方差贡献率，可见3个主成分的累计方差贡献率为1，完全可以解释原始数据的信息。\n进一步我们发现，第一主成分主要和 $x_6\\sim x_{10}$ 有关，即院校在大学生职业规划中的参与情况；第二主成分主要和 $x_2\\sim x_4$ 有关，即职业规划的实践形式；第三主成分主要和 $x_1,x_5$ 有关，即职业规划的授课辅导（和专题讲座呈负相关，看来大家比较反感在就业辅导时进行上课学习，这似乎比较符合现状）。\n上述的案例在不进行主成分旋转时也能有一个合理的解释，这并不适用于一般的数据。感兴趣的读者可以自行搜集数据进行分析比较，看看两者之间的差异。\n5 结语 本期我们的主要学习了PCA的基本原理和R实现的方式。最后还要提醒大家，PCA虽然应用广泛，但不能滥用，要针对不同数据自身的特点选择合适的模型。\n","id":11,"section":"posts","summary":"主成分分析（principle component analysis，PCA）是经典的降维分析工具之一，在数据挖掘、图像处理、信号分析等众多领域被广泛的研究和应","tags":["机器学习","多元统计"],"title":"主成分分析原理","uri":"https://qkai-stat.github.io/2020/10/pca/","year":"2020"},{"content":"在运用SVM处理非线性可分数据时，我们会用到核函数进行转换。与之同时，我们也常常遇到再生核希尔伯特空间（Reproducing Kernel Hilbert Space）的概念。这些听上去很高大上的名词，其基本含义并不是十分生涩。本文主要介绍再生核希尔伯特空间和核函数的相关内容。\n1 希尔伯特空间 现代数学研究问题常常以集合为对象，而赋予了某种结构或规则的集合便称为空间。所谓结构或规则，它必须可以描述集合中元素之间的某种关系，这个关系你可以自定义，如代数运算、距离、内积等等。下面介绍一些具体的空间概念，这对理解希尔伯特空间有很大的帮助。\n1.1 线性空间 线性空间就是定义了加法和数乘的空间。换句话说，任意空间中的两个元素 $a,b$ 和数 $\\lambda,\\mu$ 要满足下面的8条公理：\n $a + b = b + a$ $a + (b + c) = (a + b) + c$ $\\lambda(\\mu a) = (\\lambda\\mu)a$ $\\lambda(a + b) = \\lambda a + \\lambda b$ $(\\lambda + \\mu)a = \\lambda a + \\mu a$ $a + 0 = a$，即有零元素 $a + (-a) = 0$ $a\\cdot 1 = a$，即有单位1  给空间赋予加法和数乘之后，我们就可以进一步寻找空间中的一组基（basis），从而把空间中任一元素用基的线性组合表示。这样，研究空间的性质就简化了。\n1.2 度量空间 度量空间就是定义了距离的空间。空间中任意两个元素 $x,y$，距离 $d(x,y)$ 的定义需要满足下面三个条件：\n 非负性：$d(x,y) \\geq 0$，且 $d(x,y)=0 \\Leftrightarrow x = y$ 对称性：$d(x,y) = d(y,x)$ 三角不等式：$d(x,y) \\leq d(x,z) + d(z,y)$  通过定义距离，空间中的各个元素之间可以相互比较。定义了距离的线性空间叫线性度量空间。\n1.3 赋范空间 赋范空间就是定义了范数的空间。空间中任一元素 $x$，范数 $\\Vert x\\Vert$ 的定义需要满足下面三个条件：\n 非负性：$\\Vert x\\Vert \\geq 0$，且 $\\Vert x\\Vert = 0 \\Leftrightarrow x = 0$ 数乘法则：$\\Vert\\lambda x\\Vert = \\vert\\lambda\\vert \\cdot \\Vert x\\Vert$ 三角不等式：$\\Vert x + y\\Vert \\leq \\Vert x\\Vert + \\Vert y\\Vert$  通过定义范数，元素本身可以度量大小。另一方面，根据距离的定义可以将范数看成是从零点到 $x$ 的距离，也即是\n$$ \\Vert x\\Vert = d(x,0) $$\n但同时比距离的定义要严格，因为范数有数乘规则而距离不一定有。换句话说，有了范数定义可以直接根据该范数定义距离，反之则不行。\n例如\n$$ d(x,y) = \\Vert x - y\\Vert $$\n显然满足距离定义。而下述定义的距离\n$$ d(x, y) = \\frac{\\sqrt{\\sum (x_i - y_i)^2}}{1 + \\sqrt{\\sum (x_i - y_i)^2}} $$\n不满足范数定义的条件。因此范数是比距离更具体的一个概念，或者说赋范空间也是度量空间。\n此外，完备的赋范空间叫做巴拿赫空间。空间的完备性是指空间中的任一柯西序列的极限仍在该空间中，即取极限运算不会跑出空间。例如，实数集是完备的而有理数集是不完备的，因为有理数序列的极限可能是无理数。举例说明，考虑下面的数列\n$$ x_n = \\frac{[\\sqrt{3}n]}{n} $$\n其中 $[\\cdot]$ 是取整函数。显然 $x_n$ 是有理数，但不难看出 $x_n\\rightarrow \\sqrt{3}$ 是一个无理数。\n 柯西序列中提到了距离的概念，也就是说在定义完备空间之前，先要有距离的概念。所以完备空间也是完备度量空间\n 1.4 内积空间 内积空间就是定义了内积运算的空间。空间中任意两个元素 $x,y$，内积 $\u0026lt;x,y\u0026gt;$ 的定义需要满足下面三个条件：\n 对称性：$\u0026lt;x,y\u0026gt; = \u0026lt;y, x\u0026gt;$ 线性性：$\u0026lt;\\lambda x, y\u0026gt; = \\lambda \u0026lt;x, y\u0026gt;$ 正定性：$\u0026lt;x,x\u0026gt; \\geq 0$，且 $\u0026lt;x,x\u0026gt; = 0 \\Leftrightarrow x = 0$  对比范数的定义，给了内积定义后我们可以根据该内积定义范数\n$$ \\Vert x \\Vert = \\sqrt{\u0026lt;x,x\u0026gt;} $$\n但是范数的定义不满足上述的线性性，故而内积是比范数更具体的一个概念。内积将两个矢量与一个纯量连接起来，允许我们严格地谈论矢量的“夹角”和“长度”，并进一步谈论矢量的正交性。\n我们用一张图来描述线性空间、度量空间、赋范空间和内积空间的关系：\n1.5 欧几里得空间 欧式空间是定义了内积的有限维实线性空间。\n1.6 希尔伯特空间 希尔伯特空间是欧氏空间的一推广，即为完备的内积空间（允许无穷维且不限于实数情形）。希尔伯特空间的研究对象通常是函数，而函数可以看成无穷维的向量（相对于欧氏空间的有限维做了拓展）。因为对连续函数 $f(x)$ 可以按照 $x$ 进行采样，则函数可以表示成\n$$ (f(x_0), f(x_1),\\ldots, f(x_n)) $$\n采样的间隔越小，函数刻画的越精确。如果采用间隔趋于无穷，那么函数就变成一个无穷维的向量了。所以，函数空间内积可以类比欧式空间的内积定义成\n$$ \u0026lt;f(x), g(x)\u0026gt; = \\int f(x)g(x)dx $$\n从函数内积的定义来看，函数空间的元素（即函数）不是随意取的，起码要满足可积性可平方可积性。由此可以导出函数空间的范数\n$$ \\Vert f(x)\\Vert = \\sqrt{\u0026lt;f(x), f(x)\u0026gt;} $$\n 这部分以及接下来的内容都涉及函数的积分描述，我们默认所有操作都是可以进行的，只去理解其主要的思想。至于具体要满足何种条件、如何证明等，可以参考\n 2 核函数 有了函数 $f(x)$ 可以看成无穷维向量的意识后，我们可以把二元函数 $\\kappa(x, y)$ 看成一个无穷维的矩阵（类比两个方向无穷采样）。若 $\\kappa(x, y)$ 满足\n  正定性：即对任意的 $f(x)$ 都成立着 $$ \\iint f(x)\\kappa(x,y)f(y)dxdy \\geq 0 $$\n  对称性：即 $\\kappa(x, y) = \\kappa(y, x)$\n  则称之为核函数（正定核）。上述是从积分的角度定义的，Mercer定理告诉我们任何半正定的函数都可以作为核函数。不过，根据定义来检验某个函数是否为核函数仍然比较困难，好在已有一些被大家证明过的核函数可供选择，如\n 线性核：$K(x_i,x_j)=x_i^Tx_j$； 高斯核：$K(x_i,x_j)=\\exp(-\\frac{\\Vert x_i-x_j\\Vert^2}{\\sigma})$，$\\sigma\u0026gt;0$； Sigmoid核：$K(x_i,x_j)=\\tanh(\\beta x_i^Tx_j + \\theta)$，$\\beta\u0026gt;0,\\theta\u0026lt;0$；  在一般的欧氏空间中，我们可以定义 $n\\times n$ 阶矩阵 $A$ 的特征值和特征向量\n$$ Ax = \\lambda x $$\n当一个矩阵可以进行特征值分解时，其特征向量 $(u_i)_{i=1}^{n}$ 就构成了这个 $n$ 维空间的一组正交基，并且我们可以得到\n$$ A = \\sum_{i=1}^n \\lambda_i u_i {u_i^T} $$\n对照欧氏空间的特征值分解，我们可以定义希尔伯特空间的特征值和特征向量：\n$$ \\int \\kappa(x, y) \\psi(x)dx = \\lambda \\psi(y) $$\n对于不同的特征值 $\\lambda_1$ 和 $\\lambda_2$ 及对应的特征函数 $\\psi_1(x)$ 和 $\\psi_2(x)$，我们可以看到\n$$ \\begin{split} \\int \\lambda_1 \\psi_1(x)\\psi_2(x)dx \u0026amp;= \\int\\int\\kappa(y,x)\\psi_1(y)dy,\\psi_2(x)dx\\newline \u0026amp;= \\int\\int\\kappa(x,y)\\psi_2(x)dx,\\psi_1(y)dy\\newline \u0026amp;= \\int \\lambda_2 \\psi_1(x)\\psi_2(x)dx \\end{split} $$\n所以\n$$ \u0026lt;\\psi_1, \\psi_2\u0026gt; = 0 $$\n所以 $(\\psi_i)_{i = 1}^{\\infty}$ 就是原函数空间的一组正交基。类似的，核函数就可以写成基函数的和\n$$ \\kappa(x,y) = \\sum_{i = 1}^{\\infty} \\lambda_i \\psi_i(x)\\psi_i(y) $$\n3 再生核希尔伯特空间 现在我们考虑基为 $(\\sqrt{\\lambda_i} \\psi_i) _{i = 1}^{\\infty}$ 的希尔伯特空间 $\\mathcal{H}$。对于 $\\mathcal{H}$ 中的任意元素 $f(x)$ 和 $g(x)$，可以将其表示成基 $(\\sqrt{\\lambda_i}\\psi_i) _{i = 1}^{\\infty}$ 的和\n$$ f(x) = \\sum_{i = 1}^{\\infty} \\alpha_i \\sqrt{\\lambda_i}\\psi_i(x),\\quad g(x) = \\sum_{i = 1}^{\\infty} \\beta_i \\sqrt{\\lambda_i}\\psi_i(x) $$\n那么在 $\\mathcal{H}$ 中 $f(x)$ 和 $g(x)$ 就可以表示成下面两个无穷维向量\n$$ f = (\\alpha_1, \\alpha_2, \\ldots) _{\\mathcal{H}}^T,\\quad g = (\\beta_1, \\beta_2, \\ldots) _{\\mathcal{H}}^T $$\n两者之间的内积便为\n$$ \u0026lt;f, g\u0026gt; = \\sum_{i = 1}^{\\infty} \\alpha_i \\beta_i $$\n再来看核函数 $\\kappa(x, y)$，我们若固定其中一个变量如 $x = x_0$，则 $\\kappa(x_0, \\cdot)$ 可表示成无穷维矩阵 $\\kappa(x, y)$ 的第 $x_0$ 行。显然 $\\kappa(x_0, \\cdot)$ 在我们定义的 $\\mathcal{H}$ 中，那么用基表示成\n$$ \\kappa(x_0, \\cdot) = \\sum_{i=1}^{\\infty} \\sqrt{\\lambda_i}\\psi_i(x_0) \\sqrt{\\lambda_i}\\psi_i $$\n对应 $\\mathcal{H}$ 中的一个无穷维向量\n$$ \\kappa(x_0, \\cdot) = (\\sqrt{\\lambda_1}\\psi_1(x_0), \\sqrt{\\lambda_2}\\psi_2(x_0), \\ldots)_{\\mathcal{H}}^T $$\n类似的，对 $\\kappa(\\cdot, y_0)$ 有 $\\kappa(x, \\cdot) = \\sum_{i=1}^{\\infty} \\sqrt{\\lambda_i}\\psi_i(y_0) \\sqrt{\\lambda_i}\\psi_i$。所以两者内积的计算为\n$$ \u0026lt;\\kappa(x_0, \\cdot), \\kappa(\\cdot, y_0)\u0026gt; = \\sum_{i=1}^{\\infty} \\lambda_i \\psi_i(x_0)\\psi_i(y_0) = \\kappa(x_0, y_0) $$\n这个性质就叫做核函数的再生性 。若令 $f(\\cdot) = \\kappa(x_0, \\cdot)\\in \\mathcal{H}$，则有 $\u0026lt;f(\\cdot), \\kappa(\\cdot, y_0)\u0026gt; = f(y_0)$，原空间的样本点经过核运算后与 $\\mathcal{H}$ 中的元素做内积运算，最后又跑出来了（再生出来）。空间 $\\mathcal{H}$ 叫做再生核希尔伯特空间（Reproducing Kernel Hilbert Space，RKHS）。\n 从上面的介绍可以看出，对于每个正定核函数，都隐式的定义了一个RKHS的特征空间。\n 4 RKHS和SVM 4.1 高维映射 我们已经知道核函数 $\\kappa(x, y)$ 对应的再生核希尔伯特空间 $\\mathcal{H}$ 中的向量内积可以用核函数计算。那么，如何将给定的样本空间（一般是有限维）的任一点 $x$ 映射到高维甚至无穷维的 $\\mathcal{H}$ 中呢？考虑映射 $$ \\phi(x) = \\kappa(x, \\cdot) $$ 显然该映射将点 $x$ 对应到 $\\mathcal{H}$ 中的向量 $$ (\\sqrt{\\lambda_1}\\psi_1(x), \\sqrt{\\lambda_2}\\psi_2(x), \\ldots)_{\\mathcal{H}}^T $$ 按照这种映射得到 $\\mathcal{H}$ 中的两个向量 $\\phi(x)$ 和 $\\phi(y)$ 的内积即为 $\\kappa(x, y)$。\n例如，对于二维样本空间中任意点 $x = (x_1, x_2)$ 和 $y = (y_1, y_2)$，考虑核函数 $$ \\kappa(x, y) = (x_1, x_2, x_1 x_2) \\left(\\begin{matrix} y_1 \\newline y_2 \\newline y_1 y_2 \\end{matrix}\\right) =x_1y_1 + x_2y_2 + x_1 x_2 y_1 y_2 $$ 不妨令 $\\lambda_1 = \\lambda_2 = \\lambda_3 = 1$，$\\psi_1(x) = x_1, \\psi_2(x) = x_2, \\psi_3(x) = x_1x_2$，定义映射 $$ \\left( \\begin{matrix} x_1 \\newline x_2 \\end{matrix} \\right) \\overset{\\phi}{\\rightarrow} \\left( \\begin{matrix} x_1 \\newline x_2 \\newline x_1 x_2 \\end{matrix} \\right) $$ 就把二维空间映射到三维空间了，相应的内积运算满足 $$ \u0026lt;\\phi(x), \\phi(y)\u0026gt; = \\kappa(x, y) $$\n4.2 核技巧 SVM的基本原理是从线性可分数据出发的，如果数据不可分直接用SVM建模显然不合理。为此，我们考虑将原始的原本映射到更高维度的空间中，使得在高维空间中的每个样本是线性可分的。\n【定理】如果原始空间是有限维的，则一定存在一个高维空间使样本线性可分。\n事实上，给定一个样本空间 $\\mathcal{X}$ 后，将其变成线性可分的映射的具体形式一般很难确定。然而，我们可以借助核函数来定义这样的映射。显然，若核函数选的不合适，对应的映射空间不能将原始数据线性可能，那最终的SVM分类效果也不太好。尽管如此，比起确定映射的具体形式，选择核函数显得要容易多了。\n对于原始空间 $\\mathcal{X}$ 的样本点 $x_1, \\ldots, x_n \\in R^p$，给定核函数 $\\kappa(x, y)$，做映射 $$ x_i\\in R^p \\rightarrow \\phi(x_i) = \\kappa(x_i, \\cdot)\\in \\mathscr{F} $$ 那么相应的高维空间的超平面就是 $$ \\phi(x)^T w + b = 0 $$ 根据线性SVM的求解思路，我们最后可以得到高维SVM的对偶模型为 $$ \\begin{split} \u0026amp; \\min \\frac12\\alpha^TD K D\\alpha - 1^T\\alpha\\newline \u0026amp; 0\\leq \\alpha \\leq 1c \\end{split} $$ 其中 $K$ 是核矩阵满足 $K_{ij} = \\kappa(x_i, x_j)$，最终的分类超平面为 $$ f(x)=\\sum_{i=1}^n \\alpha_iy_i\\kappa(x_i,x) + b $$\n5 结语 核函数的引入使得SVM处理非线性可分数据变得切实可行。事实上，我们由Riesz表示定理可知满足一定条件时，原始样本空间经过映射到再生希尔伯特空间后，学习得到的模型总能表示成核函数 $\\kappa(x,x_i)$ 的一个线性组合。\n【表示定理】令 $\\mathbb{H}$ 为核函数 $\\kappa$ 对应的RKHS，$\\Vert h\\Vert_{\\mathbb{H}}$ 表示空间 $\\mathbb{H}$ 中元素 $h$ 的范数。$\\forall$ 单调递增函数 $\\Omega:[0, \\infty]\\rightarrow \\mathbb{R}$ 和 $\\forall$ 非负损失函数 $l:\\mathbb{R}^n \\rightarrow [0, \\infty]$，优化问题 $$ \\min F(h) = \\Omega(\\Vert h\\Vert_{\\mathbb{H}}) + l(h(x_1), \\ldots, h(x_n)) $$ 的解总可以写成 $$ h^*(x) = \\sum_{i=1}^n \\alpha_i \\kappa(x, x_i) $$ 从此可以看出核函数的巨大威力。目前根据函数发展而成的一系列方法称之为“核方法”。\n参考资料\n[1] 数学中的各种空间\n[2] A Story of Basis and Kernel - Part II: Reproducing Kernel Hilbert Space\n[3] 李航. 统计学习方法[M]. Qing hua da xue chu ban she, 2012.\n","id":12,"section":"posts","summary":"在运用SVM处理非线性可分数据时，我们会用到核函数进行转换。与之同时，我们也常常遇到再生核希尔伯特空间（Reproducing Kernel Hilbert Space","tags":["机器学习"],"title":"再生希尔伯特空间与核函数","uri":"https://qkai-stat.github.io/2020/09/rkhs-and-kernel/","year":"2020"},{"content":" 凸优化是优化问题的重要分支之一，也是研究机器学习的重要工具之一。本文和其他几篇以“凸优化”为主题的博客都是我在学习Boyd所著《Convex Optimization》这本书时的笔记。因此，这些博客的内容结构和书中第5、9-11章节的框架相似，我在这里只不过罗列了书中的一些要点和个人的一点思考。\n 在很多优化问题中，如SVM模型，我们常常将原问题的优化转换成对偶问题的优化。对偶问题有时可以简化求解，以及对模型有更多的认识。本文主要介绍有关对偶问题的一些概念。\n1 原始问题 我们考虑一般的优化问题\n$$ \\begin{split} \\min \u0026amp;\\quad f_0(x)\\newline s.t. \u0026amp;\\quad f_i(x) \\leq 0,i = 1,\\ldots,m\\newline \u0026amp;\\quad h_i(x) = 0, i = 1,\\ldots,p \\end{split} $$\n通常我们利用拉格朗日乘子法进行优化，也就是\n$$ \\mathcal{L}(x, {\\mu}, {\\lambda}) = f_0(x) + \\sum_{i=1}^m \\lambda_i f_i(x) + \\sum_{i=1}^p \\mu_i h_i(x) $$\n现在我们来看下面的极大化问题\n$$ \\theta_P(x) = \\max_{{\\mu}, {\\lambda}: \\lambda_i \\geq 0} \\mathcal{L}(x, {\\mu}, {\\lambda}) $$\n 如果上式的 $x$ 满足原始的约束  此时 $f_i(x) \\leq 0, h_i(x) = 0$，由于是最大化，我们只要令所有的 $\\lambda_i = 0$ 即可，换句话说\n$$ \\theta_P(x) = f(x) $$\n 如果上式的 $x$ 不满足原始的约束  此时存在某个 $f_i(x) \u0026gt; 0$ 或者 $h_i(x) \\neq 0$。以某个 $f_i(x) \u0026gt; 0$ 为例：由于是最大化，我们只用令 $\\alpha_i \\rightarrow +\\infty$ 即可，换句话说\n$$ \\theta_P(x) = +\\infty $$\n若我们对 $\\theta_P(x)$ 进行最小化，显然有\n$$ \\min_{x} \\theta_P(x) = \\min_{x} \\max_{{\\mu}, {\\lambda}: \\lambda_i \\geq 0} \\mathcal{L}(x, {\\mu}, {\\lambda}) = \\min_{x} f(x) $$\n即最小化 $\\theta_P(x)$ 和原始优化问题等价，因此我们常用 $\\min_{x}\\theta_P(x)$ 表示原始问题。\n定义原问题的最优值为\n$$ p^* = \\min_{x} \\theta_P(x) $$\n2 对偶问题 定义关于拉格朗日乘子 ${\\mu}, {\\lambda}$ 的函数\n$$ \\theta_D ({\\mu}, {\\lambda}) = \\min_{x}\\mathcal{L}(x, {\\mu}, {\\lambda}) $$\n对其进行最大化\n$$ \\max_{{\\mu}, {\\lambda}: \\lambda_i \\geq 0} \\theta_D ({\\mu}, {\\lambda}) = \\max_{{\\mu}, {\\lambda}: \\lambda_i \\geq 0} \\min_{x} \\mathcal{L}(x, {\\mu}, {\\lambda}) $$\n这个优化问题的形式和原问题 $\\min_{x}\\theta_P(x)$ 的形式很类似，只是最大化和最小化的顺序不一样。我们称上面的优化问题为原始问题的对偶问题（dual problem）。\n定义对偶问题的最优值为\n$$ d^* = \\max_{{\\mu}, {\\lambda}: \\lambda_i \\geq 0} \\theta_D({\\mu}, {\\lambda}) $$\n3 原始问题和对偶问题的关系 【定理】若原始问题和对偶问题都有最优解，则\n$$ d^* = \\max_{{\\mu}, {\\lambda}: \\lambda_i \\geq 0} \\min_{x} \\mathcal{L}(x, {\\mu}, {\\lambda}) \\leq \\min_{x} \\max_{{\\mu}, {\\lambda}: \\lambda_i \\geq 0} \\mathcal{L}(x, {\\mu}, {\\lambda}) = p^* $$\n【证明】对于任意给定的 $x, {\\mu}, {\\lambda}$，根据原始问题和对偶问题的定义显然有\n$$ \\min_{x} \\mathcal{L}(x, {\\mu}, {\\lambda}) \\leq \\mathcal{L}(x, {\\mu}, {\\lambda}) \\leq \\max_{{\\mu}, {\\lambda}: \\lambda_i \\geq 0} \\mathcal{L}(x, {\\mu}, {\\lambda}) $$\n也即是\n$$ \\theta_D({\\mu}, {\\lambda}) \\leq \\theta_P(x) $$\n所以有\n$$ d^* = \\max_{{\\mu}, {\\lambda}: \\lambda_i \\geq 0} \\theta_D({\\mu}, {\\lambda}) \\leq \\min_{x} \\theta_P(x) = p^* $$\n就证明了结论。$\\blacksquare$\n【推论】设 $x ^ *$ 和 ${\\mu} ^ *, {\\lambda} ^ *$ 分别是原始问题和对偶问题的可行解，如果 $d ^ * = p ^ *$，那么 $x ^ *$ 和 ${\\mu} ^ * , {\\lambda} ^ *$ 都是各自问题的最优解。\n从上面的定理可知，当 $d ^ * = p ^ *$ 时，我们才能用对偶问题求解原始问题（当然是在对偶问题更易求的时候）。那么，满足什么条件才能使得 $d ^ * = p ^ *$ 成立呢？我们有下面的定理。\n【定理】（强对偶定理）若原始问题是凸优化问题，即 $f_i(x)$​​ 是凸函数和 $h_i(x)$​​ 是仿射函数；且可行域内部至少存在一个点使得所有不等式约束严格成立且等式约束成立（Slater条件），则有 $d^* = p^*$​​成立。\n【证明】首先，我们将所证明的凸问题重述如下： $$ \\begin{split} \\min \u0026amp;\\quad f_0(x)\\newline s.t. \u0026amp;\\quad f_i(x) \\leq 0,i = 1,\\ldots,m\\newline \u0026amp;\\quad Ax = 0 \\end{split} $$ 其中 $A\\in\\mathbb{R}^{p\\times n}$ 是线性约束的系数矩阵， 即 $h_i(x)=a_i^Tx$，其中 $a_i$ 是 $A$ 的第 $i$ 行。为了简化证明，我们对后续证明做出如下的先决假设：\n 凸问题的定义域 $\\mathcal{D} = \\bigcap_{i=0}^m f_i(x)\\cap \\bigcap_{i=1}^p h_i(x)$ 内点集不为空； $\\textrm{rank}(A)=p$，即系数矩阵行满秩。   关于上述两个条件不满足时的强对偶定理的证明，我还没查阅文献，尚待研究。\n 第一步，我们定义如下两个集合： $$ \\begin{split} \u0026amp;\\mathcal{A}=\\lbrace (u,v,t) | \\exists x\\in\\mathcal{D}, f_i(x)\\leq u_i, i=1,\\ldots,m, h_i(x) = v_i, i=1,\\ldots, p, f_0(x)\\leq t\\rbrace,\\newline \u0026amp;\\mathcal{B}=\\lbrace (0,0,s) \\in \\mathbb{R}^m\\times \\mathbb{R}^p\\times \\mathbb{R} | s \u0026lt; p^ *\\rbrace. \\end{split} $$ 根据凸集的定义很容易证明集合 $\\mathcal{A}$​ 是凸集，此处省略证明过程。\n根据集合 $\\mathcal{A}$​​ 和 $\\mathcal{B}$​​ 的定义，我们可知两者不相交。设点 $(u,v,t)\\in \\mathcal{A}\\cap\\mathcal{B}$​​，由于 $(u,v,t)\\in \\mathcal{B}$​​，我们有 $u=0$​​ 和 $v=0$​​ 以及 $t\u0026lt;p^* $​​ 成立；又 $(u,v,t)\\in \\mathcal{A}$​​，则存在 $x\\in \\mathcal{D}$​​， 使得 $$ \\begin{split} \u0026amp;f_i(x) \\leq 0,i = 1,\\ldots,m\\newline \u0026amp;h_i(x) \\leq 0,i = 1,\\ldots,p\\newline \u0026amp;f_0(x) \\leq t \u0026lt; p^* \\end{split} $$ 成立，这和 $p^ *$ 是凸优化问题的最优解矛盾。因此，集合 $\\mathcal{A}$ 和 $\\mathcal{B}$ 不相交。\n第二步，我们不加证明而给出下面的分离超平面定理：\n【定理】（分离超平面定理）假设 $C$ 和 $D$ 是两个不相交的集合，那么存在 $a\\neq 0$ 和 $b$ 使得对所有的 $x\\in C$ 有 $a^Tx \\leq b$，对所有的 $x\\in D$ 有 $a^T\\geq b$​成立。\n第三步，对集合 $\\mathcal{A}$ 和 $\\mathcal{B}$ 应用分离超平面定理，则存在 $(\\tilde{\\lambda}, \\tilde{\\mu}, \\nu)\\neq 0$ 和 $\\alpha$，使得 $$ \\forall (u, v, t)\\in \\mathcal{A}\\Rightarrow \\tilde{\\lambda}^Tu + \\tilde{\\mu}^Tv + \\nu t \\geq\\alpha \\qquad (1) $$\n和\n$$ \\forall (u, v, t)\\in \\mathcal{B}\\Rightarrow \\tilde{\\lambda}^Tu + \\tilde{\\mu}^Tv + \\nu t \\leq\\alpha \\qquad (2) $$\n这样，我们可以推断出 $\\tilde{\\lambda}\\succeq 0$​​​​ 和 $\\nu \u0026gt;0$​​​​ 成立。因为根据 $\\mathcal{A}$​​​​ 的定义，$u$​​​​ 和 $t$​​​​ 可以取到无穷，那么 $\\tilde{\\lambda}^T u + \\nu t$​​​​ 在 $\\mathcal{A}$​​​​ 上就无下界，这和分离超平面的结果矛盾。根据（2）式，$\\forall t\u0026lt;p^ *$​​​​ 有 $\\nu t\\leq \\alpha$​​​​，意味着 $\\nu p^ *\\leq \\alpha$​​​​（可以用反证法证明）。结合式（1），$\\forall x\\in \\mathcal{D}$​​​​，满足\n$$ \\sum_{i = 1}^m \\tilde{\\lambda}_if_i(x) + \\tilde{\\mu}^T(Ax - b) + \\nu f_0(x) \\geq \\alpha \\geq \\nu p^ * \\qquad (3) $$\n第四步，对式（3）进行分类讨论：如果 $\\nu \u0026gt; 0$​​，那么不等式（3）两侧同除以 $\\nu$​​，再令\n$$ \\lambda = \\tilde{\\lambda}/\\nu, \\mu = \\tilde{\\mu}/\\nu $$\n则可以得到\n$$ \\sum_{i = 1}^m \\lambda_if_i(x) + \\mu^T(Ax - b) + f_0(x) \\geq p^ * $$\n上式对 $x$ 求极小得到 $\\theta_D(\\mu, \\lambda) \\geq p^ *$成立。根据弱对偶结论，我们有 $\\theta_D(\\mu, \\lambda) \\geq p^ * $成立。因此，我们可知\n$$ d^ * = p^ * $$\n成立。这也就证明了强对偶定理的结论。\n如果 $\\nu = 0$，根据式（1），$\\forall x\\in \\mathcal{D}$ 有\n$$ \\sum_{i = 1}^m \\tilde{\\lambda}_if_i(x) + \\tilde{\\mu}^T(Ax - b) \\geq 0 \\qquad (4) $$\n这对满足Slater条件的点 $\\tilde{x}$​ 也满足式（4），即\n$$ \\sum _{i = 1}^m \\tilde{\\lambda}_i f _i(\\tilde{x}) + \\tilde{\\mu}^ T(A\\tilde{x} - b) = \\sum _{i = 1}^m \\tilde{\\lambda}_i f_i(\\tilde{x}) \\geq 0 $$\n由于 $f_i(\\tilde{x})\u0026lt;0, \\tilde{\\lambda}_i\\geq 0, i = 1, \\ldots, m$，则有 $\\tilde{\\lambda}_i = 0, i=1, \\ldots, m$ 成立。根据分离超平面定理，$(\\tilde{\\lambda}, \\tilde{\\mu}, \\nu)\\neq 0$，则可知 $\\tilde{\\mu}\\neq 0$ 成立，即表明对任意的 $x\\in \\mathcal{D}$​ 成立着\n$$ \\tilde{\\mu}^T(Ax - b) \\geq 0 \\qquad (5) $$\n此时再看Slater点 $\\tilde{x}$​​， 有\n$$ \\tilde{\\mu}^T(A\\tilde{x} - b) = 0 $$\n考虑到 $\\tilde{x}$​ 是 $\\mathcal{D}$​ 的内点，则必然存在一个 $\\Delta x$​ 使得 $\\tilde{x}+\\Delta x\\in \\mathcal{D} $​​，且\n$$ \\tilde{\\mu}^T(A(\\tilde{x}+\\Delta x) - b) \\leq 0 $$\n 这个可以办到。因为上式等于 $\\tilde{\\mu}^T A\\Delta x \\leq 0$​​​​，即 $(A^T\\tilde{\\mu})^T\\Delta x \\leq 0$​​​​， $\\Delta x$​​​​ 在 $\\tilde{x}$​​​​ 的邻域中取，总能取一个和 $A^T\\tilde{\\mu}$​​​​ 方向相反的向量，从而内积为非正。\n 注意到，为零的情形只有在 $A^T\\tilde{\\mu} = 0$ 时成立，而矩阵 $A$ 行满秩，则 $A^T\\tilde{\\mu} = 0\\Rightarrow \\tilde{\\mu}=0$，这和 $\\tilde{\\mu} \\neq 0$ 矛盾。若排除为零的情形，由于 $\\tilde{x}+\\Delta x\\in \\mathcal{D}$，负的结果又和式（5）矛盾。总之，$\\nu = 0$ 这种情况不成立。\n综述，我们证明了定理的结论。$\\blacksquare$\n【定理】若原始问题中 $f_i(x)$ 是凸函数和 $h_i(x)$ 是仿射函数，且可行域中至少存在一个点使得某个不等式约束严格成立，则 $x^ *$ 和 ${\\mu}^ *, {\\lambda}^ *$ 分别是原始问题和对偶问题的最优解的充分必要条件是 $x^ *$ 和 ${\\mu}^ *, {\\lambda}^ *$ 满足KKT条件：\n$$ \\begin{split} \u0026amp;\\nabla_x \\mathcal{L}(x^ *, {\\mu}^ *, {\\lambda}^ * ) = 0\\newline \u0026amp;\\nabla_{{\\mu}} \\mathcal{L}(x^ *, {\\mu}^ *, {\\lambda}^ * ) = 0\\newline \u0026amp;\\nabla_{{\\lambda}} \\mathcal{L}(x^ * , {\\mu}^ * , {\\lambda}^ * ) = 0\\newline \u0026amp;\\lambda_i^ * f_i(x^ *) = 0, i = 1, \\ldots, m\\newline \u0026amp;f_i(x^ *) \\leq 0, i = 1, \\ldots, m\\newline \u0026amp;\\lambda_i^ * \\geq 0, i = 1, \\ldots, m \\end{split} $$\n特别指出，$\\lambda_i^* f_i(x^*) = 0, i = 1, \\ldots, m$ 称为KKT的对偶互补松弛条件。\n4 KKT条件在SVM中的应用 我们在SVM模型的求解中涉及到拉格朗日乘子法和对偶模型，考虑硬间距SVM模型\n$$ \\begin{split} \u0026amp;\\min \\frac12 \\Vert w\\Vert_2^2\\newline \u0026amp;y_i(w^Tx_i + b) \\geq 1, i = 1,\\ldots,n\\newline \\end{split} $$\n它的拉格朗日函数就是\n$$ \\mathcal{L}(w, b, \\alpha) = \\frac12 \\Vert w\\Vert_2^2 + \\sum_{i=1}^n \\alpha_i (1 - y_i(w^Tx_i + b)), \\alpha_i \\geq 0 $$\n最终求得的超平面为\n$$ f(x) = \\sum_{i=1}^n \\alpha_i x_i^Tx + b $$\n易知其只和 $\\alpha_i \u0026gt; 0$ 的样本点有关。另一方面，根据互补对偶条件\n$$ \\alpha_i(1 - y_i(w^Tx_i + b)) = 0, \\quad i = 1, \\ldots, n $$\n当 $\\alpha_i \u0026gt; 0$ 时有 $1 - y_i(w^Tx_i + b) = 0$，也即是这些影响平面构建的样本点恰处在临界平面上，即所谓的支持向量。\n","id":13,"section":"posts","summary":"凸优化是优化问题的重要分支之一，也是研究机器学习的重要工具之一。本文和其他几篇以“凸优化”为主题的博客都是我在学习Boyd所著《Convex","tags":["机器学习","优化算法","凸优化"],"title":"对偶模型理论基础","uri":"https://qkai-stat.github.io/2020/09/dual-problem/","year":"2020"},{"content":" 凸优化是优化问题的重要分支之一，也是研究机器学习的重要工具之一。本文和其他几篇以“凸优化”为主题的博客都是我在学习Boyd所著《Convex Optimization》这本书时的笔记。因此，这些博客的内容结构和书中第5、9-11章节的框架相似，我在这里只不过罗列了书中的一些要点和个人的一点思考。\n 0 引言 KKT条件（Karush–Kuhn–Tucker conditions）是非线性优化问题中最优解的一阶微分判定条件。一般情况下，KKT条件是满足最优解的必要非充分条件。在很多问题中，如SVM等，需要求解的模型是一个凸优化的问题，此时KKT条件是充分必要的。因此，KKT条件对认识和求解SVM等问题有重要的帮助。本文主要介绍KKT条件的基本概念，以及在SVM中的应用。\n1 凸优化问题的引入  In fact the great watershed in optimization isn’t between linearity and nonlinearity, but convexity and nonconvexity. ——Rockafellar\n 1.1 凸问题的提法 一般的的优化问题可以写成如下形式：\n$$ \\begin{split} \u0026amp;\\min \u0026amp;\u0026amp; \u0026amp;\\quad f_0(x)\\newline \u0026amp;s.t. \u0026amp;\u0026amp; \u0026amp;\\quad f_i(x) \\leq 0,i = 1,\\ldots,m\\newline \u0026amp; \u0026amp;\u0026amp; \u0026amp;\\quad h_i(x) = 0, i = 1,\\ldots,p \\end{split} $$\n其中 $f_0(x)$ 为目标函数，$f_i(x)$ 为不等式约束，$h_i(x)$ 为等式约束。目前并无有效的算法来求解上述问题。本文主要关注凸优化问题，它和一般的优化问题相比主要有以下三点附加条件：\n 目标函数必须是凸函数， 不等式约束必须是凸函数， 等式约束必须为仿射函数，即形如 $h_i(x) = a_i^Tx - b_i$，其中 $a_i \\in R^n,b_i \\in R$。  凸优化问题在经济学中的投资组合优化问题、统计学中的最小二乘问题等实际问题中广泛出现。针对凸优化问题，现在发展出了很多成熟的求解算法。夸张一点地说，一旦某个模型被确定为凸优化问题，那么实际上该问题也就被求解了。\n1.2 凸问题的基础性质 【定理】凸优化问题中的任意局部最优解即为全局最优解。\n【证明】假设 $x$ 是凸优化问题中的一个局部最优解，也即是存在某个 $R\u0026gt;0$满足\n$$ f_0(x) = \\inf \\lbrace f_0(t) | \\Vert t - x\\Vert_2 \\leq R\\rbrace $$\n假设 $x$ 不是全局最优的，那么必然存在某个可行的 $y$ 满足 $f_0(x) \u0026gt; f_0(y)$， 并且易知 $y$ 满足 $\\Vert y - x\\Vert_2 \u0026gt; R$ 。现考虑下面的点\n$$ z = \\theta x + (1-\\theta)y, \\quad 1 - \\theta = \\frac{R}{2\\Vert y - x\\Vert_2} $$\n根据凸函数的性质显然有 $z$ 是可行解，再更具凸函数的性质易得\n$$ \\begin{split} f_0(z) \u0026amp;= f_0(\\theta x + (1-\\theta)y)\\newline \u0026amp;\\leq \\theta f_0(x) + (1-\\theta)f_0(y)\\newline \u0026amp;\u0026lt; f_0(x) \\end{split} $$\n另一方面，根据 $z$ 的构造可知 $\\Vert z - x\\Vert_2 = R/2 \u0026lt; R$，则有 $f_0(z) \\geq f_0(x)$，矛盾。所以假设不成立， $x$ 是全局最优的。$\\blacksquare$\n这是凸优化问题的基础性质。在一般的优化问题中，我们常常担心搜索最优解时陷入局部最优，这在凸优化问题中不用担心。因为找到了局部最优解即找到了全局最优解。\n2 拉格朗日乘子法与KKT条件 2.1 无约束凸问题 对于可微的凸函数 $f(x)$，考虑不带约束条件的凸问题（$m = p = 0$ 的情形）\n$$ \\min f_0(x) $$\n满足一阶微分条件\n$$ \\nabla f_0=0 $$\n的点为函数的极值点。而根据凸优化的基础性质，极值点就是我们要求的最优解。\n2.2 等式约束的凸问题 无约束的凸问题比较简单，现在我们来研究带有等式约束的凸问题。先看一个例子，考虑如下的二元凸优化\n$$ \\begin{split} \u0026amp;\\min f_0(x) = x_1^2 + x_2^2\\newline \u0026amp;s.t. \\quad h_1(x) = x_1 + x_2 - 1 = 0 \\end{split} $$\n下面是该问题的一个示意图\n显然点 $(1/2, 1/2)$ 是问题的最优解。分析目标函数和约束函数在该点的性质：目标函数的梯度\n$$ \\nabla f_0(x) = (2x_1, 2x_2) $$\n和约束条件的梯度\n$$ \\nabla h_1(x) = (1, 1) $$\n恰好共线。注意，由于是等式约束，我们把约束换成 $-h_1(x)$ 不改变问题，但是等式约束的梯度就变号了。因此，最优解应当满足的是下面这个梯度共线的条件\n$$ \\nabla f_0(x^* ) = \\mu\\nabla h_1(x^ * ), \\mu \\in R $$\n当然，仅仅共线是不够的，还要满足我们的等式约束才行（从图中看到个圆都有两个共线的点，而最优解是和等式相切的那个点）。\n为了同时满足上面的要求，我们构造一个函数如下\n$$ \\mathcal{L}(x, \\mu) = f_1(x) + \\mu h_1(x) $$\n原问题是凸的，不难发现 $\\mathcal{L}$ 也是凸的。对该函数最小化，即无约束凸问题的求解，最优解满足\n$$ \\begin{split} \\nabla_x \\mathcal{L} \u0026amp;= \\nabla_x f_1(x) + \\mu \\nabla_x h_1(x) = 0\\newline \\nabla_{\\mu} \\mathcal{L} \u0026amp;= h_1(x) = 0 \\end{split} $$\n第一式得到是 $-\\mu$ 显然不影响共线的关系刻画。因此，原问题的优化等价于 $\\mathcal{L}$ 的优化，也就是我们熟知的拉格朗日乘子法，其中 $\\mu$ 是拉格朗日乘子。\n 特别注意，因为我们研究的是凸问题，所以上述两个条件得到的解就是全局最优。如果问题不是凸的，那得到的只是局部最优，还需要附加多一个正定的条件才能变成充要条件。（这里就不延申讨论了）\n 2.3 不等式约束的凸问题 不等式约束的问题同等式约束略有不同，因为不等式约束如 $f_1(x) \\leq 0$ 表示一块区域，称为可行域。在考虑最优解时，我们从两个情况来研究：（1）极小值（不含不等式约束时）落在可行域中（不含边界）和（2）极小值（不含不等式约束时）落在可行域之外（含边界）。\n2.3.1 极小值点落在可行域之中 我们来看下面的例子\n$$ \\begin{split} \u0026amp;\\min f_0(x) = x_1^2 + x_2^2\\newline \u0026amp;s.t. \\quad f_1(x) = x_1^2 + x_2^2 - 1 \\leq 0 \\end{split} $$\n下面是该问题的一个示意图\n问题的最优解为 $x^* = (0,0)$ 落在可行域之中（不含边界），此时不等式约束失效（相当于没有加），我们直接通过目标函数的极值点得到最优解\n$$ \\nabla f_0(x^*) = 0 $$\n2.3.2 极小值点落在可行域之外 我们来看下面的例子\n$$ \\begin{split} \u0026amp;\\min f_0(x) = (x_1 - 1)^2 + (x_2 + 1)^2\\newline \u0026amp;s.t. \\quad f_1(x) = x_1^2 + x_2^2 - 1 \\leq 0 \\end{split} $$\n下面是该问题的一个示意图\n此时约束条件起作用。对于目标函数，我们知道其沿着自己的负梯度方向 $-\\nabla f_0(x)$ 下降，在极小值处恰好落在可行域的边界上，此时等式约束 $f_1(x) = 0$ 起作用，那么由前面的论述立即知道极小值点满足\n$$ \\begin{split} \\nabla f_0(x) \u0026amp;= \\alpha \\nabla f_1(x)\\newline f_1(x) \u0026amp;= 0 \\end{split} $$\n但是，我们进一步发现这里的 $\\alpha$ 必须非正，换句话说目标函数的负梯度要和不等式约束的梯度同向。这也很容易理解：可行域是沿着自己的梯度方向 $\\nabla f_1(X)$ 向外扩张的，若目标函数的负梯度 $-\\nabla f_0(x)$ 和 $\\nabla f_1(X)$ 反向，那目标函数沿着自己的负梯度方向可以继续下降，这和极小值点矛盾。\n因此，我们对不等式约束的凸问题的极小值点要求\n$$ \\begin{split} -\\nabla f_0(x) \u0026amp;= \\lambda \\nabla f_1(x), \\lambda \\geq 0\\newline f_1(x) \u0026amp;= 0 \\end{split} $$\n2.3.3 KKT条件 我们仍然可以借助拉格朗日乘子法来解决不等式约束的凸问题\n$$ \\mathcal{L}(x, \\lambda) = f_0(x) + \\lambda f_1(x), \\quad \\lambda \\geq 0 $$\n为了囊括不等式约束的两种情形，我们要求在极小值点处有\n $\\nabla_x \\mathcal{L}(x ^ * , \\lambda ^ * ) = 0$ $\\nabla _{\\lambda} \\mathcal{L}(x ^ *, \\lambda ^ * ) = 0$ $\\lambda ^ * \\nabla _{\\lambda} \\mathcal{L}(x ^ *, \\lambda ^ * ) = 0$  上面三条就是著名的KKT条件。KKT条件导出的是极值点（驻点），因为我们只关注凸问题，极小值就是全局最小值，也就解决了我们的问题。\n2.4 多约束的KKT条件 前面讨论是单个等式约束或不等式约束的凸问题，若是文章最开始给出的一般性凸问题，我们构造如下的拉格朗日函数\n$$ \\mathcal{L}(x, {\\mu}, {\\lambda}) = f_0(x) + {\\mu}^T H(x) + {\\lambda}^T F(x) $$\n其中 $H(x) = (h_1(x), \\ldots, h_p(x))^T, F(x) = (f_1(x), \\ldots, f_m(x))^T$，${\\mu}\\in R, {\\lambda}\\in R ^ +$。若 $x ^ * $ 是极小值点，则存在唯一的 ${\\lambda} ^ * $ 满足\n $\\nabla_x \\mathcal{L} (x^ * , {\\mu}^ * , {\\lambda}^ * ) = 0$ ${\\lambda}^* \\in R^+$ $\\lambda_i^* f_i(x^*) = 0, i = 1, \\ldots, p$ $f_i(x^*) \\leq 0, i = 1,\\ldots, p$ $H(x^*) = {0}$  以上就是一般凸问题的KKT条件。如果问题非凸，那么得到就是局部优。\n3 KKT条件在SVM中的应用 考虑硬间距SVM模型\n$$ \\begin{split} \u0026amp;\\min \\frac12 \\Vert w\\Vert_2^2\\newline \u0026amp;y_i(w^Tx_i + b) \\geq 1, i = 1,\\ldots,n\\newline \\end{split} $$\n它的拉格朗日函数就是 $$ \\mathcal{L}(w, b, \\alpha) = \\frac12 \\Vert w\\Vert_2^2 + \\sum_{i=1}^n \\alpha_i (1 - y_i(w^Tx_i + b)), \\alpha_i \\geq 0 $$\n最终求得的超平面（求解方法参看这篇博客）为\n$$ f(x) = \\sum_{i=1}^n \\alpha_i x_i^Tx + b $$\n易知其只和 $\\alpha_i \u0026gt; 0$ 的样本点有关。另一方面，最优解必然满足KKT条件，其中之一为\n$$ \\alpha_i(1 - y_i(w^Tx_i + b)) = 0, \\quad i = 1, \\ldots, n $$\n当 $\\alpha_i \u0026gt; 0$ 时有 $1 - y_i(w^Tx_i + b) = 0$，也即是这些影响平面构建的样本点恰处在临界平面上，即所谓的支持向量。\n4 结语 本文主要总结了凸优化问题中最优解满足的KKT条件。对于凸问题，KKT条件是充要条件，而对一般的问题则只能导出局部最优解。此外，我们谈论的凸问题中所有的函数都是可微的，而像LASSO类的问题存在不可导的项，这就要用到次微分的概念，同时需要对KKT条件进行修正。\n","id":14,"section":"posts","summary":"凸优化是优化问题的重要分支之一，也是研究机器学习的重要工具之一。本文和其他几篇以“凸优化”为主题的博客都是我在学习Boyd所著《Convex","tags":["机器学习","优化算法","凸优化"],"title":"拉格朗日乘子法和KKT条件","uri":"https://qkai-stat.github.io/2020/09/lagrangian-and-kkt/","year":"2020"},{"content":"支持向量机（support vector machine，SVM）是经典的机器学习模型之一，最早由Cortes和Vapnik二人于1995年为解决二分类问题而提出。SVM有坚实的统计理论基础，且决策函数具有很强的几何含义。因其在模式识别等数据分析问题中的优越表现，SVM如今已发展成为一门成熟的机器学习理论。本期我们来一起学习SVM的基本概念及在R中的使用方法。\n1 线性支持向量机 1.1 硬间距支持向量机 我们先来看一种简单的情形。假设我们得到的分类数据是线性可分的，也就是我们可以构造数据变量的一个不含交互项的线性函数作为分类的决策函数便可以实现分类的目的。例如下图（a）所示，对得到的二元变量数据（黑点+1和白点-1所示），我们直接用一条直线（三维空间是平面，三维以上是超平面）便可以实现分类的目的。这种情形便是前述的线性可分数据。\n显然，对于上述的线性可分数据，我们只要把直线稍微倾斜一点便可以得到一个新的分类直线。那么，对于线性可分数据，如何寻找一条最优的分类超平面呢？\n仔细观察图（a），对于不同的分类直线$l$，我们是始终可以将其向两类数据进行平移直至恰好接触数据点（直线$l_+$和直线$l_-$，如图（b）所示），显然分类直线$l$处在临界直线中间是合理的。我们自然希望得到的分类直线能够尽可能的将两类数据分开。换句话说，边界之间的$l_+$和直线$l_-$距离应当尽可能的大——当然，这是一种直觉上的判断，也是SVM理论的前提。\n 如果你不认同这种直觉上的判断，那么…你可以静静的想一想看~\n 上述直观的认识便是硬间距（hard-margin）SVM的理论基础。现在我们用一些数学符号进行更为细致的描述。令$(x_i,y_i)$，$i=1,\\ldots,n$是第$i$个观测数据，其中$x_i$是一个$p$维的向量，$y_i\\in{+1,-1}$是分类标签。\n假设我们的分类超平面为 $$ w^Tx+b=0 $$ 其中$w\\in R^p$是超平面的法向量，$b$是超平面的截距项。按照前面直观的认识，我们的分类超平面需要最大化临界平面之间的距离，也即是 $$ \\max d_{l_{+}l_-} $$ 该模型没有体现分类超平面对数据分类，故而我们需要加上以下约束条件： $$ \\begin{split} w^Tx_i + b \u0026amp;\\geq +\\eta, y_i = +1;\\newline w^Tx_i + b \u0026amp;\\leq -\\eta, y_i = -1. \\end{split} $$ 这里$\\eta\u0026gt;0$为临界平面相对分类超平面移动值。注意到，如果令$w/\\eta \\rightarrow w$和$b/\\eta \\rightarrow b$，那么上述的模型就可以等价的写成 $$ \\begin{split} \u0026amp;\\max d_{l_{+}l_-}\\newline \u0026amp;y_i(w^Tx_i + b) \\geq 1, i = 1,\\ldots,n\\newline \\end{split} $$ 现在，我们来看看临界距离如何表示。根据简单的几何知识，空间中任意一点$(x_i,y_i)$到给定平面的距离表示 $$ d=\\frac{\\vert w^Tx_i+b\\vert}{\\Vert w\\Vert} $$ 经过$w/\\eta \\rightarrow w$和$b/\\eta \\rightarrow b$的变换之后，临界超平面上的点带入到超平面方程中等于1。那么临界平面之间的距离便为 $$ d_{l_{+}l_-}=\\frac{2}{\\Vert w\\Vert} $$\n因此，最终的模型可以等价得写成 $$ \\begin{split} \u0026amp;\\min \\frac12 \\Vert w\\Vert_2^2\\newline \u0026amp;y_i(w^Tx_i + b) \\geq 1, i = 1,\\ldots,n\\newline \\end{split} $$\n 很容易看出最小化$\\Vert w\\Vert$等价于最小化$\\Vert w\\Vert_2^2$.\n 以上便是硬间距SVM模型，也是最简单、最理想化的SVM分类模型。\n1.2 软间距支持向量机 虽然硬间距SVM思想朴素，但是足以阐释SVM核心思想——最小化经验风险的同时最小化结构风险。尽管如此，硬间距SVM要求构建的分类超平面完全正确的分类所有训练数据，这显得非常严格。这样要求100%正确训练的模型容易产生过拟合，从而降低模型的泛化能力。\n为了提升硬间距SVM的泛化能力以及扩大其应用场景，我们在训练SVM的过程中，允许数据被错误分类，这就得到了软间距（soft-margin）SVM，如图（c）所示。\n从硬间距SVM的模型看出，不等式约束条件代表着分类超平面对数据的分类约束。我们允许数据被错误分类，也即是$y_i(w^Tx_i + b)$大于一个小于1的数即可。换句话说，存在$\\xi_i\\geq0$，满足 $$ y_i(w^Tx_i + b)\\geq 1-\\xi_i $$ 另一方面，违反约束的数据太多，我们得到的模型也不好。因此，我们在提升SVM的泛化能力同时，又希望违反约束的训练数据不能太多。\n综上所述，软间距SVM模型就写成 $$ \\begin{split} \u0026amp;\\min \\frac12 \\Vert w\\Vert_2^2 + c\\sum_{i=1}^n \\xi_i\\newline \u0026amp;y_i(w^Tx_i + b)\\geq 1-\\xi_i\\newline \u0026amp;\\xi_i\\geq0,i = 1,\\ldots,n\\newline \\end{split} $$ 其中$c\\geq0$是调节参数，需事先给定，表示对违反约束的惩罚程度，数值越大表示模型期望的违反约束数据越少。实际使用时，$c$要通过交叉验证来确定最优值。\n软间距SVM描述的场景更加丰富，因此在实际问题中，我们也只采用该种形式的SVM模型。\n2 模型的求解 SVM的求解算法一般通过优化其对偶模型得到最优解。为了方便描述，我们将软间距SVM模型写成矩阵形式 $$ \\begin{split} \u0026amp;\\min \\frac12 \\Vert w\\Vert_2^2 + ce^T\\xi\\newline \u0026amp;D(Xw + 1b)\\geq e-\\xi\\newline \\end{split} $$ 其中$X=(x_1^T,\\ldots,x_n^T)^T\\in R^{n\\times p}$是数据矩阵；$D$是对角元素为$y_1,\\ldots,y_n$的对角矩阵；$e$是一个长度为$n$、所有元素为1的向量；$\\xi=(\\xi_1,\\ldots,\\xi_n)$是一个长度为$n$的向量。\n首先，我们可以得到相应的Lagrangian方程 $$ L=\\frac12 \\Vert w\\Vert_2^2 + ce^T\\xi-\\alpha^T(D(Xw + eb)- e+\\xi) $$ 其中$\\alpha=(\\alpha_1,\\ldots,\\alpha_n)\\geq0$是Lagrangian乘子向量。\n接着，我们分别对$w$，$b$和$\\xi$求偏导数并令为零得到 $$ \\begin{split} \u0026amp;\\frac{\\partial L}{\\partial w}=w-X^TD\\alpha=0\\newline \u0026amp;\\frac{\\partial L}{\\partial b}=-\\alpha^TDe=0\\newline \u0026amp;\\frac{\\partial L}{\\partial \\xi}=ec-\\alpha-\\xi=0 \\end{split} $$ 将上式结果带回到Lagrangian方程中，可以得到 $$ L=e^T\\alpha - \\frac12\\alpha^TDXX^TD\\alpha $$ 结合约束条件可以得到SVM的对偶模型为 $$ \\begin{split} \u0026amp; \\min \\frac12\\alpha^TDXX^TD\\alpha - e^T\\alpha\\newline \u0026amp; 0\\leq \\alpha \\leq ec \\end{split} $$ 这是一个二次优化问题（quadratic programming problem，QPP），有很多成熟的算法可以求解。当然，很多学者针对SVM设计了一些计算快速的算法，如SMO、LIBSVM等。我们可以在R中利用程序包轻易的求解SVM模型，因此不再详述具体的算法。\n求出$\\alpha$之后，我们根据Lagrangian的偏导数结果可得最终的决策函数为 $$ f(x)=w^Tx+b=\\sum_{i=1}^n \\alpha_iy_ix_i^Tx + b $$ 可见，最终的决策函数只和$\\alpha_i\u0026gt;0$对应的训练数据$(x_i,y_i)$有关。\n 那些$\\alpha_i\u0026gt;0$对应的训练数据$(x_i,y_i)$“支持”着最终的分类超平面的构建，因此被称为支持向量。这便是支持向量机名称的由来。\n 另一方面，优化SVM的过程需要满足KKT条件，其中之一为 $$ \\alpha_i(y_i(w^Tx_i+b)-1+\\xi_i)=0 $$ 可见，当$\\alpha_i\u0026gt;0$时，必有$(y_i(w^Tx_i+b)-1+\\xi_i)=0$，此时不难看出数据点对应临近平面上的点或者错分类的点。由此看来，SVM模型的构建正是依赖于这些训练数据点。\n 如果是硬间距SVM，$\\alpha_i\u0026gt;0$对应的便只是临界平面上的点——这更加可以看出”支持“二字的含义。\n 3 非线性支持向量机 软间距SVM有一个很大的缺陷，即依赖于数据是线性可分的假设。实际上，很多问题中的数据是非线性可分的，如图（d）所示。\n这种情形应当如何处理呢？\nCortes和Vapnik采用了一种非常巧妙的方式。他们借助函数映射，将低维数据映射到高维空间中，使得原来不能线性可分的数据变得线性可分（这样的映射可以证明是始终存在的），如图（e）所示。\n假设这种映射为 $$ x\\in R^p\\rightarrow \\phi(x)\\in \\mathscr{F} $$ 那么重复前面的优化过程不难得到需要优化的对偶模型为 $$ \\begin{split} \u0026amp; \\min \\frac12\\alpha^TD\\phi(X)\\phi(X)^TD\\alpha - 1^T\\alpha\\newline \u0026amp; 0\\leq \\alpha \\leq 1c \\end{split} $$ 一般来说，求解出具体的映射函数形式非常困难（或者根本就无法求出），我们如何解决该问题呢？Cortes和Vapnik注意到，SVM模型优化的过程中始终是保内积运算的。因此，他们利用空间映射和核函数的对应关系，将高维空间的内积运算$\\phi(X)\\phi(X)^T$转换成核矩阵$K(X,X^T)$的计算，从而实现非线性SVM的优化！\n 核函数是一种特殊的函数，它与某个映射$\\phi$的内积运算对应。映射函数难以给出，但可以根据核函数的特征来给定某种形式的核函数。由于我们不知道具体的映射，因此给定核函数是一种可行且简便的尝试。\n 常见的核函数有：\n 线性核：$K(x_i,x_j)=x_i^Tx_j$； 高斯核：$K(x_i,x_j)=\\exp(-\\frac{\\Vert x_i-x_j\\Vert^2}{\\sigma})$，$\\sigma\u0026gt;0$； Sigmoid核：$K(x_i,x_j)=\\tanh(\\beta x_i^Tx_j + \\theta)$，$\\beta\u0026gt;0,\\theta\u0026lt;0$；  前面提到的核矩阵$K(X,X^T)$是一个$n\\times n$的矩阵，其中第$(i,j)$元素为$\\kappa(x_i,x_j)$。\n最后，我们得到非线性SVM的分类超平面为 $$ f(x)=\\tilde{w}^T\\phi(x)+b=\\sum_{i=1}^n \\alpha_iy_i\\kappa(x_i,x) + b $$ 我们仍然可以看到，最终的超平面的构建只和$\\alpha_i\u0026gt;0$对应的训练样本点有关。\n4 R求解支持向量机 目前在R中比较常用的SVM求解程序包是e1071包和kernlab包，这里用e1071包给一个SVM问题的算例。我们首先安装并载入该程序包：\ninstall.packages(\u0026quot;e1071\u0026quot;) library(e1071)  我们用R中的鸢尾花数据集建立SVM模型，该数据中包含三种鸢尾花（分别标记为setosa、versicolor和virginica）的花萼和花瓣数据，最早由1936年Fisher发表的一篇重要论文给出。\n我们以花瓣的长度核宽度为变量做出数据的图像，可见setosa和另外两种花很好的区分开来。我们下面选择versicolor和virginica进行SVM建模。\ne1071中求解SVM的函数为ksvm，它的基本用法如下\nsvm(formula, data= NULL, subset, na.action = na.omit , scale= TRUE) # formula为拟合的公式 # data为数据集  参数na.action用于指定当样本数据中存在无效的空数据时系统应该进行的处理。默认值na.omit表明程序会忽略那些数据缺失的样本。另外一个可选的赋值是na.fail，它指示系统在遇到空数据时给出一条错误信息。参数scale为一个逻辑向量，指定特征数据是否需要标准化（默认标准化为均值0，方差1）。索引向量subset用于指定那些将被来训练模型的采样数据。\n我们首先拟合一个基本的模型\npartdata \u0026lt;- subset(iris, Species %in% c(\u0026quot;versicolor\u0026quot;,\u0026quot;virginica\u0026quot;)) fit \u0026lt;- svm(Species~Petal.Length + Petal.Width, data = partdata)  查看结果为\n\u0026gt; summary(fit) Call: svm(formula = Species ~ Petal.Length + Petal.Width, data = partdata) Parameters: SVM-Type: C-classification SVM-Kernel: radial cost: 1 Number of Support Vectors: 23 ( 12 11 ) Number of Classes: 2 Levels: versicolor virginica  通过summary函数可以得到关于模型的相关信息。其中，SVM-Type项目说明本模型的类别为C-SVM（SVM发展至今已有很多类型的SVM，上述介绍的为最基本的C-SVM）；SVM-Kernel项说明本模型所使用的核函数为高斯内积函数且核函数；cost项目说明本模型确定的约束违反成本为1。而且我们还可以看到，模型找到了23个支持向量：第一类包含有12个支持向量，第二类包含有11个支持向量。\n当然，我们还可以利用程序包得到分类的示意图，可见模型还是训练的不错的。\n更加详细的使用方式，有兴趣的读者可以自行阅读相关文档进行实践。\n5 结语 本文我们主要介绍了经典支持向量机的基本概念核求解算法，并用一个简单的案例介绍R求解SVM的方式。支持向量机发展至今已有很多类型，但基本的思想是共同的。感兴趣的读者可以进一步阅读相关文献材料。\n参考文献\n[1] Cortes C, Vapnik V. Support-vector networks[J]. Machine learning, 1995, 20(3): 273-297.\n[2] Suykens J A K, Vandewalle J. Least squares support vector machine classifiers[J]. Neural processing letters, 1999, 9(3): 293-300.\n[3] Schölkopf B, Smola A J, Williamson R C, et al. New support vector algorithms[J]. Neural computation, 2000, 12(5): 1207-1245.\n[4] Mangasarian O L, Musicant D R. Lagrangian support vector machines[J]. Journal of Machine Learning Research, 2001, 1(Mar): 161-177.\n","id":15,"section":"posts","summary":"支持向量机（support vector machine，SVM）是经典的机器学习模型之一，最早由Cortes和Vapnik二人于1995年为解决二分类问","tags":["机器学习"],"title":"支持向量机原理","uri":"https://qkai-stat.github.io/2020/09/svm/","year":"2020"},{"content":"腾讯在2018年就宣布以后申请的公众号不再支持留言评论功能。确实，无法和读者进行直接的交流沟通会严重打击分享者的热情。所以我的好友H君经常唠叨，依托平台大都会受限制而自己往往又无能为力，要尽量去平台化啊！\n在之前的博客中，我曾分享过利用Hugo+Gitee快速构建个人博客实现去平台化的经验，但并没有涉及博客评论如何设置。这篇博客就介绍一款基于LeanCloud的快速、简洁且高效的无后端评论系统——Valine。\n 话说我又不是程序猿，看来以后要抵制程序猿H君的洗脑了。\n 1 准备工作 实现个人博客评论功能，我们首先自然需要先搭建好个人博客网站，推荐Hugo+Gitee/Github（因为我只会用Hugo搭建，哈哈哈），具体方法可以看网上教程或者我前面的经验分享文章。一般来说，Hugo上的博客主题都整合了Valine评论，我们只需要进行相应的配置即可。\n如上图，这是我的博客配置文件config.yml中关于评论系统的配置区域。\n2 开始配置 博客上面的评论留言数据自然要存放于某处，一般是放在服务器上，但并不是人人都有私人服务器。LeanCloud是一个简单易用（主要提供免费套餐）的云服务器，有强大的数据库支持。Valine的配置也即是将个人博客与之对接。\n首先，我们要去LeanCloud官网申请一个账号。为了后续的配置可以顺利进行，请申请国际版的账号。\n然后，我们登录国际版的LeanCloud，选择创建应用并输入一个你喜欢的名称，例如\n接着，点击进入到我们所创建的应用中，依次选择设置 \u0026gt; 应用Keys，便可以查看当前应用的账号（AppID）和密钥（AppKey）。\n最后，我们只需将上述的AppID和AppKey复制粘贴到博客配置文件config.yml中Vlaine配置区域的对应位置即可。\nValine的基本配置就结束了。可以看出，使用Valine评论非常简单！其他的参数通过阅读后面的英文介绍即可理解其意义，我将我的设置列举如下\n   参数 值     notify true   verify true   placeholder 欢迎留言【填写邮箱后才能接收回复通知】   avatar mm   meta nick, mail   pageSize 10   visitor false    对一些参数设置做一点解释。由于可能需要和留言者交流，所以在评论框放了填写邮箱的提示，然后meta参数设置成可以允许留言者留下自己的昵称（nick）和用来接收评论反馈消息的邮箱（mail）。因为我的博客模板集成了其他的访问量统计方式，因此visitor参数设置false，即不需要通过LeanCLoud进行访问量统计。\n配置好了之后重新发布一下博客，即可在自己的每篇文章下看到评论框\n可见，Valine评论支持Markdown输入和表情输入，功能还是非常友好和强大的！留言者只需输入邮箱（对他人不可见），今后自己的评论有他人回复即会收到邮件通知和评论链接，点击链接便可直接跳转到原先的留言位置进行回复。\n3 评论邮件通知 文章有了新的评论，作者应该要能及时收到通知并快速进入到评论界面进行查看和回复；留言者的评论有了回复，正常情况应该能及时收到通知并快速进入原始评论的位置进行回复。Valine是利用邮箱通知实现该功能的。然而，你直接在config.yml将通知的参数notify设置成true，发现在博客下面评论后并不能收到邮箱通知！！！\n这么尴尬的事情自然还是有解决方案的。这里采用的是Valine-Admin的解决方案。\n首先，访问你的LeanCloud主页，进入到与博客绑定的应用中。\n然后，依次选择设置 \u0026gt; 邮箱模板，将用于重置密码的邮件主题和对应的内容分别改成下面的内容\n## 用于重置密码的邮件主题 你在 {{appname}} 的评论收到了新的回复 ## 内容 \u0026lt;p\u0026gt;Hi, {{username}}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; 你在 {{appname}} 的评论收到了新的回复，请点击查看： \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;a href=\u0026quot;你的博客主页链接\u0026quot; style=\u0026quot;display: inline-block; padding: 10px 20px; border-radius: 4px; background-color: #3090e4; color: #fff; text-decoration: none;\u0026quot;\u0026gt;马上查看\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;  该设置即为在有评论时将评论的内容和地址发送到对应的邮箱中。\n接着，我们需要在LeanCloud上部署一个项目，以实现实时邮箱通知的功能。依次选择云引擎 \u0026gt; 部署，点击项目部署并采用Git部署，配置Git到 https://github.com/zhaojun1998/Valine-Admin并保存\n之后，回到部署页面点击部署\n看到部署成功后，关闭部署日志窗口\n最后，我们要切换到云引擎 \u0026gt; 设置中进行必要的变量设置（选择添加变量，填上变量名和值即可），主要包括\n   变量名 含义     SENDER_EMAIL 评论通知推送采用的邮箱   SMTP_PASS 邮箱密码（163邮箱要授权码）   SENDER_NAME 发件人的昵称   SITE_PASS 博客网址   SITE_URL 博客主页链接（末尾不带/）   SMTP_SERVICE 邮箱服务商，支持QQ、163、126、gmail等   SMTP_USER 和邮箱保持一致即可    上述的工作结束后，回到自己的博客进行评论，然后你的邮箱就可以收到评论的通知了，界面如下\n评论者没有填写昵称，默认便是Anonymous显示。\n4 休眠策略和定时任务 免费版的LeanCloud是有强制休眠的限制的，无法全天候运行：\n 每天必须休眠6小时； 30分钟内没有外部访问便休眠； 休眠后如果有新的外部访问请求则会马上启动，但是该请求的邮件会发送失败。  借用网上的已有经验，我们可以采用的应对措施为：如果不想付费的话，最佳使用方案就设置定时器，每天 7 - 23 点每25分钟访问一次，这样可以保持每天的绝大多数时间邮件服务是正常的。\n具体的休眠策略和定时任务设置，依次在云引擎 \u0026gt; 定时任务中设置\n然而，单纯的如此设置依然不能达到我们预想的目的，即不能正常唤醒。网上主流的解决方案有些已经失效了，唤醒过程中会报如下的错误\n 因流控原因，通过定时任务唤醒体验版实例失败，建议升级至标准版云引擎实例避免休眠\n 下面介绍一种基于Github TOKEN的解决方式，该方案已经被我验证是成功的了。\n首先，我们在云引擎 \u0026gt; 设置中设置自己的云引擎域名\n我们在自定义环境变量中添加该域名，如下所示\n这个域名的好处就是，可以直接访问并管理自己博客的所有评论，不需要再进入到LeanCloud中进行查看。\n然后，我们利用该域名进行休眠策略和定时任务的设置。进入到自己的Github主页，选择setting \u0026gt; Developer settings \u0026gt; Personal access tokens，建立一个新的token，要求如下\n 名称必须为GITHUB_TOKEN； 勾选 repo、admin:repo_hook、workflow 等选项  接着，我们从这里FORK一个项目。FORK成功后，进入到项目的settings选择Secrets，添加上述设定的域名\n名称必须为SITE，值为自己的域名。\n最后，给自己的项目点个 star 就能启动了，启动后请切换到 actions，看看是否运行成功。如果想获取该方法的更多细节，请参考下面的信息\n 若想得到更详细的解决方法，请前往该方案原文章：https://www.antmoe.com/posts/ff6aef7b/\n请多支持作者：Dreamy.TZK\n本解决方案转载自：小康博客：https://www.antmoe.com/\n 5 结语 如果大家感兴趣的话可以试试Valine评论。\n","id":16,"section":"posts","summary":"腾讯在2018年就宣布以后申请的公众号不再支持留言评论功能。确实，无法和读者进行直接的交流沟通会严重打击分享者的热情。所以我的好友H君经常唠","tags":["git"],"title":"Valine: 高效的无后端评论系统","uri":"https://qkai-stat.github.io/2020/09/valine-introduction/","year":"2020"},{"content":"人工智能（Artificial Intelligence，AI）是计算机科学（Computer Science，CS）领域的分支之一，通常包含机器人、语言识别、图像识别、自然语言处理和专家系统等方向。在CS领域，科研人员通常更关注与之相关的各个国际顶级会议的研究成果——因为CS研究更新速度快，而会议效率更高、更容易宣传、获取和交流最新的成果。相对而言，其他的领域则偏爱将成果刊登在期刊上。\n1 CCF推荐列表 关于CS领域的期刊与会议，中国计算机协会（CCF）官方有一个推荐国际学术会议和期刊目录，也即是大家常说的CCF列表，目前最新的是2019年4月25日发布的第五版。CCF将每个方向的刊物划分成A类、B类和C类三个等级，期刊和会议分开划分。同一类的刊物不分先后顺序，被认为是同一层次的，与影响因子（IF）“锱铢”必较的排序截然不同。\n一般而言，A类上上，属于该方向最顶级的刊物；B类中上，属于该方向卓越但还达不到巅峰层次的刊物；C类上，属于该方向的优秀刊物。B类刊物作为A类和C类的分水岭，是很多稍有理想的科研小白冲击的主要目标——因为A类刊物数目太少（AI方向A类期刊4个，A类会议7个），难度极大。在B类刊物上发表一篇论文，有的985和211高校都会通报表扬了。\n现在，CCF列表已经成为很多高校CS专业研究生毕业的重要参考标准，与其他专业将中科院/JCR分区作为毕业标准相比，这或许是CS专业的又一个特色。\n 必须指出的是，本《目录》是认为值得计算机界研究者们发表研究成果的一个推荐列表，其目的不是作为学术评价的（唯一）依据，而仅作为CCF的推荐建议供业界参考。——CCF官方\n 综上所述，CCF列表更多的考量各个方向的同行认可度，而这方面IF则体现的比较模糊。因此，你常常看见分区不高的期刊在CCF列表上属于B类甚至A类，而C类或者B类的期刊在分区上是二区、一区甚至被中科院评为TOP期刊。\n 当然，一千个人眼里有一千个哈姆雷特，也有很多人质疑这份列表的合理性。不过总体而言（也从小Q看的文献感觉），CCF推荐列表还是较为合理的。\n 接下来，我们认识一些有代表性的刊物。\n2 期刊类 人工智能方向的期刊很多，侧重点不尽相同，因此这里只关注了机器学习（ML）方向的期刊。从ML的角度来看，最顶级的期刊莫过于下面两个\n Journal of Machine Learning Research （JMLR） IEEE Transactions on Pattern Analysis and Machine Intelligence （PAMI）  两者皆是CCF A类期刊，但JMLR在中科院分区中为二区，PAMI则为中科院分区一区TOP期刊。不过，在真正ML研究人心中（小Q心中），JMLR的地位还是更甚一筹。因为PAMI毕竟是IEEE旗下的，更关注模型的创新度和应用效果；JMLR则是你的模型厉害，但要用理论证明它到底哪里厉害？有多厉害？换句话说，JMLR的文章更多的涉及到了ML的本质。\n顺便说一下，鉴于ML和统计学的密切关系，统计学的顶级期刊也不乏ML的文章。这方面，统计四大天王中的\n Annals of Statistics Biometrika  中就见过不少讨论ML方面的文章。当然，这些文章都很理论，看起来也相当头疼。\n比上面次一点的期刊就比较多了，例如\n Machine Learning （ML） Neural Computation IEEE Transactions on Neural Networks and Learning Systems（TNNLS） Pattern Recognition （PR）  以上皆为CCF B类期刊。ML作为出过SVM等著名模型的期刊，文章质量不言而喻。不过，就小Q看过的最近5年的期刊质量而言，窃以为现在质量下滑有点厉害啊（网上评价亦是如此）。Neural Computation在中科院分区一直为三区，但基本是大佬发挥的场地——年发文量仅90左右——文章一般很理论、有一定的深度。TNNLS在2012年前名为TNN，网友一般认为改名后期刊质量下滑，也有人认为这是柠檬精言论。PR通常被认为是除PAMI之外模式识别最好的期刊之一了。网友的评价一致较好，可见发文有难度、认可度也很高。\n以上是小Q结合自身看论文的感觉和网友评价的结果，仅供参考。不过，网上常常充斥着一种言论：只要是国内学者发文量多的期刊，就认为是水刊。这个现象不敢评论，因为小Q还在发文的征途中瑟瑟发抖~\n3 会议类 一般谈到ML方向的会议，通常离不开下面几个\n AAAI Conference on Artificial Intelligence（AAAI） Annual Conference on Neural Information Processing Systems（NeurIPS） International Conference on Machine Learning（ICML） International Joint Conference on Artificial Intelligence（IJCAI）  以上皆是CCF A类会议。小Q并不是CS专业出身，仅仅对这方面有兴趣，因此对会议的关注不太多，基本集中在NeurIPS和ICML上。从文章的风格来看，NeurIPS似乎侧重算法的创新但立足点要高；ICML似乎更加侧重模型创新和理论一点。不过，这两者应该没有明确的界限吧。AAAI在网上看到过不少负面评价，似乎文章质量在下滑。IJCAI则接触的比较少。总之，能够在A类会议发表文章，是非常需要实力的。\n4 结语 你是否好奇，同一类的会议和期刊哪个的质量更高呢？这个不好比较，但通常认为期刊的质量的更高。这是因为期刊的发文时间久，文章的各个部分可以做的更完整丰富。不管怎么说，能够在认可度较高的刊物上发表论文都是不容易且值得自豪的！\n","id":17,"section":"posts","summary":"人工智能（Artificial Intelligence，AI）是计算机科学（Computer Science，CS）领域的分支之一，通常包含机","tags":["机器学习"],"title":"人工智能方向的优秀刊物","uri":"https://qkai-stat.github.io/2020/08/ml-pub-introduction/","year":"2020"},{"content":" 文本分析是指对文本的表示及其特征项的选取；文本分析是文本挖掘、信息检索的一个基本问题，它把从文本中抽取出的特征词进行量化来表示文本信息。\t——百度百科\n 自然语言处理领域近些年来的研究非常火热，文本分析作为其分支之一也颇受关注。我之前分享过如何利用R绘制词云，也算和文本分析沾了边。\n在绘制词云的过程中，我们主要将大篇幅的文字内容进行“分词”，然后统计各个词组的词频，最后根据词频进行绘图。虽然这个过程听起来很简单，但通过垃圾短信案例我们还是可以得到很有意义的信息。不过，仅仅靠观察词云是非常粗糙的，这无法更加量化的分析文本的特征。\n本期我们一起来解密文章作者之谜，通过一些简单但比词云更加细化的统计思路和方法来分析文本，让大家稍稍感受分析文本的乐趣。\n1 背景介绍 一名文学研究员带着一份文稿匆匆找到正在学习统计理论的你寻求帮助。他说：“我手上这份文稿共有四篇散文，目前可以根据文稿来源断定是一人所作，但是具体作者是谁尚不肯定。经过各方专家分析写作手法，初步断定可能是鲁迅、朱自清或徐志摩所作。如今大数据这么火，请你帮我判断一下文章作者。”\n2 初步探索 读者此时可能正在努力回忆自己中学时代学过的有关文章，暗自揣摩它们之间的异同。诚然，想要判断文章作者，我们必须要了解每个作者的写作特点。因此，我们不妨在网上搜集到鲁迅、朱自清和徐志摩三人的散文文集。接着可以按照前文的方式分别做出它们的词云图案，观察之间的异同点。这里，笔者用R绘制出了他们三人各自散文的词云\n从上图看，三名作家的高频词汇竟然大同小异！回忆上一节的案例，读者不难想到其中原因。一方面，文章写作中很多字是常用字，大家出现的频率都高很正常。事实上，早期人们在破解密码时就利用写作中字（字母）出现的频率不同找到突破口。另一方面，一个朝代的文学尚且有一种主流，何况一个时代呢？三名作家生活年代相近，且正好处于新、旧文化更替时期，文章用字（词）大趋势类似也无可厚非。那么，我们的解谜难道就此止步么？\n3 细致分析 大趋势类似，之于个人风格的细节肯定互有不同。如果比较每个人风格的细节，应该就能得到蛛丝马迹。为了验证这样的想法，我们利用R软件对三名作家散文中的词频进行更为细致的比较，得到结果如图所示\n从图示结果来看，我们的猜测是正确的。图中每个点的横、纵坐标表示相应作者散文集中某词汇出现的频率，频率一致的点即构成灰色的虚线。显然，实际词汇的分布和虚线有一定的位移，说明鲁迅和另外两名作家的风格不同（类似的，可以对另外两名作家分析，不赘述）。比起徐志摩和朱自清，鲁迅似乎更喜欢在散文中提到动物，如猫、鼠、象和老虎等。\n继续观察图像，可看到鲁迅散文中的词频没有徐、朱二人散文中的词频更接近低频区，说明后两者在作文时采用的词汇更加丰富。这似乎符合我们对三人的印象——徐、朱的文章多具美感，故而可能使用更加丰富的辞藻；鲁迅的文章则冷静犀利，词汇的变化可能不太大。\n找到突破口后，我们还要确定一件事情：\n 假设没有重大的变故，一名作家的的文风数年之后会发生很大改变么？\n 如果是，那么这无疑增加了我们分析的难度。现在，我们以鲁迅的散文集为例，将其按时间顺序差不多等分成三部分，对比这三部分各自词汇频率的相似程度，结果如下图所示\n从图中可以看出，第2、3部分的词汇频率和第1部分词汇的频率基本对称分布在虚线两侧。这说明，鲁迅前后的文风没有发生很大的改变！同理，我们可以对其他两位作者的散文集做相同的分析，可以发现结果类似。那么，我们现在就可以利用词汇的频率进行相似度检验了。\n在R中，检测两个数值量之间的相似度可以用cor.test()命令，它基于的是皮尔逊相关性检验。我们使用下述命令\n# 相关性检测 cor.test(data = frequency[frequency$author == \u0026quot;zhuziqing\u0026quot;,],~proportion + test) cor.test(data = frequency[frequency$author == \u0026quot;luxun\u0026quot;,],~proportion + test) cor.test(data = frequency[frequency$author == \u0026quot;xuzhimo\u0026quot;,],~proportion + test)  分别得到待测文本和鲁迅散文集、朱自清散文集与徐志摩散文集的相似度系数为0.90917、0.9461695以及0.905819。至此，我们可以对研究员说“您这份文稿和三名作家的风格都极为接近，但经过我的分析，它出于朱自清先生手笔的可能性更大！”\n事实上，研究员的故事是笔者杜撰的，文稿也是笔者事先从朱自清散文集中随机抽取的四篇文章。笔者希望通过这个小小的案例让读者感受到探究文本分析背后奥秘的乐趣，以及R在数据分析中的强大之处。\n4 结语 上述案例的灵感来源本科大一的新生研讨课。那时刚刚步入大学接触统计学，主讲老师（也是我现在的博导）通过各种精彩的案例让我们感受到了统计学的美妙。期间，老师给我们讲到学者如何利用统计知识分析《红楼梦》前80回和后40回的差别，又在课程结束让我们探究老子的《道德经》是否为一人所作。因此，我在研究生多元课上接触到文本分析时，首先就想到该经历——也就杜撰了这个案例。\n参考文献 韦博成.《红楼梦》前80回与后40回某些文风差异的统计分析(两个独立二项总体等价性检验的一个应用)[J].应用概率统计,2009,25(04):441-448.\n","id":18,"section":"posts","summary":"文本分析是指对文本的表示及其特征项的选取；文本分析是文本挖掘、信息检索的一个基本问题，它把从文本中抽取出的特征词进行量化来表示文本信息。 ——","tags":["数据可视化","文本分析"],"title":"解密文章作者之谜","uri":"https://qkai-stat.github.io/2020/08/author-detection/","year":"2020"},{"content":"相较于QQ、微博、CSDN等博客（空间）平台，个人专属博客有很多优势：网站主题可定制性更强，博客写作更自由，保存与迁移更灵活，不会被删帖（哈哈哈哈~）等等。不过，对于我们广大网站搭建小白，学习从头部署一个博客网站耗时耗力、实属没必要，我们完全可以站在“巨人”的肩膀上快速实现部署博客的目的。\n本期给大家介绍如何使用Hugo和Gitee部署私人博客网站，你需要提前安装好Git软件。当然，想要成功实现本期的内容，懂得一些简单的Git命令和Markdown的语法不可或缺，因为网站的搭建是基于Git，网站的博客渲染是基于Markdown的。\n1 Hugo安装与配置  Hugo is one of the most popular open-source static site generators. With its amazing speed and flexibility, Hugo makes building websites fun again.\n Hugo是由Go语言实现的静态网站生成器，相较于同类的Jekyll和Hexo，Hugo最大的优势在于编译、渲染速度快。因此，Hugo 非常适合博客、文档等等网站的生成。目前，Hugo在Windows、MAC、Linux等平台都可下载安装。\n虽然制作网站听起来很复杂，但是Hugo实际使用过程很友好，加之Hugo官网有300+博客主题可供下载，用户可以在很短的时间内生成满意的博客网站。\n首先我们从官方渠道下载相应的安装包，Win10平台实际就是一个Zip压缩包，解压后可以看到一个50M不到的hugo.exe程序——没错！我们生成博客网站的主角就是它！\n我们只需将其中hugo.exe程序放在你喜欢的某个目录下（后面不要改动位置），例如我放在D:\\Program Files\\hugo下。在正式使用之前，我们需要将这个目录保存到系统环境变量中，这可以在网上轻易的找到教程，故不赘述。\n 所谓环境变量，简单来说就是在系统层面给这个程序的安装路径进行登记，使得我们通过CMD或Git直接输入程序名就能全局调用。\n 下面，我们随便在某处右键Git Bash Here打开Git，然后输入\nhugo version  若返回了相应的版本号，如\n则说明Hugo配置成功。\n2 初始化博客网站 我们在某处，如电脑桌面，右键Git Bash Here打开Git，然后输入\n# hugo new site \u0026lt;网站文件存放的名字\u0026gt; hugo new site myblogfiles  可以看到桌面出现一个名为myblogfiles的文件夹，里面就是生成的网站的配置文件。\n我们主要了解这么几个文件（夹）的作用即可：\n content：通常存放网站用到的文档 static：通常存放网站使用到的图片、音视频 themes：网站使用到的主题 config.toml：网站的配置文件  想要预览生成的网站，可以在myblogfiles目录下打开Git，然后输入命令\nhugo server  然后在自己的浏览器上访问网址：http://localhost:1313/ 就可看到当前网站的模样。\nhugo server运行后，只要不在Git中使用快捷键Ctrl+C停止，**就可以对网站的改动实时进行预览！**显然，我们打开网页什么也看不到——因为我们还没有对网站进行个人配置，自然什么也看到。\n3 使用Hugo主题 我们不必自己将网站从头配置成型（那样学hugo的意义也不大了），只须学会使用他人已经设计好的Hugo主题即可。目前，Hugo官网聚集了300+博客主题，内容涵盖学术风格类博客、技术风格类博客、文艺风格类博客等，可以直接下载使用。\n进入任一博客主题，你可以点击Demo查看网站的显示效果——换句话说，自己以后的网站就长这样，不同之处是内（nei）容（han）。有得必有失嘛，方便的同时必然牺牲了一些个性化，不过比之于平台提供的个人主页，Hugo主题的自由性显然高太多了。此外，博客主题的主页还详细介绍了该主题的使用方式。\n我们下载上述的Pure主题压缩包，解压后（文件夹名为pure）放在网站的theme文件下。\n为了让使用者可以在本地看到主题的效果，每个主题都含有一个名为exampleSite的文件夹，里面有主题作者事先放好的材料，他们最终将在网站上呈现出来。网站呈现的效果一般就是文章+图片（音视频），根据我们前面的介绍，通常只用干三件事：\n  将exampleSite中的content文件夹内容复制到我们网站的content文件夹中；\n  将exampleSite中的配置文件替换掉我们网站的配置文件；\n  将pure（主题根目录）中的static文件夹内容复制到我们网站的static文件夹中。\n  这时，我们回到网站的根目录打开Git，输入命令\nhugo hugo server  再访问 http://localhost:1313/ 时就看到pure主题渲染出的网站了\n可见，pure主题下博客分成三大区域展示，对于博文可以进行归档（Archive）、分类（Categories）和标签（Tags）。此外，该主题还可以设置博客的评论方式、分享渠道等。\n总之，网页上我们看到的内容基本都可以替换成自己的信息。具体替换要参考每个主题的使用介绍，但基本都是围绕content（放文章）、static（放图片、音视频）两个文件夹和网站配置文件。\n4 实现外网访问 如果做好的博客只能在本地自娱自乐，那就太没意思了！因此，学会如何实现外网访问很重要。这里我们选用国内的Gitee进行网站部署。\n Gitee不是必选项，国外的Github才是主流。不过因其服务器在国外，国内访问速度很慢，使得他人浏览博客的体验大打折扣。\n 首先，我们在Gitee上新建一个仓库，用来存放渲染好的博客网站的全部文件。这里，一定要注意！一定要注意！一定要注意！仓库的名称必须和你的Gitee账户名一致。\n 其实也不是必须一致，只是我们最终想实现通过网址 https://用户名.gitee.io/ 来访问博客的效果。如果你设置成其他名称，那博客网址就变成：https://用户名.gitee.io/博客名/ ，略显累赘。\n 然后，我们把网站本地配置文件中的链接改成自己的博客链接，也即是将baseURL对应的网址换成自己的博客链接。\n接着，我们就要把本地渲染好的博客网站文件推送到仓库中。问题来了：推送哪些文件呢？我们在博客目录下可以看到一个名为public的文件夹，它是在运行hugo后出现的——其中包含了博客网站的全部配置和内容。因此，我们只要将public文件夹下的内容推送到仓库即可。\n最后，我们回到Gitee中的博客仓库，可以看到服务中有个Gitee Pages选项。\n点击进入，选择强制使用HTTPS并更新。等待更新完成，博客网站就部署成功了！\n Gitee每次将public文件推送到仓库，要手动进行更新，否则博客还是前一次的内容；而自动更新是要收费的！Github可以免费使用自动更新功能，使用更友好，就是访问速度头疼。\n 读者可以访问我的个人博客，看看pure主题的最终效果。欢迎大家对我的博文评论，不喜勿喷~\n# 我的个人博客网址 https://qkai-stat.github.io/  5 结语 本期学习了使用Hugo和Gitee搭建个人博客。从上面的介绍来看，Hugo上手简单、渲染高效且配有众多主题进行网站快速部署。如果觉得有意思，就赶快动手实践吧！\n相关链接 Hugo官网：https://gohugo.io/\nHugo主题网址：https://themes.gohugo.io/\nHugo下载网址：https://github.com/gohugoio/hugo/releases\n","id":19,"section":"posts","summary":"相较于QQ、微博、CSDN等博客（空间）平台，个人专属博客有很多优势：网站主题可定制性更强，博客写作更自由，保存与迁移更灵活，不会被删帖（哈","tags":["git"],"title":"Hugo+Gitee搭建个人专属博客","uri":"https://qkai-stat.github.io/2020/07/hugo-gitee/","year":"2020"},{"content":"1 什么是Git-Submodule 在进行项目开发和管理的过程中，我们经常遇到这种情况：自己的A项目需要引用另一B项目中的内容，直接将B项目中的文件克隆合并到A项目中显得臃肿，不能保证两者的独立性；此外，直接克隆合并的方法也无法记录对于B项目的改动信息。\nGit给出了一种解决方案：将B项目作为A项目的Submodule进行植入。\n所谓Submodule，就是将一个代码仓库（次）作为另一个代码仓库（主）的子模块。在主项目开发过程中，既能引入Submodule中的内容，又能保证两者的相对独立。\n2 Submodule的创建 首先，我们在Gitee上创建两个新的仓库作为，其中一个作为主仓库（main-repository）而另一个作为子仓库（sub-module）。当然，刚刚创建的仓库还是平等的，没有主次之分。\n现在，我们在本地计算机的桌面上右键Git Bash Here打开Git终端，将main-repository仓库克隆到本地来\n# git clone \u0026lt;主仓库url\u0026gt; git clone git@gitee.com:jiandan94/main-repository.git  由于在创建仓库时我选择使用README初始化仓库，因此可以在本地看到仓库中有下面的文件\n下面，我们将创建的sub-module仓库以Submodule身份克隆到main-repository仓库中\ncd main-repository # 进入到main-repository仓库根目录 # git submodule add \u0026lt;子仓库的url\u0026gt; \u0026lt;子仓库本地路径\u0026gt; git submodule add git@gitee.com:jiandan94/sub-module.git submod  我将Submodule直接放在main-repository仓库的根目录下，执行上述命令后可见目录中文件变成\n多了一个.gitmodules文件和submod文件夹，其中sub-module仓库中的文件都存放在submod文件夹中。进入到main-repository仓库的config文件中，可见最后一行添加了Submodule的信息\n至此，我们成功的将一个仓库以Submodule身份引入主仓库。\n现在我们将main-repository仓库同步到远程仓库中\ngit add . git commit -m \u0026quot;upload submodule\u0026quot; git push -u origin master  这时去Gitee上查看main-repository仓库，发现多了.gitmodules文件和一个Submodule链接，点击链接可以跳转到我们在Gitee上创建的sub-module仓库。\n3 克隆带有Submodule的仓库 从上面可以看出，提交一个含有Submodule的仓库，不会将本地Submodule中的文件上传到main-repository仓库中，那么问题来了：克隆一个含有Submodule的远程仓库，会把Submodule仓库中的文件克隆下来么？\n我们不妨将本地的main-repository仓库文件夹删除掉，然后再次执行克隆命令得到\n然而，进入到submod文件夹中，发现是一个空文件夹！\n 也就是说，直接克隆一个含有Submodule的仓库，是不会将Submodule对应的仓库中的文件克隆下来。\n 为了将sub-module仓库中的文件也同步到本地，我们需要使用命令\ngit submodule init git submodule update  此时，再进入submod文件中，就可以看到同步过来的文件了。\n4 同步Submodule 如果远程的Submodule仓库中发生改动，比如我们在Gitee上的sub-module仓库中添加一个sub-aa.md的文件，我们只需要在本地main-repository仓库根目录用命令\ngit submodule update --remote  相当于进入到submod文件夹中执行git pull，将文件拉到最新。\n不过，在本地的submod文件夹中做修改，直接进入到submod中进行push通常是不会成功的。例如我们在submod文件夹中新建一个sub-bb.md文件，然后cd到submod文件夹中进行同步，\n请求被拒绝了！仔细观察，不难发现，此时所在的位置不是master分支上！\n原来，直接进入到submod文件夹可能不会切换到任何分支（branch）上，默认的Submodule的HEAD是出于游离状态的（detached）。因此，我们需要回到master分支上\ngit checkout master  这时，我们在本地Submodule上的改动就被提交到远程sub-module仓库中了。\n5 删除Submodule 想要删除已有的Submodule，首先删除下面的文件\n 删除.gitmodules文件 删除主仓库config中关于submodule的配置信息 删除子仓库的目录  最后，执行命令\n# git rm --cached \u0026lt;本地路径\u0026gt; git rm --cached submod  即可完成删除。\n","id":20,"section":"posts","summary":"1 什么是Git-Submodule 在进行项目开发和管理的过程中，我们经常遇到这种情况：自己的A项目需要引用另一B项目中的内容，直接将B项目中","tags":["git"],"title":"Git-Submodule的基本使用","uri":"https://qkai-stat.github.io/2020/07/git-submodule/","year":"2020"},{"content":" 热力图最初来源于网络，是以特殊高亮的形式显示访客热衷的页面区域和访客所在的地理区域的图示。在如今的大数据时代，热力图可是直接推动大数据发展的技术。\n 本期我们将用R来实现绘制区域热力图。在实现过程中需要用到下面几个程序包，大家可以提前下载安装好。\ninstall.packages(mapdata) install.packages(maptools) install.packages(ggplot2) install.packages(plyr) install.packages(rgdal) install.packages(mapproj)  1 mapdata绘制热力图 热力图的绘制自然离不开地图数据，maps是R中一个基本地图数据包，但是其中不包含中国的数据，因此我们需要额外下载mapdata数据包。\nmapdata数据包搜集了中国地区的地图数据，以及中国各省边界线的数据。使用下面的命令绘制出中国省级地图：\nchina_map \u0026lt;- rgdal::readOGR(\u0026quot;geo_data//bou2_4p.shp\u0026quot;) plot(china_map)  得到的地图较为扁平（左图），我们使用下面的命令进行调整使其显示正常（右图）。\n# 图像过于扁平，需要进行调整 ggplot(china_map, aes(x = long, y = lat,group = group)) + geom_polygon(fill = \u0026quot;white\u0026quot;, colour = \u0026quot;black\u0026quot;) + coord_map(\u0026quot;polyconic\u0026quot;) + theme( panel.grid = element_blank(),panel.background = element_blank(), axis.text = element_blank(),axis.ticks =element_blank(), axis.title = element_blank(), legend.position = c(0.2, 0.3) )  接下来我们可以使用在股票市场搜集的全国各省2018年9月4号的换手率（HS）和量比（LB）数据来绘制热力图，参考代码如下\nx \u0026lt;- china_map@data xs \u0026lt;- data.frame(x, id = seq(0:924) -1)# 地图中共计有925个地域信息 china_map1 \u0026lt;- fortify(china_map) china_map_data \u0026lt;- join(china_map1, xs,type = \u0026quot;full\u0026quot;)# 基于id进行连接# unique(china_map@data$NAME)# 查看地图数据中保存的地域名称，编辑自己的数据与其一致 mydata \u0026lt;-read.csv(\u0026quot;geo_data//data_area_stock.csv\u0026quot;, header = T, as.is = T) china_data \u0026lt;- join(china_map_data,mydata, type = \u0026quot;full\u0026quot;)# 基于NAME字段进行连接，NAME字段来自于地图文件中 province_city\u0026lt;-read.csv(\u0026quot;geo_data//pcity.csv\u0026quot;,header=T,as.is=T)#获取省会城市坐标 ggplot(china_data,aes(long,lat))+ geom_polygon(aes(group=group,fill =HS),colour=\u0026quot;grey\u0026quot;,size=0.01)+ scale_fill_gradient(low=\u0026quot;white\u0026quot;,high=\u0026quot;red\u0026quot;)+ coord_map(\u0026quot;polyconic\u0026quot;)+ geom_text(aes(x=jd,y=wd,label=name),data=province_city,colour=\u0026quot;black\u0026quot;,size=2.5)+ theme( panel.grid=element_blank(), panel.background=element_blank(), axis.text=element_blank(), axis.ticks=element_blank(), axis.title=element_blank() )##参数“HS”为换手率，基于该指标绘制热力图  就得到下面两张换手率和量比的全国各省热力图，通过颜色的深浅可以直观得看出各个省份地区之间的差异。\n2 baidumap绘制热力图 虽然mapdata可以绘制省级的热力图，但是其包含的而数据还是较少，这使得热力图的创建形式大打折扣。\n2011年1月10日，百度统计迎来了历史上最为重要的一次功能升级，全球第一款免费智能热力图功能正式上线。热力图可显性、直观地将网页流量数据分布通过不同颜色区块呈现，给中小网站网页优化与调整提供了有力的参考依据，方便合作网站提高用户体验。\n百度地图API是一套为开发者免费提供的基于百度地图的应用程序接口，提供基本地图搜索、位置搜索、周边搜索等功能。下面我们介绍使用R软件对接百度地图API来获取地图数据，从而进行热力图的绘制。\n这一部分的实现需要运用下面的软件包，大家在操作前先下载安装好。\ninstall.packages(baidumap) install.packages(REmap) install.packages(ggmap) install.packages(RCurl) install.packages(rjson)  要能够顺利的和百度地图API进行对接，一定要在百度地图平台申请到自己ak密钥，在R软件向百度地图发出请求时是需要该密钥才能成功获取相应的地图数据。\n## 绘制重庆大学2018年本科生全国计划招生人数热力图 province \u0026lt;- mapNames(\u0026quot;china\u0026quot;) # 全国省份 number \u0026lt;- read.table(\u0026quot;2018jhzs.csv\u0026quot;,header = T)# 重庆大学招生人数 cqu_data \u0026lt;- data.frame(province,number) cqu_data \u0026lt;- cqu_data[order(cqu_data[,\u0026quot;number\u0026quot;],decreasing=T),] cqu_data \u0026lt;- cqu_data[-(32:34),] origin \u0026lt;- rep(\u0026quot;重庆\u0026quot;,length(province)) destination \u0026lt;- province line_data \u0026lt;- data.frame(destination,origin) map_out1 \u0026lt;- remapC(cqu_data, maptype = \u0026quot;china\u0026quot;,# 绘制全国地图 title = \u0026quot;重庆大学2018年全国计划招生热力图\u0026quot;, theme = get_theme(\u0026quot;Drak\u0026quot;),# 设置热力图的主题 color=c(\u0026quot;#CD0000\u0026quot;,\u0026quot;#FFEC8B\u0026quot;),# 设置热力图颜色变化起、终点 markLineData=line_data,# 连线所需数据 markLineTheme=markLineControl(# 对连线进行格式设置 color=\u0026quot;white\u0026quot;, lineWidth=2, lineType=\u0026quot;dashed\u0026quot; ), markPointData=cqu_data[1],# 标注的地图地点数据 markPointTheme=markPointControl(# 对地图地点数据格式设置 symbolSize=13, effect=T, effectType=\u0026quot;scale\u0026quot;, color=\u0026quot;white\u0026quot; ) ) plot(map_out1)  我在网上搜集了2018年自己的母校重庆大学在各个省份的计划招生人数，通过R软件利用Remap包画出各个省份向重庆迁徙的示意图，同时每个省份的颜色深浅按照招生人数来着色。\n可以看出，R最终调用web来打开图片，并且这张图片是动态的且与用户交互的。\n大家可以通过baidumap获取地区数据，画出某个省份各个市的热力图。比如面这张我画的2018年1月份重庆市各个区县的平均房价热力图。\n3 结束语 当然，baidumap只是提供更加庞大的地图数据，想要画出精美的热力图还要好好研究Remap包的使用方式，通过调整不通的参数从而提高图片的表现力。\n","id":21,"section":"posts","summary":"热力图最初来源于网络，是以特殊高亮的形式显示访客热衷的页面区域和访客所在的地理区域的图示。在如今的大数据时代，热力图可是直接推动大数据发展的","tags":["数据可视化"],"title":"R绘制热力图","uri":"https://qkai-stat.github.io/2020/07/r-heatmap/","year":"2020"},{"content":"1 Git的工作原理 关于Git的简单介绍可以参照我的这篇博客。安装好Git之后，我们就要学会利用Git建立本地项目的代码仓库，以及同远程仓库进行同步。不过在学习搭建本地仓库之前，我们需要了解Git的工作原理，以便理解后续各段代码背后的含义。\n如上图所示。假设我们在本地计算机上建立一个仓库（文件夹）以存放项目的各个文件。进入该文件夹，也即是进入工作目录（working space）。然而，你直接在工作目录里面增加一个文件，比如files文件，从Git的视角来看，这个文件此时还不属于本地仓库的内容——换句话说，Git此时还没有登记该文件的信息。通过命令\n$ git add files  将files文件移到暂存区（Staged(indexed)）中，表明该文件即将存入本地仓库中。通过命令\n$ git commit -m \u0026quot;提交说明信息\u0026quot;  将暂存区的files文件移到本地仓库（Repository）中，并加上本次提交的的说明信息以方便以后快速了解本次操作的内容和目的。\n 上面我多次用到移到这个词，但是在仓库中并没有真的出现Staged或者Repository等文件夹或内容。实际上，以上的操作只是Git用自己的方式记录仓库中文件的变化，这些变动信息保存在仓库的.git隐藏文件夹中。\n 最后，我们通过命令\n$ git push  将本地的仓库同步到远程的仓库（Remote）中，完成本地和远程的连接。图中右侧的命令是每个阶段的逆操作。\n以上的介绍可以看出在Git的视角下，本地仓库的建立、改动以及和远程仓库的同步有着一套自身的规则，理解这套规则和理念有助于git的操作使用。\n2 本地仓库初始化 我们在本地计算机上建立一个文件夹来存放项目文件，然后进入到该文件中初始化仓库\n$ mkdir test $ cd test $ git init # 初始化仓库  此时，在test文件夹中便会出现一个.git隐藏文件夹，其中存放了仓库的各种信息如仓库的分支、操作的记录等。\n现在我们通过git status查看仓库的当前状态，返回结果\n$ git status On branch master # 在master分支上 No commits yet # 没有仓库操作 nothing to commit (create/copy files and use \u0026quot;git add\u0026quot; to track)  最后一句提示我们可以使用git add向暂存区添加文件。我们先创建一个文件aa.md，然后将其移到暂存区\n$ touch aa.md $ git add aa.md  再查看当前状态\n$ git status On branch master No commits yet Changes to be committed: (use \u0026quot;git rm --cached \u0026lt;file\u0026gt;...\u0026quot; to unstage) new file: aa.md  表明我们可以向仓库中提交aa.md，也使用git rm --cached aa.md将aa.md文件从暂存区移除。我们将文件提交到本地仓库，并再次查看当前的状态\n$ git commit -m \u0026quot;upload aa.md\u0026quot; [master (root-commit) cbf4f11] upload aa.md 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 aa.md $ git status On branch master nothing to commit, working tree clean  可见，Git已经将本次的改动记录下来，当前的工作区是干净的。\n3 与远程仓库同步 为了远程仓库可以识别本地仓库的作者信息，我们需要在本地设置用户名和用户邮箱\n$ git config --global user.name \u0026quot;****\u0026quot; $ git config --global user.email \u0026quot;****\u0026quot;  下面搭建远程仓库。远程仓库有很多平台，比如国外的Github和国内的Gitee，Github使用更加广泛、友好但国内访问速度很慢，这里用Gitee举例。我们访问Gitee官网，按照要求注册登陆后，创建一个仓库。\n然后在本地生成SSH密钥，以保证本地端和远程端的通信安全以及免密码登陆远程仓库，命令为\n$ ssh-keygen  生成的密钥在本地计算机用户文件夹下，将.ssh文件夹中的id_rsa.pub中的内容粘贴到Gitee账户中（设置\u0026gt;SSH公钥）即可。\n接着，我们在仓库的根目录下打开Git，连接到远程仓库\n$ git remote add origin **** # 这里的****是远程的仓库的SSH链接  最后，我们将本地仓库同步到远程仓库，命令为\n$ git push -u origin master  此时再去刷新远程仓库，就可以看到本地仓库的内容了。\n","id":22,"section":"posts","summary":"1 Git的工作原理 关于Git的简单介绍可以参照我的这篇博客。安装好Git之后，我们就要学会利用Git建立本地项目的代码仓库，以及同远程仓库进","tags":["git"],"title":"Git基本操作","uri":"https://qkai-stat.github.io/2020/07/git-basic-operation/","year":"2020"},{"content":"1 Git的历史 Git是一个分布式版本控制系统，由Linux的创始人Linus在开发Linux系统的过程中，为了各个参与者维护系统而开发的分布式版本控制系统。Git最初就是为了辅助Linux系统开发而诞生的，是版本控制系统Bitkeeper的替代者。\nGit是目前最先进的分布式版本控制系统。\n当然，版本控制系统除了分布式以外，还有其他的处理方案：\n 如果项目在开发过程中的各个版本的文件都存储在一台电脑上，就称之为本地版本控制 。显然，这种方式不适用于多人协同开发，因此较为复杂的项目不适用该方案。 另外一种常用的版本控制方式为集中式版本控制，代表产品有：SVN，CVS，VSS等。这种方式下，项目的所有历史文件和记录都保存在中央服务器上；每个开发者的电脑上只有自己之前同步过后的版本，没有联网时，无法查看历史版本信息。因此，集中式版本控制容易出现单点故障。 目前比较流行的分布式版本控制方案以Git为代表，它将项目的所有版本信息保存在服务器上，每个开发者的电脑上同时也保存了所有的版本信息。如果A或者服务器端的文件损坏，可以通过B进行恢复，避免了单点故障。   其实集中版本控制方案也很有存在的价值，分布式方案也不是最优方案。例如，公司的项目若很重要，分布式版本控制下，参与者带着全部文件跑路也不是不可能。为了介绍Git，只能强行踩踩SVN了，哈哈哈哈\n 2 Git的安装 首先去Git的官网下载相应的版本，当然，嫌下载速度慢的话可以去国内各个镜像进行下载，例如淘宝的镜像源下载即可。\n安装比较简单，记得选择在右键菜单中整合Git Bash，其他的直接无脑下一步就行了。\n安装好之后，我们可以在右键菜单中看到选项Git Bash Here，点击它会出现Git的命令行运行终端，并且当前的目录就是Git的工作目录。我们可以通过Git终端输入命令进行相应的操作和设置。\n3 常用的Linux命令 由于在Git使用中，我们通常使用命令行进行操作，这些命令又和Linux系统中的命令行格式类似，这里介绍一些常见的Linux命令。\n   命令 说明 示例     cd 改变工作目录 cd D:/ # 进入到D盘   cd .. 返回到上级目录    pwd 显示当前所在的路径    ls或ll 列出当前目录所有的文件    touch 在当前目录下新建一个文件 touch test.doc # 新建test.doc文件   rm 删除一个文件 rm test.doc # 删除test.doc文件   mkdir 在当前目录下创建一个文件夹 mkdir files # 新建files文件夹   rm -r 删除文件夹 rm -r files # 删除files文件   mv 移动文件 mv test.doc files # 移动test.doc到files文件夹中   clear 清除屏幕    history 显示操作历史    exit 退出    help 显示帮助     4 Git的简单配置 安装好之后，我们就可以对Git进行简单的环境配置。首先，我们在任意目录下右键Git Bash Here打开终端，然后输入\n# 查看Git系统配置信息 $ git config --system --list # 查看G当前用户(global)配置信息 $ git config --global --list  通常，我们需要更改用户的配置——因为我们要用Git实现本地和服务器的版本同步，配置好用户信息就方便我们和服务器进行联通。用户信息配置包括用户名和邮箱，这是服务器识别我们的重要依据，尽量配置正确的信息。\n# 配置用户名 $ git config --global user.name \u0026quot;myname\u0026quot; # 配置用户邮箱 $ git config --global user.email \u0026quot;myemail\u0026quot;  当然，配置的信息实际储存在本地的文件中，我们可以去相应的目录找到对应文件进行更改，也能实现配置的目的。\n# 系统配置文件路径 Git安装目录\\Git\\etc # 用户配置文件路径 C:\\Users\\用户名  系统配置文件名为gitconfig，用户配置文件名为.gitconfig，找到后对应修改即可。\n 换句话说，以文件为本质的操作，可以通过写命令创建/修改文件进行批处理，从而实现某个目的。\n ","id":23,"section":"posts","summary":"1 Git的历史 Git是一个分布式版本控制系统，由Linux的创始人Linus在开发Linux系统的过程中，为了各个参与者维护系统而开发的分布","tags":["git"],"title":"Git简介","uri":"https://qkai-stat.github.io/2020/07/git-introduction/","year":"2020"},{"content":"0 写在前面 作为经常用R跑程序、处理数据的你是否有过这样的苦恼：台式机性能强大但携带不便，笔记本携带自如但性能薄弱，每次只能在两台电脑间来回操作、疲惫不已却无能为力。\n那这样的场景多半会吸引到你：在有网络的地方，通过笔记本可以轻松的访问远程的R客户端，利用远程电脑运行程序；若程序比较耗时，你尽可以将R界面关闭去用笔记本处理其他事务，只用再次连接远程电脑就可以查看程序的运行情况\u0026hellip;..\n现在有很多云服务器可以实现上述的工作情景，不过可能价格不菲。但如果你手上恰有一台台式机+笔记本，以上情景可以通过RStudio-Server实现。本文就跟大家分享一下RStudio-Server环境的搭建经验。\n1 准备工作 1.1 Ubuntu系统 一般来说，Linux系统在处理程序时相较于Windows系统更高效，服务器端环境的搭建常常是在Linux上展开的。因此，我们首先要在台式机上安装Linux系统。目前比较流行的Linux系统版本有很多，例如受美国打压后国内热度迅速上升的deepin，我在好友H君的建议选择了Ubuntu 18.04，因为Ubuntu发展至今系统较为稳定，社区也很成熟，很多坑和雷都可以在网上找到解决方案。\n 事实上，作为一个爱国青年我最开始也尝试了deepin，桌面颜值很高但系统还不是很流畅。致命的问题是在安装RStudio-Server时，总报错OpenSSL版本不对，试了网上几乎所有的方法皆以失败告终。无奈，放弃。\n Ubuntu系统的安装也很简单。首先，我们在Ubuntu官网下载对应的版本镜像iso文件；官网的下载速度可能比较慢，我们可以选在国内的镜像源进行下载，以下是我搜集的一些常见下载镜像源，进入后选择对应的版本即可。\n# 中国官网（推荐） https://cn.ubuntu.com/ # 中科大源 http://mirrors.ustc.edu.cn/ubuntu-releases # 阿里云开源镜像站 http://mirrors.aliyun.com/ubuntu-releases # 兰州大学开源镜像站 http://mirror.lzu.edu.cn/ubuntu-releases # 北京理工大学开源 http://mirror.bit.edu.cn/ubuntu-releases # 浙江大学 http://mirrors.zju.edu.cn/ubuntu-releases  其次，我们在网上下载软碟通UltraISO软件，利用该软件进行装机启动U盘的制作。具体的制作过程可以问度娘，步骤很简单。\n最后，我们利用制作好的装机U盘进行系统安装。这个过程网上很多教程可参考，不过还是可能遇到一些不可控的问题。例如，我在安装的过程中就报错。好在网上有对应解决的策略，这也说明选择一个成熟的系统就是为自己省心。\n 安装系统只用记住一件事，设置好系统的用户名和密码并牢记！\n 1.2 安装R和RStudio-Server 系统安装好之后，连上网，然后就可以着手安装R和RStudio-Server两个客户端——工欲善其事必先利其器嘛。\n首先按住Ctrl+Alt+T快捷键呼出终端——这相当于系统的一个控制台，功能相当强大。用户可以通过终端输入命令进行各种操作，如网页访问、文件下载、文件解压、程序安装、各种环境部署等。\n)\n虽然Ubuntu提供了很多可视化的工具，但是作为一个有理想的小白，我还是觉得适应Linux就要从适应命令行开始！输入下面的命令安装r环境\n# 更新软件源 apt-get update # 安装r apt-get install r-base r-base-dev  安装完毕后，在终端输入R返回下面的结果就说明安装成功。\n)\n上面的返回结果其实就是R的操作环境 (和我们在Windows上的图形界面看到的结果一样)，我们可以接着输入R程序运行即可。例如，我输入了下面的命令进行画图，就会返回相应的图像。\n)\n接着，我们继续在终端中依次输入下面的命令进行RStudio-Sever安装。与桌面版的RStudio不同，服务器版提供了远程网页的访问功能，使用起来更加方便。\n# 安装必备文件 sudo apt-get install gdebi-core # 下载1.0.136版的rstudio-server安装文件，最新版自行去官网查看 wget https://download2.rstudio.org/rstudio-server-1.0.136-amd64.deb # 进行安装 sudo gdebi rstudio-server-1.0.136-amd64.deb  我们输入如下的命令测试安装情况\n# 开启rstudio-server sudo rstudio-server start # 查看rstudio-server状态 rstudio-server status  由于我的RStudio-Sever已在运行，这里只给大家展示运行状态，可见显示为Active，表明RStudio-Sever正在运行中。\n)\n你此时可能一脸懵逼，因为电脑上并没有看到RStudio的界面。前面说过，服务器版的RStudio是通过网络进行访问，它的端口号为8787，在本地计算机上只用在浏览器上输入ip:8787或者localhost:8787就可以看到图形界面。如果你可以看到，就说明你已经建好RStudio-Sever环境。\n)\n我们可以看到RStudio初始界面需要用账号密码登录，这个账号密码就是安装系统时设置的用户名和密码。我们输入后，就可以看到熟悉的操作界面了\n此时，你可以对其进行个性化设置，例如界面的布局、主题、字体；你也可以进行程序的下载和安装等等，这些操作如同Windows上一样，没有任何迁移的成本。\n2 配置远程访问环境 2.1 问题分析 看到这里，折腾一圈，似乎也没有看出和Windows端RStudio有多大区别，而且在另外一台电脑上输入ip:8787或者localhost:8787也无法访问。现在，我们就开始着手配置远程访问的环境。\n配置之前，我们要明白一个问题，为什么输入ip:8787或者localhost:8787无法访问服务器上的RStudio-Sever呢？因为通常情况下，我们每个人的网络环境都不是公网IP地址。所谓公网IP地址，可以简单的理解成全球唯一网络识别身份证号，在任何地方输入它就能访问对方。\n可惜，公网IP很稀缺，不是人人都有，这时就要知道内网的概念。以我们学校为例，学校有公网IP地址，但学校内部众多师生上网怎么互相识别身份？此时，我们可以通过交换机生成内网IP地址，这个地址在学校内部可以互相区别，但出学校后就无法访问了。\n上述的ip是内网地址，我们通过外网无法访问，从而不能实现远程操作的目的。因此，我们需要解决IP地址的问题。\n2.2 花生壳内网穿透 一个简单的解决策略就是，利用花生壳软件进行内网穿透，也即是生成一个可以被外网识别的IP地址。如何实现呢？\n首先，我们在服务器端安装花生壳软件。第一步，访问花生壳官网下载安装文件。第二步，按照官方的安装说明进行安装。第三步，按照官方的安装说明登录进行设置。\n 如果SN码不能正常登陆，可以使用手机号注册账户，然后用手机号登陆，再选择添加设备输入SN号绑定即可。\n 然后，我们在花生壳管理界面上选择增加映射，其中应用名称和应用图标可以随便取，只需注意填对服务器的IP地址和RStudio-Sever的端口号：服务器IP地址在终端输入ifconfig -a回车，返回的inet后面即是本地IP地址；RStudio-Sever的端口号为8787.\n)\n最后，我们在服务的终端上输入\nsudo phddns start phddns status  返回下面的就说明本地的花生壳已经在运行。\n)\n我们回到花生壳管理界面打开映射即可，此时我们就可以通过上述显示的的访问地址来进行RStudio-Sever的远程访问了。\n)\n在另外一台电脑的浏览器上输入上述访问地址，可以看到，我们能够从其他位置访问该服务器 (注意访问的IP地址是花生壳生成的)。\n)\n至此，我们终于实现一台笔记本走天下，重活累活都不怕的小目标了！！！\n3 写在最后 限于篇幅，很多问题在这里不能详尽介绍。例如，Ubuntu系统安装可能遇到的坑，花生壳内网映射设置的详细步骤等等，不过这些都可以通过上网快速找到解决方案，故而没有赘述。\n此外，关于RStudio-Sever本身，还有很多重要的问题需要读者自行摸索。例如，如何设置群组实现一个团队共同协作？实现不同账户之间的相对独立？例如，本地的代码如何同服务器端有效对接？例如，本地的数据表格如何同服务器端有效的互传？等等，都是在实际中常常面临的问题，这都需要读者自行去探索。\n","id":24,"section":"posts","summary":"0 写在前面 作为经常用R跑程序、处理数据的你是否有过这样的苦恼：台式机性能强大但携带不便，笔记本携带自如但性能薄弱，每次只能在两台电脑间来回操","tags":["R语言","算法"],"title":"RStudio-Server与远程交互","uri":"https://qkai-stat.github.io/2020/07/rstudio-server/","year":"2020"},{"content":"1 为什么要做变量选择？  变量选择又叫模型选择，因为选出不同的变量自然就建立出不同的模型。近30年来，变量选择理论繁荣发展并逐渐渗透到各个领域中，成为大数据时代背后的中坚理论之一。\n 随着技术手段的发展和丰富，我们获取数据变得越来越容易，对于一个问题可能搜集到成百上千的变量。有人说，变量越多对一个问题的刻画就越细致，这是好事啊。那么，我们为什么在建立模型时要煞费苦心思考变量选择问题呢？\n一方面，搜集到的变量中可能存在与目标完全无关的变量（冗余变量）。如果将冗余变量纳入模型之中肯定会得到一个错误的模型，用其进行预测自然就会做出误判，这明显与我们的目标相违背。因此，模型能够剔除不相关变量显得很重要。\n另一方面，尽管有些变量已知和目标相关，但实际的影响微乎其微（类似线性模型中变量系数的绝对值近乎为零），把这些变量包含在模型中无疑增加了理解模型的难度。比如，建立一个含有50+变量与一个仅有5个变量的公司员工能力评估模型，如果两者在评估员工能力时效果相当，显然后者更易于理解员工能力的影响因素且更易于指导实际工作。\n所以，研究变量选择是很有现实意义的。\n2 早期的思考 原有的方法若可用，就省的重新造轮子了。然而，经典的最小二乘 (OLS) 没有变量筛选的能力。回忆OLS估计表达式可知，不论变量是否重要，OLS表达式\n$$ \\hat{\\beta}^{ols} = (X^TX)^{-1}X^Ty $$\n没有体现出变量是否重要的差异性。\n因此，大家开始采用的是子集回归法 (subset regression) 而不是OLS方法。\n子集回归的思想很朴素。给定数据集，我们从中选出最佳变量组合建立回归模型即可 (全子集回归)。不过，考虑变量的所有组合，这和变量数 $p$ 呈 $2^p$ 关系。这种指数级的关系使得全子集回归成本高昂、不切实际。因此，大家想到一种折衷的办法——考虑一部分变量组合，找到其中的最优模型。这种思路类似优化理论的局部最优解，不过也可以得到较为满意的结果。相关的方法有逐步回归 (stepwise regression)等。\n子集回归有个严重缺陷：当原始数据中加入或者剔除某些变量后，子集回归得到的新模型可能发生巨大的变动。 比如，研究基因和某个疾病之间的关系。先搜集100个基因用子集回归得到3个重要的基因。后面研究后发现另外几个基因可能也和该疾病有关，但用子集回归重新建模后，原先的3个基因竟然没有出现在新模型中！这让人对该方法的可靠性产生怀疑。\n于是，就轮到LASSO上场了。\n3 LASSO模型 LASSO是Least Absolute Shrinkage and Selection Operator 的缩写。据LASSO的提出者Tibshirani本人说，LASSO是受到Breiman (1995) 提出的Nonnegative Garotte方法启发而得到的。NG方法首先建立原始数据的OLS回归，然后对得到的系数施加一个带有非负权重的约束惩罚，使得某些影响较小的系数被直接压缩到零，从而实现变量选择的目的。\n$$ \\begin{split} NG:\\quad\u0026amp;\\min \\sum _{i=1}^n (y_i - \\sum _{j=1}^p t_j \\hat{\\beta} _j^{ols} x _{ij})^2 \\newline \u0026amp;t_j\\geq 0,\\quad\\sum _{j=1}^p t_j \\leq s \\end{split} $$\nNG方法基于OLS，因而深受OLS影响。换句话说，OLS做不好时，NG也好不到哪里去。因此，Tibshirani抛弃了“两步估计”，直接给系数添加了一个绝对值约束，这就是LASSO模型：\n$$ \\begin{split} LASSO:\\quad \u0026amp;\\min \\sum _{i=1}^n (y_i - \\sum _{j=1}^p \\hat{\\beta}_j x _{ij})^2 \\newline \u0026amp;\\sum _{j=1}^p \\vert \\hat{\\beta}_j \\vert \\leq s \\end{split} $$\n两个模型形式确实相似。不过，LASSO和Hoerl与Kernnard在1970揭示的岭回归 (Ridge Regression) 更为相似，仅仅是罚项由绝对值变成平方项：\n$$ \\begin{split} Ridge:\\quad \u0026amp;\\min \\sum _{i=1}^n (y_i - \\sum _{j=1}^p \\hat{\\beta}_j x _{ij})^2 \\newline \u0026amp;\\sum _{j=1}^p \\hat{\\beta}_j^2 \\leq s \\end{split} $$\n因此，这么一个看似极其微小的改动，究竟神奇在哪里呢？\n- 参数s\n模型的参数s是需要事先给定的一个非负常数。以LASSO模型为例，如果 $s=0$，那么显然每个模型的系数都是零；如果 $s$ 趋于正无穷，那么约束相当于没加，结果就是OLS估计。当 $s$ 取一个不太大的数时，比如OLS系数估计的绝对值之和的一半。显然此时LASSO无法得到OLS估计，它需要降低某些系数的绝对值以满足约束条件。所以，参数 $s$ 实际控制了系数的压缩程度。\n OLS是无偏估计，但在变量数较多时估计量的MSE较大 (估计的系数不可靠)。MSE由估计量的方差和偏度组成，因此牺牲无偏性可以换来估计量方差显著减小，从而得到一个更加可靠的估计。\n - 罚函数\nLASSO约束系数从而提升估计量可靠性，你用这个思路分析岭回归也有同样的结论。那么，我们再来看两者罚函数形式的区别。考虑二变量情形，画出LASSO和Ridge模型的几何示意图有\n两者目标函数都是二次损失，对应一个椭圆。LASSO罚函数采用的是绝对值优化，可行域的边界有很多“尖点” (不可导点)。因此，目标函数优先和尖点触碰，也就形成了稀疏解 (某些系数为零的解)。 然而Ridge罚函数边界光滑，故而难以形成稀疏解。\n于是，我们可以看到LASSO的美妙之处：罚函数的小小改动就可以同时提升模型的性能、估计系数以及实现变量选择。\n LASSO最早发表在统计“四大天王”期刊之一的JRSSB上。原文没有一个理论证明，但仍然影响深远。可见，创新不见得一定要平地起高楼，站在巨人的肩膀上完全可以有卓越的突破。\n 4 LASSO模型的求解 4.1 设计阵正交 一般来说，求解LASSO这种带约束的凸优化问题，常常先考虑相应的拉格朗日函数。对于LASSO，拉格朗日函数为\n$$ LASSO:\\quad \\min \\sum_{i=1}^n (y_i - \\sum_{j=1}^p \\hat{\\beta}_j x_{ij})^2 + \\lambda \\sum_{j=1}^p \\vert \\hat{\\beta}_j \\vert $$\n其中 $\\lambda$ 是非负的拉格朗日乘数。如果我们假设数据矩阵 $X$ 是一个列正交矩阵，我们就能得到LASSO的显示解为\n$$ \\begin{split} \\hat{\\beta} _j^{LASSO} \u0026amp;= sign(\\hat{\\beta} _j^{ols})\\left(\\hat{\\beta} _j^{ols} - \\frac{\\lambda}{2} \\right) _+ \\newline (x) _+ \u0026amp;= \\begin{cases}x,\u0026amp;x\\geq 0 \\newline 0,\u0026amp;x \u0026lt; 0 \\end{cases} \\end{split} $$\n其中sign()表示取符号。在X列正交时，我们顺便求出Ridge的系数估计表达式为\n$$ \\hat{\\beta}_j^{Ridge} = \\frac{1}{1+\\lambda}\\hat{\\beta}_j^{ols} $$\n可见，LASSO的解其实就是对最小二乘的解做了一个压缩：对于较小的那部分系数，直接将其压缩到零；对于较大的系数，作了一个平移压缩， 如左图中橙色的线所示。Ridge估计就是对OLS估计做了一个整体的放缩，并没有直接压缩为零的过程， 如右图中蓝色线所示。\n因此，我们在这里也能看到罚函数的微小差异带来的巨大不同。绝对值惩罚在零点的不光滑性是LASSO产生稀疏性的本质。\n4.1 LARS算法 实际中，数据矩阵X一般不列正交，此时LASSO没有显示解。\n LASSO模型罚函数的奇异性像一柄双刃剑：它可以同时实现系数估计和变量选择，但也让基于梯度的算法失效而令其难以有效计算。事实上，LASSO问世初期没有得到人们关注的最大原因就是不好算！\n 据说，Tibshrani当时为了寻找一个有效的算法，曾四处拜访名家无果。最后，Tibshirani找到了自己的恩师Efron，大师出手一举摆平了此事。这就是著名的最小角回归 (Least Angle Regression, LARS)。\n有了LARS算法，LASSO像开了挂一般，迅速得到了蓬勃的发展。确实，统计模型往往来源于实际问题，如果模型仅仅停留在理论的空中楼阁，往往难以拥有生命力。当然，LARS不仅是一个算法，重要的是其从理论上揭示稀疏性 (Sparsity) 的本质。有兴趣的朋友可以拜读一下大师这篇90多页的大作！\n4.2 坐标下降法 LARS算法将LASSO的计算复杂度降到相当于OLS的复杂度，这基本就将其计算效率提升到极致。不过，这个记录后来被Friedman打破了。Friedman采用的是坐标下降法 (Coordinate Decent Algorithm, CD)。\n我们思考这么一个问题：如果一个函数在某点处各个维度取到了最小值，那么该点是不是函数的最小值点？\n 假如该结论成立，那么全局优化的过程就可以从每个维度进行一维优化即可。这显然极大地提升了模型的求解速度。\n 可惜，上述问题在一般情形下的答案是不一定。幸运的是，在LASSO模型中这个结论是成立的！因此，我们优化LASSO可以从每个维度 (几何上看就是每个坐标) 进行优化，采用迭代计算收敛到最优解即可。这就大大简化了模型的求解过程。\n考虑第 $j$ 个系数，我们根据KKT条件得到该出的最优解满足\n$$ \\begin{split} \\hat{\\beta}_j \u0026amp;= \\text{sign}(x_j^T r_j) \\frac{(x _j^T r _j - \\frac{\\lambda}{2}) _+}{\\Vert x _j \\Vert _2^2}\\newline r_j \u0026amp;= y - \\sum _{k\\neq j}^p x_k \\hat{\\beta}_k \\end{split} $$\n于是，我们便可以利用上式来迭代求解LASSO模型。\n4.3 正则化参数的选取 LASSO模型中的参数 $s$ 控制系数的压缩程度，拉格朗日函数中则由参数 $\\lambda$ 扮演该角色。通常，我们称这种参数为正则化参数，它们需要事先给定。\n正则化参数给大了，系数压缩的过偏，影响模型的表现；正则化参数给小了，系数压缩不明显，提升模型表现的作用不大。\n那么问题来了：怎么设定一个最优参数？\n通常，我们首先选定一系列的正则化候选参数；然后针对每个参数计算某一准则 (如AIC、BIC等) 的结果；最后根据准则确定最优参数的大小。\n5 R与LASSO 在R中，我们可以使用lars进行LASSO求解。使用前需要事先安装lars程序包，它由Efron和Hastie编写。此外，我们还可以使用glmnet求解LASSO模型。使用之前需要事先安装glmnet程序包，它由Tibshirani、Friedman等编写的。它们的基本用法如下\n# lars求解lasso的基本用法 fit \u0026lt;- lars(x, y, type = \u0026quot;lasso\u0026quot;, intercept = T)  # glmnet求解lasso的基本用法 fit \u0026lt;- glmnet(x, y, alpha = 1, lambda = NULL, intercept = T) predict.glmnet(fit, newx)  其中x为数据矩阵，y是响应变量，lambda是正则化参数。当alpha=1时，glmnet计算LASSO模型；当alpha=0时，glmnet计算Ridge模型；当0\u0026lt;alpha\u0026lt;1时，glmnet计算Elastic Net模型，一个LASSO的改进模型。此外，模型的截距项参数intercept默认是T，改成F即为无截距。\nglmnet本身优化了模型的计算。对于给定的数据集，glmnet自动计算一系列lambda (默认100个) 对应的系数估计，且每次计算都会利用到上次估计的结果。因此，计算一系列lambda的速度比单独给lambda赋值计算的总耗时要快的多。如果你想用CV (cross validation) 的方式寻找最优参数，也可以自己手动设置参数或者使用cv.glmnet函数。\n6 模拟分析 我们在R当中产生一个含有10个变量和1000个观测的数据矩阵，系数为 (1,0,1,0,\u0026hellip;,1,0) 且模型无截距，具体代码如下\nset.seed(123) samplen \u0026lt;- 1000 samplep \u0026lt;- 10 x \u0026lt;- matrix(rnorm(samplep*samplen), ncol = samplep) b \u0026lt;- rep(c(1,0),5) y \u0026lt;- x%*%b + rnorm(samplen)  6.1 使用lars求解 使用lars函数求解模型，并用plot函数画出系数估计的路径图\nlibrary(lars) fit \u0026lt;- lars(x, y, type = \u0026quot;lasso\u0026quot;, intercept = F) plot(fit)  图中的竖线表示lars算法的迭代次数，非零值对应的即是被选入的变量，图的上刻度数字表示本次迭代选入的变量数，图的右刻度数字表示最终选入的变量名。\n我们可以使用summary函数查看每次迭代的具体情况，可以按照最小 $C_p$ 值得到最优的模型。\n看来 $C_p$ 准则得到的模型和真实模型有点出入。实际估计的系数可以看到，非零系数基本接近1，零系数基本接近零，但 $C_p$ 准则在本案例中稀疏的能力不佳。\n\u0026gt; fit$beta[which.min(fit$Cp),] [1] 0.98459718 0.04174324 1.02200983 0.05431813 1.02213893 0.00000000 [7] 0.95807588 0.01790684 1.02019194 0.04779414  6.2使用glmnet求解 使用glmnet求解，再用print函数输出结果：每个lambda对应的模型选入的变量数Df和模型解释的偏差%Dev (R方)。\n\u0026gt; library(glmnet) \u0026gt; fit \u0026lt;- glmnet(x, y, intercept = F) \u0026gt; print(fit) Call: glmnet(x = x, y = y, intercept = F) Df %Dev Lambda 1 0 0.00000 1.23800 2 1 0.04079 1.12800 3 3 0.08998 1.02800 4 5 0.18710 0.93620 5 5 0.29830 0.85310 6 5 0.39060 0.77730 7 5 0.46730 0.70820 8 5 0.53090 0.64530 9 5 0.58370 0.58800 10 5 0.62760 0.53570 ···  选取Df=5和最大%Dev对应得lambda值，利用coef函数得到相应得系数估计结果。\n\u0026gt; coef(fit, s = 0.06305) 11 x 1 sparse Matrix of class \u0026quot;dgCMatrix\u0026quot; 1 (Intercept) . V1 0.9243286 V2 . V3 0.9650517 V4 . V5 0.9656673 V6 . V7 0.8985474 V8 . V9 0.9700144 V10 .  看来结果不错。不过，我们利用cv.glmnet筛选最优参数发现，最优模型也是对应9个变量。\n\u0026gt; cv_fit \u0026lt;- cv.glmnet(x, y) \u0026gt; op_lam \u0026lt;- cv_fit$lambda[which.min(cv_fit$cvm)] \u0026gt; coef(fit, s = op_lam) 11 x 1 sparse Matrix of class \u0026quot;dgCMatrix\u0026quot; 1 (Intercept) . V1 0.97767537 V2 0.03679953 V3 1.01549521 V4 0.04761603 V5 1.01535413 V6 . V7 0.95157838 V8 0.01017659 V9 1.01429797 V10 0.04150092  看来我随便生成的这个数据把这两个准则给难住了。\n","id":25,"section":"posts","summary":"1 为什么要做变量选择？ 变量选择又叫模型选择，因为选出不同的变量自然就建立出不同的模型。近30年来，变量选择理论繁荣发展并逐渐渗透到各个领域中","tags":["数学史","回归分析","多元统计","变量选择","高维数据分析"],"title":"变量选择与LASSO","uri":"https://qkai-stat.github.io/2020/04/lasso-introduction/","year":"2020"},{"content":"1 背景介绍 最近，有朋友跟我说起股票数据处理的问题。由于课程要求，学生需在软件中下载股票指数数据 和对应的成分股收盘价数据，然后对指数和收盘价做一个分析，比如回归分析。\n金融软件导出的数据是指数+所有的成分股的CSV文件，每个CSV文件格式相同，包含时间、开盘价、收盘价、成交量等。那么，问题来了——怎么把这些文件整合到一起去呢？\n准确来说，这个问题要考虑以下这些点：\n 指数和所有成分股要按时间一一对应，每一行代表一个时间点的指数和成分股收盘价数据； 成分股遇到停盘数据便会有缺失，要能够根据最近的收盘价进行填补。  以前也有相应的R处理代码供使用。不过在和同学交流时发现，这个代码运行经常报错。因此，我自己就编了一个。下面说说我编写的思路。\n2 算法设计 我设计这个算法的思路很朴素。首先，指数每天都有不会缺失。因此，我首先把指数都读进来提取出指数的时间点 和收盘信息。 然后，根据成分股的个数创建合适的矩阵先保存好指数数据。如下图所示。\n接着，对第一个成分股进行处理——把时间 和收盘价 提取出来。先不用管它哪里缺失了需不需要填充数据。\n 我们要明确一点，导出的时间段内，指数肯定是最完整的。因此，我们将成分股按照时间跟它匹配肯定可以实现。\n 那么，我们就根据时间点 把第一个成分股匹配到指数所在的矩阵中，也就是下面这样的图像。\n从上图看出，第一支股票在时间点3和时间点4有缺失数据。那么按照填充要求，最近的时间点2有数据，因此可以用来填充，就得到了下面的结果。\n以此类推，我们把有的股票都按照时间匹配到这个矩阵中，并做好缺失数据填充，结果就是下面这样。\n那么最后就简单了——按照木桶原理 把前面没有信息的数据全部砍掉即可，如下图所示。\n3 R 程序代码 这个程序我当时为了赶作业图省事用了很多for循环，因此数据量大的时候，运行有点慢。\n程序分为日线数据处理代码 和分钟线数据处理代码。 显然，可以把这两个合并到一起，不过课已上完我就懒得弄了。\n另外，这是根据西南金点子软件 导出数据结构编写的，其他金融软件导出的数据对应改下应该就没问题了。\n- 代码使用方式\n【1】我桌面是默认的工作目录 【2】上证指数日线csv数据放在桌面上 【3】桌面上的data文件夹用来放置50只成分股csv数据（一定要保存成csv格式） 那么参数就可以写成 wdir\u0026lt;-\u0026quot;C:/Users/***/Desktop/\u0026quot; ######## (要换成自己的路径) filedir \u0026lt;- \u0026quot;data\u0026quot; index.name \u0026lt;- \u0026quot;sz50.csv\u0026quot; data_org(wdir,filedir,index.name)## 日线数据函数 minutedata_org(wdir,filedir,index.name)## 分钟线数据函数 其中出现下面警告直接无视，这只是将字符转换成数字时系统的温馨提示。 Warning message: In data_org(\u0026quot;C:/Users/tom/Desktop/\u0026quot;, \u0026quot;data\u0026quot;, \u0026quot;sz50.csv\u0026quot;) : NAs introduced by coercion 程序运行后就将分散的指数和成分股数据按照时间整合到一起了，其中缺失数据用前一天补充。  - 日线处理程序\ndata_org \u0026lt;- function(wdir,filedir,index.name){ #设定文件所在路径与读入文件 setwd(wdir) f.names \u0026lt;- list.files(filedir) stock_names \u0026lt;- substr(f.names,4,9) f.dir \u0026lt;- paste(\u0026quot;./\u0026quot;,filedir,\u0026quot;/\u0026quot;,f.names,sep=\u0026quot;\u0026quot;) n.names \u0026lt;- length(f.names) # 首先阅读股票指数数据 index_data \u0026lt;- read.csv(index.name,skip = 2,header = F,stringsAsFactors = F) index_data \u0026lt;- index_data[1:(nrow(index_data) - 1),c(1,5)] colnames(index_data) \u0026lt;- c(\u0026quot;date\u0026quot;,\u0026quot;index\u0026quot;) index_data$date \u0026lt;- as.Date(index_data$date)# 转换成时间格式 index_data$index \u0026lt;- as.numeric(index_data$index)# 将指数数据转换成数值型 index_data \u0026lt;- na.omit(index_data)# 去除NA的行 # 处理成分股数据 n_zero \u0026lt;- rep(0,n.names) stocks \u0026lt;- list() file_data \u0026lt;- index_data for(i in 1:n.names){ # 读取数据 stocks[[i]] \u0026lt;- read.csv(file = f.dir[[i]],skip = 2,header = F,stringsAsFactors = F) stock_data \u0026lt;- stocks[[i]][1:(nrow(stocks[[i]]) - 1),c(1,5)] colnames(stock_data) \u0026lt;- c(\u0026quot;date\u0026quot;,paste0(\u0026quot;Stock\u0026quot;,stock_names[i])) stock_data$date \u0026lt;- as.Date(stock_data$date)# 转换成时间格式 stock_data[,2] \u0026lt;- as.numeric(stock_data[,2])# 将股票数据转换成数值型 stock_data \u0026lt;- na.omit(stock_data)# 去除NA的行 # 根据指数的日期来处理成分股数据 temp1 \u0026lt;- rep(0,length(index_data$date)) for (t in 1:length(stock_data$date)){ date_index \u0026lt;- grep(stock_data$date[t],index_data$date) temp1[date_index] \u0026lt;- stock_data[,2][t] } for (k in 2:length(index_data$date)) { if(temp1[k] == 0){ temp1[k] \u0026lt;- temp1[k - 1] } } n_zero[i] \u0026lt;- which(temp1 \u0026gt; 0)[1] temp2 \u0026lt;- data.frame(temp1) colnames(temp2) \u0026lt;- paste0(\u0026quot;Stock\u0026quot;,stock_names[i]) file_data \u0026lt;- cbind(file_data,temp2) } data_trunc \u0026lt;- max(n_zero) file_data \u0026lt;- file_data[-(1:data_trunc),] write.csv(file_data,paste0(\u0026quot;org_\u0026quot;,index.name)) }  - 分钟线处理程序\nminutedata_org \u0026lt;- function(wdir,filedir,index.name){ #设定文件所在路径与读入文件 setwd(wdir) f.names \u0026lt;- list.files(filedir) stock_names \u0026lt;- substr(f.names,4,9) f.dir \u0026lt;- paste(\u0026quot;./\u0026quot;,filedir,\u0026quot;/\u0026quot;,f.names,sep=\u0026quot;\u0026quot;) n.names \u0026lt;- length(f.names) # 首先阅读股票指数数据 index_data \u0026lt;- read.csv(index.name,skip = 2,header = F,stringsAsFactors = F) index_data \u0026lt;- index_data[1:(nrow(index_data) - 1),c(1,2,6)] colnames(index_data) \u0026lt;- c(\u0026quot;date\u0026quot;,\u0026quot;minute\u0026quot;,\u0026quot;index\u0026quot;) index_data$dateminute \u0026lt;- paste0(index_data$date,index_data$minute) index_data$index \u0026lt;- as.numeric(index_data$index)# 将指数数据转换成数值型 index_data \u0026lt;- na.omit(index_data)# 去除NA的行 # 处理成分股数据 n_zero \u0026lt;- rep(0,n.names) stocks \u0026lt;- list() file_data \u0026lt;- index_data for(i in 1:n.names){ # 读取数据 stocks[[i]] \u0026lt;- read.csv(file = f.dir[[i]],skip = 2,header = F,stringsAsFactors = F) stock_data \u0026lt;- stocks[[i]][1:(nrow(stocks[[i]]) - 1),c(1,2,6)] colnames(stock_data) \u0026lt;- c(\u0026quot;date\u0026quot;,\u0026quot;minute\u0026quot;,paste0(\u0026quot;Stock\u0026quot;,stock_names[i])) stock_data$dateminute \u0026lt;- paste0(stock_data$date,stock_data$minute) stock_data[,3] \u0026lt;- as.numeric(stock_data[,3])# 将股票数据转换成数值型 stock_data \u0026lt;- na.omit(stock_data)# 去除NA的行 # 根据指数的日期来处理成分股数据 temp1 \u0026lt;- rep(0,length(index_data$dateminute)) for (t in 1:length(stock_data$dateminute)){ date_index \u0026lt;- grep(stock_data$dateminute[t],index_data$dateminute) temp1[date_index] \u0026lt;- stock_data[,3][t] } for (k in 2:length(index_data$dateminute)) { if(temp1[k] == 0){ temp1[k] \u0026lt;- temp1[k - 1] } } n_zero[i] \u0026lt;- which(temp1 \u0026gt; 0)[1] temp2 \u0026lt;- data.frame(temp1) colnames(temp2) \u0026lt;- paste0(\u0026quot;Stock\u0026quot;,stock_names[i]) file_data \u0026lt;- cbind(file_data,temp2) } data_trunc \u0026lt;- max(n_zero) file_data \u0026lt;- file_data[-(1:data_trunc),] write.csv(file_data,paste0(\u0026quot;org_\u0026quot;,index.name)) }  ","id":26,"section":"posts","summary":"1 背景介绍 最近，有朋友跟我说起股票数据处理的问题。由于课程要求，学生需在软件中下载股票指数数据 和对应的成分股收盘价数据，然后对指数和收盘价做","tags":["R语言","算法"],"title":"股票数据的合并","uri":"https://qkai-stat.github.io/2020/03/stockbind/","year":"2020"},{"content":"1 线性模型的局限性 在线性模型中，一个重要的条件便是响应变量 $y$ 须服从正态分布。然而，实际问题往往更加复杂，$y$ 并不总是满足正态分布的假设。\n例如，在医学诊断中，我们希望通过病人的各项检查数据判断其是否患癌症。这里是否患癌症作为响应变量 $y$ 只有两个可能的取值：患和不患。 显然，$y$ 不服从正态分布。\n类似的例子还有很多。在气象领域中，通过观测到的气象数据判断是否会下雨；在金融领域中，根据当前的金融指标判断是否抛售股票；在图像领域，综合输入的人脸数据判断是否通过验证\u0026hellip;\u0026hellip;\n上述问题的共同点是，$y$ 的取值是离散的，我们关心的不再是预测具体数值而是变成判断分类。\n为了讨论方便，我们以下考虑二分类问题。\n机灵的同学可能想到，依然对数据建立线性模型，然后再设定一个阈值 $a$，当 $y\u0026gt;a$ 时，预测数据归为一类；当 $y\u0026lt;a$ 时，预测数据归为另一类。\n这么做确实可以得到一个分类模型(上图红实线)。但是，如果我们在原有数据上额外得到一个新观测且远离原始数据时， 线性思路得到的新模型就会出现明显的变动(上图蓝虚线)。\n2 逻辑回归(Logistic Regression) 究竟哪里出现问题了呢？我们回忆所学的线性模型的形式：\n$$ y_i = x_i^T\\beta + \\epsilon_i, \\quad\\epsilon_i~N(0, \\sigma^2) $$\n对模型两边取期望和方差可得：\n$$ E(y_i) = x_i^T\\beta, \\quad Var(y_i) = \\sigma^2 $$\n显然可以看出：(1)线性模型对y的估计，本质是对 $y$ 期望的估计；(2)线性模型中 $y$ 的期望和方差两者互相独立。\n接着来看二分类问题，一般我们考虑 $y_i$ 独立同分布于成功概率为 $p_i$ 的伯努利分布(Bernouilli Distribution)：\n$$ y_i \\overset{iid}{\\sim} B(p_i) = p_i^{y_i}(1 - p_i)^{1 - y_i} $$\n两边取期望和方差可得：\n$$ E(y_i) = p_i, \\quad Var(y_i) = p_i(1 - p_i) $$\n此时的期望和方差两者之间存在函数关系且期望取值在 $[0,1]$ 之内。 因此，在二分类问题中完全套用线性模型的框架显然是有问题。\n该怎么办呢？\n试想 $y_i$ 是一个随机变量，在不知道额外信息的前提下，用它的期望作为估计还是挺合理的。因此，我们可以继续采用期望估计的思路。 但是二分类问题中 $y_i$ 的期望有0-1的约束，这就需要我们寻找一种满足该要求的函数。通常，这样的函数选取成Logistic函数：\n$$ E(y_i) = \\frac{1}{1 + \\exp(-x_i^T\\beta)} $$\n很容易验证Logistic函数满足前面的条件。选取这种函数形式的深层原因，从统计的角度来看，源自于指数族分布。 有兴趣的朋友可以翻阅我关于广义线性模型相关理论的介绍以及其他参考资料。\n于是，我们终于等到本次的主角——逻辑回归模型(Logistic Regression Model)：\n$$ y_i \\overset{iid}{\\sim} B(p_i),\\quad E(y_i) = \\frac{1}{1 + exp(-x_i^T\\beta)} $$\n我们之所以称其为回归模型，是因为线性回归深入人心，在其之上生长发展的理论便继承了这个名称。事实上，逻辑回归更多的是面向分类问题。\n3 逻辑回归的求解 3.1 极大似然法 最小二乘常被用来求解线性模型中的系数估计值，这在逻辑回归中失效了——我们无法得到逻辑回归的最小二乘损失函数的具体形式。\n然而我们知道问题的概率分布信息，因此可以得到相应的似然函数：\n$$ \\mathscr{L} = \\prod_{i = 1}^n \\left(\\frac{1}{1 + \\exp(-x_i^T\\hat{\\beta})} \\right)^{y_i} \\left(1 - \\frac{1}{1 + \\exp(-x_i^T\\hat{\\beta})} \\right)^{1 - y_i}$$\n再进而得到对数似然函数并进行优化即可。不过，与线性模型具有显示解不同，逻辑回归没有解析解，故而需要结合梯度下降法等算法进行求解，可参考广义线性模型中的相关方法。限于篇幅，不在赘述。\n3.2 R求解逻辑回归模型 R中内置glm函数可以求解逻辑回归模型，它的使用方式如下所示。\nlogmodel \u0026lt;- glm(formula, data, family = binomial(link = \u0026quot;logit\u0026quot;)) predict.glm(logmodel, newdata)# 预测模型  与线性模型求解函数lm区别在于，glm需要添加link参数。我们使用该函数对前面的模型再次求解并做图，如下所示。\n可见，逻辑回归很好的刻画了模型的主要特征，且不会因为新数据的出现模型发生巨大变动。\n4 案例分析 4.1 数据描述 我们从UCI机器学习数据库中获取到威斯康辛乳腺癌数据(Breast Cancer Wisconsin)共计569个观测，包含1个字符型分类变量，取值B表示良性肿瘤，取值M表示恶性肿瘤；30个数值型检测指标，如肿块厚度、大小、形态等。\n4.2 建立逻辑回归模型 我们使用R读入数据并进行分析。由于数据没有特定顺序，我们选择前70%作为训练集，剩下的作为测试集。为了方便展示结果，我们仅选取前三个变量 进行模型拟合，对应的代码如下所示。\n## 逻辑回归模型处理WDBC数据 wdbc \u0026lt;- read.csv(\u0026quot;wdbc.csv\u0026quot;, header = F, stringsAsFactors = F) colnames(wdbc) \u0026lt;- c(\u0026quot;y\u0026quot;, paste0(\u0026quot;x\u0026quot;, 1:30)) wdbc$y \u0026lt;- ifelse(wdbc$y==\u0026quot;M\u0026quot;, 1, 0) # 划分训练集和测试集 wdbc_train \u0026lt;- wdbc[1:400, 1:4] wdbc_test \u0026lt;- wdbc[401:569, 1:4] # 训练模型 wdbc_fit \u0026lt;- glm(y~., data = wdbc_train, family = binomial(link = \u0026quot;logit\u0026quot;))  4.3 评价模型 首先，我们使用summary()函数查看模型相关的统计指标，具体如下所示。\n可见，三个系数都非常显著。\n其次，我们使用predict.glm()查看模型的内预测效果，得到混淆矩阵(Confusion Matrix)如下\n    真实M 真实B 总计     预测M 215 18 233   预测B 12 155 167   总计 227 173 400    可以算出模型的预测准确率为(155+215)/400=0.925，仅三个变量就可以很好的预测病人肿瘤的状态。\n不过分类问题中，大家比较关心模型的敏感性和特异性。 敏感性指真阳性率，而特异性指真阴性率。以本案例而言，模型的敏感性为0.947，特异性为0.896。这么看来，我们的模型假阳性率为0.104，还是不太好。\nlibrary(pROC) roclong \u0026lt;- plot.roc(wdbc_train$y, lty = 1, train_pre, grid = T, percent=TRUE,ci=TRUE,col=\u0026quot;red\u0026quot;, print.auc = T,print.auc.cex = 0.8,print.auc.x = 60,print.auc.y = 50, main = \u0026quot;逻辑回归的ROC曲线和AUC值\u0026quot;)  进一步分析敏感性和特异性，可以利用ROC曲线 和AUC值 来刻画。我们在R中使用pROC包画出ROC曲线以及进行AUC的计算和检验，结果参见下图。\nAUC达到了92.2%， 说明模型能够很好的刻画我们的数据。\n最后，我们对剩下的测试集进行验证，得到预测准确率为88.76%，还是挺理想的——毕竟我们仅使用三个变量进行建模。关于逻辑回归更多的内容，还请读者自己了解。\n参考文献\n[1] Myers, Raymond H., et al. Generalized linear models: with applications in engineering and the sciences. 2012.\n","id":27,"section":"posts","summary":"1 线性模型的局限性 在线性模型中，一个重要的条件便是响应变量 $y$ 须服从正态分布。然而，实际问题往往更加复杂，$y$ 并不总是满足正态分布的假设。 例","tags":["多元统计","分类算法","r语言"],"title":"逻辑回归模型","uri":"https://qkai-stat.github.io/2020/03/logistic-regression/","year":"2020"},{"content":"远行不经意，归途暮雨袭。\n阑风入沉阁，竹斋听雨息。\n乱蝉声渐渐，梧桐叶槭槭。\n雁阵来时路，年年有归期。\n","id":28,"section":"posts","summary":"远行不经意，归途暮雨袭。 阑风入沉阁，竹斋听雨息。 乱蝉声渐渐，梧桐叶槭槭。 雁阵来时路，年年有归期。","tags":["虎溪岁月","诗文"],"title":"偶遇秋雨","uri":"https://qkai-stat.github.io/2019/11/oyqy/","year":"2019"},{"content":" 如果随便逮到一个统计专业的学生问他“统计方法谁家强”，相信大部分人会异口同声得说“最小二乘法”。\n 的确，最小二乘法是一种非常重要的统计方法，它的重要性不仅仅体现在对问题求解的自然、简单、有效层面，其背后所蕴含的“最小二乘思想”更在不同领域、不同问题中应用广泛。\n虽然以现在的眼光来看最小二乘法出发点朴素而又自然，但是它的产生也是历经波折。本文我们一起来了解一下最小二乘法的“心路历程”吧！\n1 天文和测地学 早期，在天文和测地学中经常会遇到这么一种数据分析情况：我们有若干个可以测量的量 $x_1$, $x_2$,\u0026hellip;, $x_p$ 和 $y$，他们之间呈现一种线性关系\n$$ y = \\beta_0 + \\beta_1x_1 + \\cdots + \\beta_px_p $$\n这里 $\\beta_1$,\u0026hellip;, $\\beta_p$ 都是未知参数，需要我们运用一定的方法估计出来从而应用到实际问题中去。\n现在稍有统计背景的都知道这用最小二乘可以轻易求解，不过当时还没有最小二乘的概念呢，因此求解就是一个令人头疼的问题。\n 可能有人说根据线性方程理论这也不是一个很困难的问题啊！只要测量得到 $p$ 组数据，那么应用线性方程的知识不就将未知参数求解出来了么？\n 可是在实际天文和测地学中同一问题研究人员会测量多组数据以降低测量过程中产生的误差，而相应的数据量基本都是大于未知参数的个数的 (其实就是超定线性方程组)，这就使得问题的求解比较棘手。\n我们希望的是，一方面对于方程的个数大于未知参数个数的问题，求解方法应该尽可能的简单、有效；另一方面，由于实际问题中测量误差不可避免，求解方法在理论上应该对误差控制有一个保证。所以总结起来就是四个字：又快又稳！\n2 早期工作 在最小二乘还未诞生的时代，各路英雄豪杰面对该问题也是使劲浑身解数。不过他们的方法核心思路是从大量的方程中挑选或者组合恰当数量的方程来进行参数求解。因此，各家各派使得都是挑选和组合的功夫。\n例如在1750年，天文学家梅耶发表了一种测定航海船只经度的方法，其中要从27个方程中求解出3个未知参数。他主要采用分组法将27个方程分成3组分别相加得到3个方程，最终求解出未知参数。 这个方法曾经一度流行，被冠以梅耶的名字。\n 此外，拉普拉斯和欧拉也在天文学中研究过类似的问题，但是解法极其繁杂而且杂乱无章。 他们二人这样的大数学家一生不知道解决过多少数学中的“疑难杂症”，对于这么一个并不是很困难的问题竟然束手无策。这确实让人感到不可思议。\n 3 勒让德和最小二乘 勒让德是法国大数学家，在数学的很多领域包括椭圆几分、数论和几何等方面都有着重大的贡献。\n$$ (\\hat{\\beta}, \\hat{\\beta}_0) = \\min \\sum _{i=1}^n \\left( y_i - x_i^T\\hat{\\beta} - \\hat{\\beta}_0 \\right)^2 $$\n最小二乘法最早于1805年勒让德公开发表的文章《计算彗星轨道的新方法》中问世。在这本著作的附录中，勒让德描述了最小二乘法的思想、具体做法和优缺点。\n 最小二乘法一经提出，由于其思想自然合理、操作简单有效，很快就得到欧洲一些国家的天文和测地工作者的广泛使用。据不完全统计，自1805年到1864年，有关这一方法的研究论文约有250篇。\n 尽管勒让德的工作没有涉及到最小二乘的误差分析理论，但是他也注意到了各个方程因为误差不独立而不能直接运用最小二乘法， 这的确难能可贵。最小二乘法的“快”勒让德已经说明了，关于它的“稳”则是高斯的工作了。\n4 高斯的工作 1809年，高斯在《绕日天体的运动理论》的一节中讨论了“数据结合”的问题，实际就是误差分布的确立问题。假设真值为 $\\theta$，有 $n$ 个独立的测量值 $x_1$,\u0026hellip;, $x_n$，高斯将后者的概率定为\n$$ \\mathscr{L}(\\theta) = f(x_1 - \\theta)\\cdots f(x_n - \\theta) $$\n其中 $f$ 就是待定的误差密度函数。在确立密度函数形式过程中，高斯有两个创新点。\n  一是他没有采取已有的贝叶斯推理方法，而是直接将 $\\mathscr{L}(\\theta)$ 的最大值——极大似然的思想 ——定为 $\\theta$ 的估计值。\n  二是他先承认了观测值 $x_1$,\u0026hellip;, $x_n$ 的算数平均值为 $θ$ 的估计值，然后再去找误差的密度函数来迎合这一点——在这样的 $f$ 下，$\\theta$ 的估计值就是算数平均值。最后他得出只有在\n  $$ f(x) = \\frac{1}{\\sqrt{2\\pi}h}e^{-\\frac{1}{2h^2}} $$\n时才成立。这就是均值为 $\\theta$ 标准差为 $h$ 的正态分布.\n 使用正态分布就可以对最小二乘给出一种解释，也就是可以对其误差做出理论上的分析，保证了这种方法的优越性。后世将最小二乘的发明权归功于它，也正是因为这一项工作。\n 尽管高斯讨论最小二乘法的文章发表较晚，但是他声称自己很早之前就运用勒让德的最小二乘法来解决问题。这也导致两人后来最小二乘的首创权争论。\n不过在高斯的证明中有点循环论证 的感觉，先承认算数平均值估计的优越性，再得到误差正态密度函数形式，然后再说明算术平均值作为估计的合理性。这一缺陷在拉普拉斯运用其发现的中心极限定理得以解决。 他指出现实中的误差可以看成很多量的叠加，那么根据他的中心极限定理，误差的分布就是正态分布。\n5 写在最后 从最小二乘法的发展历史来看，一项科学理论的发展并无坦途， 尽管这项理论看起来朴素而又简单。\n","id":29,"section":"posts","summary":"如果随便逮到一个统计专业的学生问他“统计方法谁家强”，相信大部分人会异口同声得说“最小二乘法”。 的确，最小二乘法是一种非常重要的统计方法，它","tags":["数学史"],"title":"漫谈最小二乘法","uri":"https://qkai-stat.github.io/2019/08/forum-ols/","year":"2019"},{"content":"1 导言 回归分析是一个古老的话题。一百多年前，英国的统计学家高尔顿 (F. Galton，1822-1911) 和他的学生皮尔逊 (K. Pearson，1857-1936) 在研究父母和其子女身高的遗传关系问题中，统计了1078对夫妇的平均身高以及他们的一个成年儿子的身高数据。\n他们将孩子的身高作为自变量 $x$，父母的平均身高作为因变量 $y$，然后将两者画在同一张直角坐标系上。结果，他们发现这些数据点“惊人的”位于一条直线的附近，并且经过计算得到了直线的拟合方程:\n$$ y = 33.73 + 0.516x $$\n 这个结果看起来是违背直觉的。因为统计的结果表明，高个子父母的子女有低于父母身高的趋势；而矮个子的子女则有高于父母的趋势。高尔顿解释说，自然界存在某种约束力将人的身高向某个“平均数”靠拢——或者说是“回归”——也即是统计学上回归的涵义。\n 那么本文的主题便是了解线性回归模型并通过R来解决线性回归分析中的若干问题。\n2 基础回顾 回归的概念来源于实际问题，那么现在我们所说的线性回归分析问题具体指的是什么呢？一般说来，如果我们研究的问题中的 $p$ 个自变量 $x_1$, $x_2$, \u0026hellip;, $x_p$ 和因变量 $y$ 的关系形式如下所示\n$$ y_i = \\beta_0 + \\beta_1x_{i1} + \\cdots + \\beta_px_{ip} + \\epsilon_i $$\n那么我们就说这是一个线性回归问题，其中 $\\epsilon_i$ 是随机误差项，$i$ 表示第 $i$ 个观测。在线性回归问题中我们的核心任务就是估计出未知参数 $\\beta_0$, $\\beta_1$, $\\cdots$, $\\beta_p$ 的值。\n 注意，线性回归问题的确定并不是通过自变量的形式，而是问题中待估计的未知参数最高次都为一次且关于未知参数呈线性关系。例如 $y = \\beta_0 + \\beta_1x_1^2 + \\epsilon$；$y = \\beta_0 + \\beta_1x_1x_2 + \\epsilon$ 都是线性回归问题。\n 通常在线性回归中估计未知参数方法是最小二乘法（OLS），而为了保证估计值能够很好的解释模型，我们又有如下前提条件：\n 正态性：$\\epsilon_i$ 服从正态分布； 独立性：$\\epsilon_i$ 之间是独立的； 线性性：$x$ 和 $y$ 必须线性相关； 同方差性：$\\epsilon_i$ 的方差不变。  这些条件又被称为高斯—马尔可夫条件， 它们保证了在经典线性回归中最小二乘估计的优越性。\n3 求解线性回归模型函数 3.1 极大似然法 最小二乘法和极大似然法都可以用来求解线性回归模型，我们在往期文章中讨论过最小二乘法，这里对似然法进行简单介绍。\n假设我们得到下面一组观测数据：\n$$ (x_1, y_1), (x_2, y_2), \\cdots, (x_n, y_n) $$\n那么根据高斯-马尔可夫假设，我们可以得到残差估计的似然函数 为\n$$ \\mathscr{L} = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[-\\frac{(y_i - x_i^T\\hat{\\beta} - \\hat{\\beta}_0)^2}{2\\sigma^2}\\right] $$\n 这个式子的成立还需要假设残差分布的均值为0，标准差为 $\\sigma$。这个假设是可行的。因为残差如果均值不为零，可以将其移到模型的截距项里。\n 如何通过上面的函数得到系数的估计值呢？极大似然的思想便是，让这些估计值使得似然函数达到最大！ 这个想法很朴素：每个观测数据随机且互相独立，我们一次搜集便得到眼前的数据，那么自然而然认为这些数据组合出现的概率是最大的。\n不过，数据已经搜集好便不能改动。我们自然想到，系数的估计值便是让这些数据对应的概率可能性最大——也即是似然函数最大。\n现在假装大家已经理解了极大似然的原理，下面我们来求解它。直接最大化不太可行，我们通常对似然函数取对数得到对数似然函数\n$$ \\ln\\mathscr{L} = -\\frac{n}{2}\\ln(2\\pi\\sigma^2) - \\sum_{i = 1}^n\\frac{(y_i - x_i^T\\hat{\\beta} - \\hat{\\beta}_0)^2}{2\\sigma^2} $$\n然后再分别对各个参数进行优化。限于篇幅，不再赘述。\n3.2 R求解线性回归模型 我们可以利用现有软件进行模型求解。在R中求解线性回归问题的最基本函数就是lm()，其格式为：\nmyfit \u0026lt;- lm(formula, data) # formula 是要拟合的模型形式，用一个R公式表示 # data 就是模型的数据构成的数据框  下面我们解释一下formula具体的形式，首先看下表总结的formula中常用的符号\n   符号 说明     ~ 分隔符号，左边为因变量，右边为自变量   + 分隔自变量   : 自变量的交互项，如 xz 可以表示成 x:z   * 自变量的所有交互项，如 x*z*w 展开即为 x+z+w+x:z+x:w+z:w+x:z:w   ^ 交互项可以达到某个次数，如(x+z+w)^2展开即为x+z+w+x:z+x:w+z:w   . 除因变量外的所有自变量   -1 删除截距项   I() 如x+I((z+w)^2)等价于x+h，h是z+w平方构成的新变量    如果自变量为 $x_1$, $x_2$ 和 $x_3$ 而预测变量为 $y$，我们假定的线性模型形式为：\n$$ y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + \\epsilon $$\n那么formula可以写成：\ny ~ x1 + x2 + x3 # 或者为 y ~ .  其他形式的模型formula的表达式还请读者自行琢磨。\n当模型拟合成功后，我们使用summary()函数来得到拟合的具体结果。而其他常用的获取线性回归模型拟合结果的函数如下表所示。\n   函数 说明     summary() 拟合情况概述，包括系数显著性、模型显著性等   coefficients() 拟合系数   fitted() 因变量拟合值   residuals() 残差值   plot() 画出模型诊断图    4 实例分析 下面我们将用实例具体介绍lm()函数的使用方法。\n4.1 简单线性回归 本例中我们使用基础安装中的数据集women数据，它记录了15个年龄在30~39岁间女性的身高和体重信息，我们现在来探究体重关于身高的关系。\nmyfit \u0026lt;- lm(weight~height, data = women) summary(myfit) # 展示拟合详细结果  程序的输出结果如下所示\n这里主要给读者解释这么几项指标的含义：\n  Residuals 体重预测值和真实值之差的统计信息，从左到右分别为最小值、下四分位数、中位数、上四分位数和最大值。\n  Coefficients 第一列Estimate中Intercept对应的数值为截距项，height对应的即为身高变量前的估计系数。\n  Multiple R-squared 介于0-1之间，越接近1说明线性关系越强。\n  p-value 模型的F检验统计量的p值，值越小说明模型越可靠。\n  因此本例中体重和身高的回归方程为：\n$$ \\hat{Weight} = -87.51667 + 3.45\\times Height $$\n根据R方 (Multiple R-squared) 和p值 (p-value) 可知模型是可靠的。此外，我们可以作图观察最终的拟合结果。\n4.2 具有交互项的线性回归 继续考虑上例，如果模型中存在一个交互项比如一个平方项，那么即有：\nmyfit \u0026lt;- lm(weight~height + I(height^2), data = women) summary(myfit) # 展示拟合详细结果  程序的输出结果如下所示。\n可以看到通过比较R方、p值，添加了平方项的线性模型效果更好。我们同样可以做出相应的图像。 5 写在最后 本文主要介绍了R中线性回归分析的简单操作方法。不过，这里仅仅涉及线性回归分析的冰山一角，关于线性回归问题中的回归诊断和异常点的判断等内容，限于篇幅这里就不做介绍了。有兴趣的读者可以学习《R in action》第8章中关于回归的讲解。\n","id":30,"section":"posts","summary":"1 导言 回归分析是一个古老的话题。一百多年前，英国的统计学家高尔顿 (F. Galton，1822-1911) 和他的学生皮尔逊 (K. Pearson，185","tags":["多元统计","r语言","回归分析"],"title":"简单回归分析的R实现","uri":"https://qkai-stat.github.io/2019/07/simplelm/","year":"2019"},{"content":"风清露重晓月寒，\n跫音嘶语破门关。\n玉兔何知人间苦，\n捣碎桐叶一地残。\n","id":31,"section":"posts","summary":"风清露重晓月寒， 跫音嘶语破门关。 玉兔何知人间苦， 捣碎桐叶一地残。","tags":["虎溪岁月","诗文"],"title":"望月有感","uri":"https://qkai-stat.github.io/2019/05/wyyg/","year":"2019"},{"content":"远山疏林轻雾笼，暖阳探窗听晨钟。\n庭前初落银杏雨，芳华凋零又几重。\n不期秋霜侵柳绿，可怜金风老梧桐。\n昨日阆苑尚新蕊，行人行色行匆匆。\n","id":32,"section":"posts","summary":"远山疏林轻雾笼，暖阳探窗听晨钟。 庭前初落银杏雨，芳华凋零又几重。 不期秋霜侵柳绿，可怜金风老梧桐。 昨日阆苑尚新蕊，行人行色行匆匆。","tags":["虎溪岁月","诗文"],"title":"久雨初晴有感","uri":"https://qkai-stat.github.io/2018/11/jycqyg/","year":"2018"},{"content":"晴岚裙腰山染黛，\n空谷幽幽暗香来。\n时人剪却好风光，\n月下说与娇娘猜。\n","id":33,"section":"posts","summary":"晴岚裙腰山染黛， 空谷幽幽暗香来。 时人剪却好风光， 月下说与娇娘猜。","tags":["虎溪岁月","诗文"],"title":"七夕","uri":"https://qkai-stat.github.io/2018/08/qx/","year":"2018"},{"content":"1 背景介绍 人工神经网络其实是一种过旧的概念，早在20世纪80年代至90年代初期就已被广泛使用，但后来热度逐渐消退。近年来，人工神经网络强势回归，现在被认为是处理很多机器学习难题的最先进技术。追本溯源，当时人工神经网络热度消退的原因之一是其需要大量、繁复的运算。但是如今计算机的运算能力已经远超往日，故缺陷不再而优势凸显。\n本期主要的目标是用R实现人工神经网络算法，并用一个实际案例来检验效果。在本次案例分析中需要用到的R包为neuralnet，需要大家事先下载安装好。\n2 神经网络的概念 这一节我们先一起来认识一下人工神经网络的概念。看完之后，大家对神经网络的工作原理和运作机制有一个整体的了解。\n2.1 人工神经元 人工神经网络顾名思义，就是模拟人的神经网络运行机制来实现学习和处理信息的目的。要了解其本质必然要研究人体神经网络原理，那么研究其中的一个个体——神经元就理所应当了。下图是一个人体神经元的示意图。\n一般当人体接受到外界的信息后，这些信息将通过神经元的树突传入到神经细胞体，接着细胞体将对这些信息进行分析处理，例如加权，从而得到一个综合结果。如果综合值超过一个阈值，那么细胞体将会被激活从而输出一个对应的信号到达轴突。人工神经网络中人工神经元正是模仿了这种机制。\n从外界接受到的信号X包含若干个变量，他们分别按照一定的权重被吸收，然后在某种阈值机制（根据某个函数来确定，该函数叫激励函数）最终输出信号。这个结果可以用如下公式表示。\n$$ y(x) = f\\left(\\sum_{i=1}^n w_i x_i\\right) $$\n人工神经网络正是使用这种方式定义的神经元来构建复杂的数据模型。一般我们要把握人工神经网络以下几个概念：激励函数，拓扑结构，训练算法。\n2.2 激励函数 激励函数可以理解成一个过程，包括对输入的总信号求和，以及计算其是否满足激活的阈值。如果满足，人工神经元即传递信号；否则，人工神经元不做任何操作。通常的激励函数形式如下图所示。\n激励函数和具体的神经网络有关，对于某些特别的数据，激励函数的选择也是十分重要，不过一般我们可以选择高斯函数就可以了。\n2.3 拓扑结构 人工神经网络的强大能力来源于它的拓扑结构，而把握其拓扑结构可以抓住下面3个关键概念：\n 层数 网络中信息是否向后传播 每一层的节点数  对照上图，一般人工神经网络分为3层：输入层、隐藏层和输出层。输入层和输出层顾名思义不再赘述，我们简单说说隐藏层。回忆前面所说神经元的机制，输入信号直接按权重求和并计算激励函数值做判断，这个过程过于简单。隐藏层的意义在于，它使得整个分析处理的过程更加精细，同时也更加复杂。\n神经网络中，一般信息的传播方向是从输入信号一个节点接一个节点直至传到输出层。这样的网络称为前馈网络。虽然听上去很简单但产生的能量是很大的。与之对应的是反馈神经网络，信号可以循环在两个方向传播。虽然这种方式更加贴近实际神经网络工作模式，但是实际用到的很少。\n可能不用我说大家也知道，网络中的节点数对最终的结果影响显著。但是不幸的是，并没有可信的规则来确定每一层的节点数。 我们可以根据具体问题、运算量等尝试选取。\n2.4 误差向后传播 前面一直没有讨论权重的选取，显然这深深影响人工神经网络的实际表现。实际中我们先随机赋予权重，然后采取一种误差向后传播的策略来调整权重，即\n 在前向阶段中，神经元在从输入层到输出层的序列中被激活，沿途应用每一个神经元的权重和激励函数，一旦到达最后一层，就产生一个输出信号。 在后向阶段中，由前向阶段产生的网络输出信号与训练数据中的真实目标值进行比较，网络的输出信号与真实目标值之间的差异产生的误差在网络中向后传播，从而来修正神经元之间的连接权重，并减少将来产生的误差。  这里还有一个重要的问题就是，输入信号众多且关系复杂，如何调整具体一个信号的权重从而降低总的误差呢？这不在本文讨论之内。但是在R中直接调用相关包进行计算，所以这不是问题。\n3 R人工神经网络实现 3.1 数据搜集 经过前面的认真铺垫，终于进入到本期的主题——用R实现人工神经网络！本次使用的案例是研究某地公路客运量和公路货运量的预测，先使用head()函数查看数据的前几行：\n可见该数据集一共有五个变量，按照上图从左到右分别是：人数（万人）、机动车数量（万辆）、公路面积（万平方公里）、公路客运量（万人）和公路货运量（万吨）。\n 这里要提醒大家，神经网络的运行最好是将数据标准化到0附近。如果数据呈现正态，则直接使用R中scale()函数标准化即可；如果数据呈现严重偏态如均匀分布，则对其采用极大极小化处理即可。本案例中数据不满足要求，需做处理。\n 我们采用极大极小化标准化方法——将每列数据转换成0-1之间。这里我们直接编写一个函数来完成这个操作。\nminmax \u0026lt;- function(x){#R代码—极大极小标准化： for(i in 1:ncol(x)){ x[,i] \u0026lt;- (x[,i] - min(x[,i]))/(max(x[,i]) - min(x[,i])) } return(x)}  3.2 neuralnet函数介绍 接下来我们要了解neuralnet包中neuralnet()函数的使用方法。\nneuralnet(target~predictors, data, hidden) # target表示模型的输出变量 # predictors表示输入变量，一般用一个R公式表示 # data则是包含上面两者数据框 # hidden则是隐藏层的神经元数目（默认为1）  3.3 应用模型建模求解 有了前面的介绍，这一步就显得水到渠成了，请看下面代码\nlibrary(neuralnet) conv_road_data \u0026lt;- minmax(road_data)# 分割数据集 train_data \u0026lt;- conv_road_data[1:18,] test_data \u0026lt;- conv_road_data[19:20,] # 建立模型 m_train \u0026lt;- neuralnet(glkyl+glhyl~sqrs+sqjdcs+sqglmj, data = train_data,hidden = 6) plot(m_train)# 画出模型拓扑图  我们将数据集前18个作为训练，后2个用来预测。运行上述代码可以得到训练的神经网络模型的拓扑结构图。\n这显示了最终模型得到的权重信息。并且可以看到，本案例中神经网络一共迭代314次，总的误差为0.049404.\n接下我们使用compute()函数进行预测\nm_test \u0026lt;- compute(m_train,test_data[,1:3])  那么我们怎么得到训练样本和预测样本的具体估计值呢？不管是neuralnet()函数还是compute()函数，它们的输出结果中net.result一项就是具体的估计值，因此可以从前面结果中获取。\ntrian_estimate \u0026lt;- matrix(unlist(m_train$net.result),ncol = 2) test_estimate \u0026lt;- m_test$net.result  值得一提的是m_train$net.result得到的是一个列表，因此在这里我们将其去列表化转成了一个矩阵以便于后续分析。接下来不要忘记，我们是的数据都是标准化了的，所以要通过逆变换得到真实结果。\n最后我们可以画出真实值和估计值之间的对比图，观察具体的模型效果。\n从最终的对比图来看，模型的效果还是不错的。\n","id":34,"section":"posts","summary":"1 背景介绍 人工神经网络其实是一种过旧的概念，早在20世纪80年代至90年代初期就已被广泛使用，但后来热度逐渐消退。近年来，人工神经网络强势回","tags":["机器学习"],"title":"人工神经网络","uri":"https://qkai-stat.github.io/2018/08/ann/","year":"2018"},{"content":" 顾名思义，词云就是由词汇组成类似云彩的图形。对于篇幅庞大的文本而言，使用词云进行描述可以过滤掉很多冗杂而无意义的信息，诸如空格、标点符号、停词（像 and，or，but 等）等。\n 1 什么是词云？ 词云是近几年来比较新潮的一个名词，是很多媒体或者社交软件用来展现文本中“关键词”出现频率的有效手段。\n观看者一眼扫过就可以快速获取文本中的高频词汇信息。 因此，词云在文本可视化展示中有着很强的表现力。\n词云制作需要词和相应频率的信息，但通常我们面临的是原始文本数据。因此，本文试着完整展示整个过程——先从原始文本数据出发，经过必要的数据清洗，最后实现画词云的目的。\n在实现过程中需要借助强大的文本处理包tm包和SnowballC包，以及具有较大可操作性的词云制作包wordcloud2包，大家提前在 R 中安装好。\ninstall.packages(\u0026quot;tm\u0026quot;) install.packages(\u0026quot;SnowballC\u0026quot;) install.packages(\u0026quot;wordcloud2\u0026quot;)  2 案例导入  本案例是Brett Lantz所著的《机器学习与R语言》中的相关章节内容的重现。\n 我们考虑垃圾短信分类问题——现实中很多人深受垃圾短信其害，因此我们希望设计一套算法通过分析短信中的文本内容来判断其是否为垃圾短信从而采取规避措施。\nsms_raw \u0026lt;- read.csv(\u0026quot;sms_spam.csv\u0026quot;,stringsAsFactors = F) str(sms_raw) 'data.frame': 5559 obs. of 2 variables: $ type: chr \u0026quot;ham\u0026quot; \u0026quot;ham\u0026quot; \u0026quot;ham\u0026quot; \u0026quot;spam\u0026quot; ... $ text: chr \u0026quot;Hope you are having a good week. Just checking in\u0026quot; \u0026quot;K..give back my thanks.\u0026quot; \u0026quot;Am also doing in cbe only. But have to pay.\u0026quot; \u0026quot;complimentary 4 STAR Ibiza Holiday or 拢10,000 cash needs your URGENT collection. 09066364349 NOW from Landline\u0026quot;| __truncated__ ...  可见该数据包含两个变量，分别是短信的标签（type）：垃圾（spam）和正常（ham）以及短信内容（text）。\n3 程序设计 为了分别得到垃圾短信和正常短信的词云，首先采用subset函数按照短信标签将spam和ham的短信内容分别提取出来。\nsms_spam \u0026lt;- subset(sms_raw,type == “spam”)# 选取所有的垃圾信息 sms_ham \u0026lt;- subset(sms_raw,type == “ham”)# 选取所有的正常信息  以正常短信为例，考虑先将所有的正常短信内容拼接到一起。\ncomposition \u0026lt;- paste0(sms_ham,collapse = ' ')# 将所有信息拼合在一起  接下来就是词云制作过程中的重要也是困难部分——文本处理。\n通常短信内容包含了很多冗杂的信息比如前面提到的标点等，显然对于本问题能提供的有效信息微乎其微，因此我们需要在制作词云前将他们移除出去。这里就需要R中强大的tm包和SnowballC包。\nsms_corpus \u0026lt;- VCorpus(VectorSource(composition))# 建立语料库 sms_corpus_clean \u0026lt;- tm_map(sms_corpus,content_transformer(tolower))# 全部转换成小写字母 sms_corpus_clean \u0026lt;- tm_map(sms_corpus_clean,removeNumbers)# 移除数字 sms_corpus_clean \u0026lt;- tm_map(sms_corpus_clean,removeWords,stopwords())# 移除停词 replacePunctuation \u0026lt;- function(x){gsub(\u0026quot;[[:punct:]]+\u0026quot;,\u0026quot; \u0026quot;,x)} sms_corpus_clean \u0026lt;- tm_map(sms_corpus_clean,removePunctuation)# 移除标点 sms_corpus_clean \u0026lt;- tm_map(sms_corpus_clean,stemDocument)# 提取词干 sms_corpus_clean \u0026lt;- tm_map(sms_corpus_clean,stripWhitespace)# 移除多余空格  至此文本处理工作就结束了，我们可以通过命令\nstr(sms_corpus_clean) List of 1 $ 1:List of 2 ..$ content: chr \u0026quot;c chope good week just check kgive back thank also cbe pay aiya discuss later lar pick u much buzi pleas ask mu\u0026quot;| __truncated__ ..$ meta :List of 7 .. ..$ author : chr(0) .. ..$ datetimestamp: POSIXlt[1:1], format: \u0026quot;2018-09-08 07:08:14\u0026quot;...  看到短信文本内容已经转换成意义较强的词汇（当然，你可能觉得里面仍然包括一些无意义的词汇，这通常和你所采用的停词表有关）。在制作词云前还有一项关键的步骤就是计算词频。\nwordfreq \u0026lt;- termFreq(sms_corpus_clean[[1]])# 计算词频 wordfreq \u0026lt;- sort(wordfreq,decreasing = T)# 词频排序  最后我们就可以使用wordcloud2包制作词云了。在wordcloud2包中制作词云的命令就是wordcloud2函数，它的基本使用方式如下。\nwordcloud2(data, size = 1, minSize = 0, gridSize = 0, fontFamily = NULL, fontWeight = 'normal', color = 'random-dark', backgroundColor = \u0026quot;white\u0026quot;, minRotation = -pi/4, maxRotation = pi/4, rotateRatio = 0.4, shape = 'circle', ellipticity = 0.65, widgetsize = NULL) # 常用参数： # （1）data：词云生成数据，包含具体词语以及频率； # （2）size：字体大小，默认为1，一般来说该值越小，生成的形状轮廓越明显； # （3）fontFamily：字体，如‘微软雅黑’； # （4）fontWeight：字体粗细，包含‘normal’，‘bold’以及‘600’； # （5）color：字体颜色，可以选择‘random-dark’以及‘random-light’，其实就是颜色色系； # （6）backgroundColor：背景颜色，支持R语言中的常用颜色，如‘gray’，‘blcak’， # （7）minRontatin与maxRontatin：字体旋转角度范围的最小值以及最大值，选定后，字体会在该范围内随机旋转； # （8）rotationRation：字体旋转比例，如设定为1，则全部词语都会发生旋转； # （9）shape：词云形状选择，默认是‘circle’，即圆形。还可以选择# ‘cardioid’（苹果形或心形），‘star’（星形），‘diamond’（钻石），‘triangle-forward’（三角形），‘triangle’（三角形），‘pentagon’（五边形）；  通过对两种信息的内容处理，我们使用wordcloud2函数做出两者的词云图形。\n相信大家通过这两幅图就可以判断出左侧是来自于垃圾短信内容。比较右侧图片可以看出，左侧的高频词汇类似free和prize等很具有煽动性，这正是很多广告短信的特点。\n4 其他参数调试 当然我们可以改变函数中的一些参数，从而画出不同形式、形状的词云。例如，设置shape为star画出星形词云。\nwordcloud2(data,size = 1,shape = \u0026quot;star\u0026quot;)# 星形  我们还利用wordcloud2包中的letterCloud函数，通过设置参数word为R画出字母R形的词云图。\nletterCloud(data, word =\u0026quot;R\u0026quot;, wordSize = 2)# 字母R形  此外，wordcloud2还有一个非常好用的功能，就是可以将词云的图形画成由用户输入的一张黑色图案。例如，我们可以输入一张奔跑的马形图案，然后利用该图案绘制马形词云图。\nwordcloud2(data,size = 1,figPath = \u0026quot;fig/ma.png\u0026quot;)# 马  5 结语 本期我们主要学习了R中词云的制作方法，通过wordcloud2包实现各种词云的制作方式。其中，可以允许用户使用自定义的图案作词云图极大的丰富了词云的开发。不过，在制作词云过程中文本数据的处理始终是一个难点，这里没有详述这方面内容，需要读者自己了解这方面的知识。\n参考文献 Brett Lantz, 《机器学习与R语言》\n","id":35,"section":"posts","summary":"顾名思义，词云就是由词汇组成类似云彩的图形。对于篇幅庞大的文本而言，使用词云进行描述可以过滤掉很多冗杂而无意义的信息，诸如空格、标点符号、停","tags":["数据可视化"],"title":"R制作词云","uri":"https://qkai-stat.github.io/2018/07/wordcloud-introduction/","year":"2018"},{"content":"1 指数族分布与广义线性模型 1.1 引入指数族分布 在线性模型中，一个重要的条件便是响应变量 $y$ 须服从正态分布。然而，实际问题中 $y$ 并不总是满足正态分布的假设。因此，我们考虑更加一般的指数族分布。\n指数族分布定义如下：\n$$ f\\left( y;\\theta ,\\phi \\right) =\\exp \\left[ \\frac{y\\theta -b\\left( \\theta \\right)}{a\\left( \\phi \\right)}+c\\left( y,\\phi \\right) \\right] $$\n其中 $a(\\cdot)$，$b(\\cdot)$ 和 $c(\\cdot)$ 是已知给定的函数。参数 $\\theta$ 为分布族的位置参数（location parameter），参数 $\\phi$ 通常被称为分散参数（dispersion parameter）。一般来说，函数 $a(\\phi)$ 通常有形式 $a(\\phi)=\\phi\\cdot\\omega$，其中 $\\omega$ 是一个已知的常数。指数族分布包括常见的二项分布、泊松分布、正态分布和指数分布等。\n1.2 联系回归问题 前面直接给出指数族分布，有点让人一时难以和回归问题构建联系。我们不妨回忆线性模型的内容，此时响应变量 $y$ 满足下面的正态分布\n$$ N(x^T\\beta, \\sigma^2) $$\n我们写出它的密度函数具体表达式\n$$ f\\left( y \\right) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[ -\\frac{(y - x\\beta)^2}{2\\sigma^2} \\right] = \\exp\\left[ -\\frac{(y^2 - 2yx\\beta + \\beta^Tx^Tx\\beta)}{2\\sigma^2} - \\frac{1}{2} \\ln(2\\pi\\sigma^2) \\right] $$\n整理一下就得到\n$$ f\\left( y \\right) = \\exp\\left[ \\frac{yx\\beta - 1/2\\beta^Tx^Tx\\beta}{\\sigma^2} - \\frac{y^2}{2\\sigma^2} - \\frac{1}{2} \\ln(2\\pi\\sigma^2) \\right] $$\n原来线性回归问题就是 $\\theta = x\\beta$ 和 $\\phi = \\sigma^2$ 的指数族问题。 因此，我们保留线性模型中的线性结构假设，把正态分布约束推广成指数族分布，这就得到的更为一般的广义线性模型。\n对应线性模型的假设，广义线性模型也有若干前提假设：\n 观测 $y_1,\\cdots ,y_n$ 是相互独立的，且对应的均值为 $\\mu _1,\\mu _2,\\cdots ,\\mu _n$； 每个观测 $y_i$ 具有指数族分布； 模型建立在线性预测因子 $\\eta _1,\\cdots ,\\eta _n$ 上，其中 $\\eta _i=x _{i}^{\\mathrm{T}}\\beta $，$x_i$ 是设计矩阵第 $i$ 个行向量； 模型通过链接函数（link function）建立，其中：$\\eta _i=g\\left( \\mu _i \\right) ,\\ i=1,2,\\cdots ,n$； 链接函数是单调可微的（其反函数存在）。  不难看出待求参数 $\\beta$ 和指数族分布之间的关系链接：\n$$ \\beta \\overset{\\eta _i=g\\left( \\mu _i \\right)}{\\leftrightarrow}\\mu _i\\overset{\\mu _i=\\dot{b}\\left( \\theta _i \\right)}{\\leftrightarrow}\\theta _i $$\n上述这种关系对我们逐步得到参数 $\\beta$ 的求解公式意义重大。\n2 广义线性模型的参数估计 在线性模型的假设下，最小二乘法和极大似然法都能用于参数的求解。在广义线性模型中，我们无法写出二乘形式的优化函数。因此，我们根据分布信息利用极大似然法来估计参数。 模型的似然函数 $\\mathscr{L}(\\theta;Y)$ 为\n$$ \\mathscr{L}(\\theta;Y) = \\prod _{i=1}^n \\exp \\left[ \\frac{y_i\\theta_i -b\\left( \\theta-i \\right)}{a\\left( \\phi_i \\right)}+c\\left( y_i,\\phi_i \\right) \\right] $$\n为了方便理解算法的具体推导过程，下面首先介绍指数族的两个重要结论，然后再具体推导求解算法。\n2.1 两个重要的结论 对于指数族分布，首先讨论两个重要的结论。根据密度函数可以得到相应的对数似然函数：\n$$ \\ln \\mathscr{L}\\left( \\theta ;Y \\right) =\\sum _{i=1}^n{\\frac{y _i\\theta _i-b\\left( \\theta _i \\right)}{a\\left( \\phi _i \\right)}}+\\sum _{i=1}^n{c\\left( y _i,\\phi _i \\right)} $$\n这里仅假定 $y_i$ 是独立的。则似然函数对 $\\theta$ 求导有：\n$$ \\frac{\\partial \\ln \\mathscr{L}\\left( \\theta ;Y \\right)}{\\partial \\theta}=\\sum _{i=1}^n{\\frac{y _i-\\dot{b}\\left( \\theta _i \\right)}{a\\left( \\phi _i \\right)}}=\\sum _{i=1}^n{S _i} $$\n似然函数对 $\\theta$ 的二阶导为：\n$$ \\frac{\\partial ^2\\ln \\mathscr{L}\\left( \\theta ;Y \\right)}{\\partial \\theta ^2}=\\sum _{i=1}^n{\\frac{\\ddot{b}\\left( \\theta _i \\right)}{a\\left( \\phi _i \\right)}} $$\n其中 $\\dot{b}(\\theta)$ 和 $\\ddot{b}(\\theta)$ 是 $b(\\theta)$ 对 $\\theta$ 的一阶导和二阶导（下同）。那么，大部分指数族分布（要求密度函数积分号和求导号可以交换顺序）满足：\n$$ E\\left( \\frac{\\partial \\ln \\mathscr{L}\\left( \\theta ;Y \\right)}{\\partial \\theta} \\right) =0, \\quad E\\left( \\frac{\\partial \\ln \\mathscr{L}\\left( \\theta ;Y \\right)}{\\partial \\theta} \\right) =0 $$\n显然，根据上述两条性质可以推出：\n$$ \\mu _i=Ey_i=\\dot{b}\\left( \\theta _i \\right) $$\n和\n$$ Var\\left( y_i \\right) =\\ddot{b}\\left( \\theta _i \\right) a\\left( \\phi \\right) =\\frac{\\mathrm{d}\\mu _i}{\\mathrm{d}\\theta _i}a\\left( \\phi \\right) = Var _{\\mu _i}a\\left( \\phi \\right) $$\n2.2 极大似然估计 根据极大似然思想，对数似然函数对 $\\beta$ 求导得到：\n$$ \\begin{split} S\\left( \\beta \\right) \u0026amp;=\\frac{\\partial \\ln \\mathscr{L}\\left( \\beta ;Y \\right)}{\\partial \\beta}=\\sum _{i=1}^n{\\frac{\\partial}{\\partial \\theta _i}\\left( \\frac{y _i\\theta _i-b\\left( \\theta _i \\right)}{a\\left( \\phi _i \\right)} \\right) \\cdot \\frac{\\partial \\theta _i}{\\partial \\mu _i}\\cdot \\frac{\\partial \\mu _i}{\\partial \\beta}} \\newline \u0026amp;=\\sum _{i=1}^n{\\frac{y _i-\\dot{b}\\left( \\theta _i \\right)}{a\\left( \\phi _i \\right)}\\cdot \\frac{\\partial \\theta _i}{\\partial \\mu _i}\\cdot \\frac{\\partial \\mu _i}{\\partial \\beta}}=\\sum _{i=1}^n{\\frac{y _i-\\dot{b}\\left( \\theta _i \\right)}{a\\left( \\phi _i \\right)}\\cdot \\frac{1}{\\frac{\\partial \\mu _i}{\\partial \\theta _i}}\\cdot \\frac{\\partial \\mu _i}{\\partial \\beta}} \\newline \u0026amp;=\\sum _{i=1}^n{\\left( y _i-\\dot{b}\\left( \\theta _i \\right) \\right) \\frac{1}{a\\left( \\phi _i \\right) \\frac{\\partial \\mu _i}{\\partial \\theta _i}}\\cdot \\frac{\\partial \\mu _i}{\\partial \\beta}}=\\sum _{i=1}^n{\\left( y _i-\\mu _i \\right) \\frac{1}{Var\\left( y _i \\right)}\\cdot \\frac{\\partial \\mu _i}{\\partial \\beta}} \\newline \u0026amp;=\\left( \\frac{\\partial \\mu _1}{\\partial \\beta},\\cdots ,\\frac{\\partial \\mu _n}{\\partial \\beta} \\right) \\left( \\begin{matrix} \\frac{1}{Var\\left( y _1 \\right)}\u0026amp;\t\u0026amp;\t\\newline \u0026amp;\t\\ddots\u0026amp;\t\\newline \u0026amp;\t\u0026amp;\t\\frac{1}{Var\\left( y _n \\right)}\\newline \\end{matrix} \\right) \\left( \\begin{array}{c} y _1-\\mu _1\\newline \\newline y _n-\\mu _n\\newline \\end{array} \\right) \\newline \u0026amp;=\\frac{\\partial \\mu ^{\\mathrm{T}}}{\\partial \\beta}V^{-1}\\left( y-\\mu \\right) =\\left( \\frac{\\partial \\mu}{\\partial \\beta ^{\\mathrm{T}}} \\right) ^{\\mathrm{T}}V^{-1}\\left( y-\\mu \\right) \\newline \u0026amp;=D^{\\mathrm{T}}V^{-1}\\left( y-\\mu \\right) \\end{split} $$\n显然，这和之前求解得到的得分函数形式是一致的。为了得到具体的表达式，需要进一步求解 $D$ 的具体形式。\n因为 $D=\\frac{\\partial \\mu}{\\partial \\beta ^{\\mathrm{T}}}$，而 $x_{i}^{\\mathrm{T}}\\beta =\\eta _i=g\\left( \\mu _i \\right) $，考虑到链接函数 $g(\\cdot)$ 是单调可微函数，则其反函数存在，不妨设为 $h(\\cdot)$，所以\n$$ \\frac{\\partial \\mu _i}{\\partial \\beta}=\\frac{\\partial}{\\partial \\beta}h\\left( x _{i}^{\\mathrm{T}}\\beta \\right) =\\dot{h}\\left( x _{i}^{\\mathrm{T}}\\beta \\right) x_i $$\n进而\n$$ \\begin{split} D\u0026amp;=\\frac{\\partial \\mu}{\\partial \\beta ^{\\mathrm{T}}}=\\left( \\frac{\\partial \\mu}{\\partial \\beta _1},\\cdots ,\\frac{\\partial \\mu}{\\partial \\beta _n} \\right) =\\left( \\dot{h}\\left( x _{1}^{\\mathrm{T}}\\beta \\right) x_1,\\cdots ,\\dot{h}\\left( x _{n}^{\\mathrm{T}}\\beta \\right) x _n \\right) \\newline \u0026amp;=\\left( \\begin{matrix} \\dot{h}\\left( x _{1}^{\\mathrm{T}}\\beta \\right)\u0026amp;\t\u0026amp;\t\\newline \u0026amp;\t\\ddots\u0026amp;\t\\newline \u0026amp;\t\u0026amp;\t\\dot{h}\\left( x _{n}^{\\mathrm{T}}\\beta \\right)\\newline \\end{matrix} \\right) \\left( x _1,\\cdots ,x _n \\right) \\newline \u0026amp;=\\Delta X \\end{split} $$\n所以得到 $S(\\beta)$ 的最终表达式为\n$$ S\\left( \\beta \\right) =\\left( \\Delta X \\right) ^{\\mathrm{T}}V^{-1}\\left( y-\\mu \\right) =X^{\\mathrm{T}}\\Delta V^{-1}\\left( y-\\mu \\right) $$\n为了得到极大似然估计，我们接着求解对应的Fisher信息矩阵。根据定义有：\n$$ I\\left( \\beta \\right) =E\\left( \\frac{\\partial}{\\partial \\beta ^{\\mathrm{T}}}S\\left( \\beta \\right) \\right) =E\\left( \\frac{\\partial ^2\\ln \\mathscr{L}}{\\partial \\beta ^{\\mathrm{T}}\\partial \\beta} \\right) $$\n根据指数族的性质有\n$$ E\\left( \\frac{\\partial ^2\\ln \\mathscr{L}}{\\partial \\beta ^{\\mathrm{T}}\\partial \\beta} \\right) =-E\\left( SS^{\\mathrm{T}} \\right) $$\n考虑其第 $(i,j)$ 个元素 $S_{ij}=S_iS_j$，则\n$$ \\begin{split} E\\left( S_iS_j \\right) \u0026amp;=E\\left[ x _{i}^{\\mathrm{T}}\\Delta V^{-1}\\left( y-\\mu \\right) x _{j}^{\\mathrm{T}}\\Delta V^{-1}\\left( y-\\mu \\right) \\right] \\newline \u0026amp;=E\\left[ x _{i}^{\\mathrm{T}}\\Delta V^{-1}\\left( y-\\mu \\right) \\left( y-\\mu \\right) ^{\\mathrm{T}}V^{-1}\\Delta x _j \\right] \\newline \u0026amp;=x _{i}^{\\mathrm{T}}\\Delta V^{-1}\\Delta x _j \\end{split} $$\n所以\n$$ I\\left( \\beta \\right) =X^{\\mathrm{T}}\\Delta V^{-1}\\Delta X $$\n这时，再回到头考虑得分函数，根据极值的必要条件有\n$$ S\\left( \\hat{\\beta} \\right) =X^{\\mathrm{T}}\\Delta V^{-1}\\left( y-\\mu \\right) =0 $$\n将上式在真值 $\\beta$ 处进行Taylor展开\n$$ S\\left( \\hat{\\beta} \\right) =S\\left( \\beta \\right) +\\frac{\\partial}{\\partial \\beta ^{\\mathrm{T}}}S\\left( \\beta \\right) \\left( \\hat{\\beta}-\\beta \\right) =0 $$\n所以\n$$ \\hat{\\beta}=\\beta +\\left( -\\frac{\\partial S\\left( \\beta \\right)}{\\partial \\beta ^{\\mathrm{T}}} \\right) ^{-1}S\\left( \\beta \\right) $$\n其中括号内的一项过于复杂，在实际求解中可以根据大数定律用其期望代替，由前面的讨论可知其期望即为 $I(\\beta)$。这样，模型的极大似然估计可以近似成\n$$ \\begin{split} \\hat{\\beta}\u0026amp;\\doteq \\beta +\\left( X^{\\mathrm{T}}\\Delta V^{-1}\\Delta X \\right) ^{-1}X^{\\mathrm{T}}\\Delta V^{-1}\\left( y-\\mu \\right) \\newline \u0026amp;=\\beta +I\\left( \\beta \\right) ^{-1}D^{\\mathrm{T}}V^{-1}\\left( y-\\mu \\right) \\end{split} $$\n2.3 拟似然方法 广义线性模型中对于分布有前提假设，但是实际问题中并不知道具体的分布是什么。考虑到在求解问题中一个重要的信息就是信息函数，它和分布前两阶矩有关，故而可以假设分布的前两阶矩存在，从而推出不含分布信息的拟似然方法。\n假定响应变量 $y_i$ 的均值为 $\\mu_i$，方差函数为 $\\mathrm{Var}y_i=a\\left( \\phi \\right) \\mathrm{Var} _{\\mu _i}$，并且假定均值 $\\mu_i$ 和 $x_i^T\\beta$ 之间存在链结函数，链结函数的性质和广义线性模型的链结函数性质一样。据此，根据广义线性模型的思想实施拟似然方法。\n构造逆得分函数\n$$ S_i\\left( \\mu _i \\right) =\\frac{y_i-\\mu _i}{\\mathrm{Var}y_i} $$\n它满足\n$$ \\begin{cases} ES_i\\left( \\mu _i \\right) =0\u0026amp;\t\\newline ES _{i}^{2}=E\\left( -\\frac{\\partial S_i}{\\partial \\mu _i} \\right)\u0026amp;\t\\newline \\end{cases} $$\n其中第二条性质是因为\n$$ \\begin{split} E\\left( -\\frac{\\partial S_i}{\\partial \\mu _i} \\right) \u0026amp;=-E\\left( \\frac{\\partial}{\\partial \\mu _i}\\left( \\frac{y_i-\\mu _i}{\\mathrm{Var}y_i} \\right) \\right) \\newline \u0026amp;=\\frac{1}{\\mathrm{Var}y_i}=E\\left( S _{i}^{2} \\right) \\end{split} $$\n所以根据广义线性模型极大似然思想有\n$$ \\theta _i=\\int _{y_i}^{\\mu _i}{S_i\\left( t \\right) \\mathrm{d}t}, \\quad \\theta \\left( \\mu \\right) =\\sum _{i=1}^n{\\theta \\left( \\mu _i \\right)}=\\theta \\left( \\beta \\right) $$\n那么拟似然方法的得分函数可以写成\n$$ S\\left( \\beta \\right) =\\frac{\\partial}{\\partial \\beta}\\theta \\left( \\beta \\right) =\\sum _{i=1}^n{\\frac{\\partial \\theta _i}{\\partial \\beta}=\\sum _{i=1}^n{\\frac{\\partial \\mu _{i}^{\\mathrm{T}}}{\\partial \\beta}\\frac{\\partial \\theta _i}{\\partial \\mu _i}=D^{\\mathrm{T}}V^{-1}\\left( y-\\mu \\right)}} $$\n类似的可以得到Fisher信息矩阵为\n$$ I\\left( \\beta \\right) =D^{\\mathrm{T}}V^{-1}D $$\n3 迭代求解算法 考虑模型的极大似然估计，其中括号内的一项过于复杂，在实际求解中可以根据大数定律用其期望代替，由前面的讨论可知其期望即为 $I(\\beta)$。这样，模型的极大似然估计可以近似成\n$$ \\begin{split} \\hat{\\beta}\u0026amp;\\doteq \\beta +\\left( X^{\\mathrm{T}}\\Delta V^{-1}\\Delta X \\right) ^{-1}X^{\\mathrm{T}}\\Delta V^{-1}\\left( y-\\mu \\right) \\newline \u0026amp;=\\beta +I\\left( \\beta \\right) ^{-1}D^{\\mathrm{T}}V^{-1}\\left( y-\\mu \\right) \\end{split} $$\n这样就得到了估计量的求解迭代公式。\n4 模拟分析 在R中，我们有glm函数求解广义线性模型。这里，我结合前面的分析自己编写的相应的参数估计和假设检验的函数，包括二项分布（逻辑回归）、Poisson分布。这些代码附在最后供参考。\n以Poisson分布举例，考虑符合Poisson分布的观测，其中链接函数为\n$$ \\eta _i=\\ln \\mu _i $$\n我们生成 $\\mathbb{R}^{1000\\times 5}$ 的设计矩阵 $X$，系数 $\\beta =\\left( 1,3,2,4,5 \\right) ^{\\mathrm{T}}$，使用自编的广义线性模型求解算法求解该模型得到\n这里的myglm是我自编的广义线性模型求解函数，其中囊括了线性模型、二项分布模型（逻辑回归）和Poisson模型的参数估计和显著性检验。对比一下R中的glm包求解的结果\n可以看到结果几乎是一样的。\n5 自编函数代码 5.1 模型求解 实际使用，主要运行该代码即可。这个代码整合了参数估计、检验等代码，将结果合并输出。\n## This program is to solve the generalized linear models: logistic, poisson myglm \u0026lt;- function(x,y,b0,alpha = 0.05,family = \u0026quot;poisson\u0026quot;){# options(digits = 4) # family has two choice: logistic and poisson # alpha is the significance of the interval estimate of the coefficients # solve the model if(family == \u0026quot;logistic\u0026quot;){ glm.result \u0026lt;- myglmlogistic(x,y,beta1 = b0) } else if(family == \u0026quot;poisson\u0026quot;){ glm.result \u0026lt;- myglmpoisson(x,y,beta1 = b0) } else{ stop(cat(\u0026quot;!!!!!!模型参数错误，请选择给定模型logistic,poisson之一\u0026quot;)) } b \u0026lt;- glm.result$模型的解 y.fit \u0026lt;- glm.result$拟合值 num \u0026lt;- glm.result$算法迭代次数 # hypothesis testing indivi \u0026lt;- myglmindivi(x,y,b,expr = family) # confidence interval inter.sig \u0026lt;- myglminterval(x,y,b,alpha = alpha,expr = family) bname \u0026lt;- paste0(rep(\u0026quot;beta\u0026quot;,length(b)),1:length(b)) # the residuls # glm.residual \u0026lt;- y - y.fit # res1 \u0026lt;- summary(glm.residual) # names(res1) \u0026lt;- c(\u0026quot;最小值\u0026quot;,\u0026quot;下四分位数\u0026quot;,\u0026quot;中位数\u0026quot;,\u0026quot;均值\u0026quot;,\u0026quot;上四分位数\u0026quot;,\u0026quot;最大值\u0026quot;) estimate.test \u0026lt;- data.frame( \u0026quot;估计值\u0026quot; = b, \u0026quot;下界\u0026quot; = inter.sig[,2], \u0026quot;上界\u0026quot; = inter.sig[,3], \u0026quot;z值\u0026quot; = indivi$Tu, \u0026quot;P值(\u0026gt;|z|)\u0026quot; = indivi$Tp, \u0026quot;置信度\u0026quot; = indivi$Ts,check.names = F ) # output the results cat(\u0026quot;\\n\u0026quot;) cat(\u0026quot;Call: 这是不带截距项的\u0026quot;,family,\u0026quot;模型,\u0026quot;,\u0026quot;下面是模型的分析结果：\u0026quot;,seq = \u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) cat(\u0026quot;\\n\u0026quot;) cat(\u0026quot;参数估计结果:\u0026quot;,\u0026quot;\\n\u0026quot;) cat(\u0026quot;\\n\u0026quot;) print(estimate.test) cat(\u0026quot;---\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) cat(\u0026quot;置信度: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) cat(\u0026quot;区间估计的置信水平为：\u0026quot;,alpha,\u0026quot;\\n\u0026quot;) cat(\u0026quot;---\u0026quot;,\u0026quot;\\n\u0026quot;) cat(\u0026quot;Fisher信息矩阵的迭代次数为:\u0026quot;,num,\u0026quot;次\u0026quot;,\u0026quot;\\n\u0026quot;) }  5.2 Logistic模型的解 ##本程序用来求解广义线性模型中——logistic模型的解 myglmlogistic \u0026lt;- function(x,y,beta1,N = 5000,e = 1e-10){# # x is the design matrix # b is the starting value of the iteration # N is the upper bound of the times of the iteration # e is the convergence criteria n \u0026lt;- length(x) x \u0026lt;- as.matrix(x) y \u0026lt;- as.matrix(y) g \u0026lt;- expression(log(mu/(1 - mu)))# the link function h \u0026lt;- expression(1/(1 + exp(-eta)))# the inverse of the link funtion dh \u0026lt;- D(h,\u0026quot;eta\u0026quot;)# the first derivative funtion of h b \u0026lt;- expression(log(1 + exp(theta)))# the b(theta) function of the pdf db \u0026lt;- D(b,\u0026quot;theta\u0026quot;)# the first derivative funtion of b ddb \u0026lt;- D(db,\u0026quot;theta\u0026quot;)# the second derivative funtion of b db.inverse \u0026lt;- expression(log(mu/(1 - mu)))# the inverse of db k \u0026lt;- 1 beta0 \u0026lt;- beta1 + 1 while(sum((beta0 - beta1)^2) \u0026gt;= e){ beta0 \u0026lt;- beta1 # compute the D matrix eta \u0026lt;- x%*%beta1 delta \u0026lt;- diag(as.vector(eval(dh))) D \u0026lt;- delta%*%x # compute v.inverse—the inverse matrix of the covariance matrix of y mu \u0026lt;- eval(h) theta \u0026lt;- eval(db.inverse) v.inverse \u0026lt;- diag(1/as.vector(eval(ddb))) p1 \u0026lt;- solve(t(D)%*%v.inverse%*%D) p2 \u0026lt;- t(D)%*%v.inverse%*%delta p3 \u0026lt;- x%*%beta1 + v.inverse%*%(y - mu) beta1 \u0026lt;- p1%*%p2%*%p3# the kth estimates # check if it is divergent if(k \u0026gt; N){ cat(\u0026quot;算法不收敛，已达到最大迭代次数：\u0026quot;,N,\u0026quot;\\n\u0026quot;) cat(\u0026quot;此时得到的解为：\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) print(beta1) break } else{ k \u0026lt;- k + 1 } } colnames(beta1) \u0026lt;- c(\u0026quot;估计值\u0026quot;) rownames(beta1) \u0026lt;- paste0(rep(\u0026quot;系数\u0026quot;,length(beta1)),1:length(beta1)) eta \u0026lt;- x%*%beta1 y.fit \u0026lt;- eval(h)# compute the fitting values of y glmlogistic.result \u0026lt;- list(\u0026quot;模型的解\u0026quot; = beta1,\u0026quot;算法迭代次数\u0026quot; = k-1,\u0026quot;拟合值\u0026quot; = y.fit) return(glmlogistic.result) }  5.3 Poisson模型的解 ##本程序用来求解广义线性模型中——Poisson模型的解 myglmpoisson \u0026lt;- function(x,y,beta1,N = 5000,e = 1e-10){# # x is the design matrix # b is the starting value of the iteration # N is the upper bound of the times of the iteration # e is the convergence criteria n \u0026lt;- length(x) x \u0026lt;- as.matrix(x) y \u0026lt;- as.matrix(y) g \u0026lt;- expression(log(mu))# the link function h \u0026lt;- expression(exp(eta))# the inverse of the link funtion dh \u0026lt;- D(h,\u0026quot;eta\u0026quot;)# the first derivative funtion of h b \u0026lt;- expression(exp(theta))# the b(theta) function of the pdf db \u0026lt;- expression(exp(theta))# the first derivative funtion of b ddb \u0026lt;- expression(exp(theta))# the second derivative funtion of b db.inverse \u0026lt;- expression(log(mu))# the inverse of db k \u0026lt;- 1 beta0 \u0026lt;- beta1 + 1 while(sum((beta0 - beta1)^2) \u0026gt;= e){ beta0 \u0026lt;- beta1 # compute the D matrix eta \u0026lt;- x%*%beta1 delta \u0026lt;- diag(as.vector(eval(dh))) D \u0026lt;- delta%*%x # compute v.inverse—the inverse matrix of the covariance matrix of y mu \u0026lt;- eval(h) theta \u0026lt;- eval(db.inverse) v.inverse \u0026lt;- diag(1/as.vector(eval(ddb))) p1 \u0026lt;- solve(t(D)%*%v.inverse%*%D) p2 \u0026lt;- t(D)%*%v.inverse%*%delta p3 \u0026lt;- x%*%beta1 + v.inverse%*%(y - mu) beta1 \u0026lt;- p1%*%p2%*%p3# the kth estimates # check if it is divergent if(k \u0026gt; N){ cat(\u0026quot;算法不收敛，已达到最大迭代次数：\u0026quot;,N,\u0026quot;\\n\u0026quot;) cat(\u0026quot;此时得到的解为：\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) print(beta1) break } else{ k \u0026lt;- k + 1 } } colnames(beta1) \u0026lt;- c(\u0026quot;估计值\u0026quot;) rownames(beta1) \u0026lt;- paste0(rep(\u0026quot;系数\u0026quot;,length(beta1)),1:length(beta1)) eta \u0026lt;- x%*%beta1 y.fit \u0026lt;- eval(h)# compute the fitting values of y glmpoisson.result \u0026lt;- list(\u0026quot;模型的解\u0026quot; = beta1,\u0026quot;算法迭代次数\u0026quot; = k-1,\u0026quot;拟合值\u0026quot; = y.fit) return(glmpoisson.result) }  5.4 拟似然 ## ##This program is quasi-likelihood method ## myquasimle \u0026lt;- function(x,y,beta1,family,N = 5000,e = 1e-10){# # x is the design matrix # b is the starting value of the iteration # N is the upper bound of the times of the iteration # e is the convergence criteria n \u0026lt;- length(x) x \u0026lt;- as.matrix(x) y \u0026lt;- as.matrix(y) v.mu \u0026lt;- expression(mu^2)# The covariance function of y if(family == \u0026quot;possion\u0026quot;){ h \u0026lt;- expression(exp(eta))# the inverse of the link funtion dh \u0026lt;- D(h,\u0026quot;eta\u0026quot;)# the first derivative funtion of h } else if(family == \u0026quot;logistic\u0026quot;){ h \u0026lt;- expression(1/(1 + exp(-eta))) dh \u0026lt;- D(h,\u0026quot;eta\u0026quot;) } else{ print(\u0026quot;分布参数错误, 请选择logistic或者possion分布！！！！\u0026quot;) } k \u0026lt;- 1 beta0 \u0026lt;- beta1 + 1 while(sum((beta0 - beta1)^2) \u0026gt;= e){ beta0 \u0026lt;- beta1 # compute the D matrix eta \u0026lt;- x%*%beta1 delta \u0026lt;- diag(as.vector(eval(dh))) D \u0026lt;- delta%*%x # compute v.inverse—the inverse matrix of the covariance matrix of y mu \u0026lt;- eval(h) v.inverse \u0026lt;- diag(1/as.vector(eval(v.mu))) p1 \u0026lt;- solve(t(D)%*%v.inverse%*%D) p2 \u0026lt;- t(D)%*%v.inverse%*%delta p3 \u0026lt;- x%*%beta1 + v.inverse%*%(y - mu) beta1 \u0026lt;- p1%*%p2%*%p3# the kth estimates # check if it is divergent if(k \u0026gt; N){ cat(\u0026quot;算法不收敛，已达到最大迭代次数：\u0026quot;,N,\u0026quot;\\n\u0026quot;) cat(\u0026quot;此时得到的解为：\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) print(beta1) break } else{ k \u0026lt;- k + 1 } } colnames(beta1) \u0026lt;- c(\u0026quot;估计值\u0026quot;) rownames(beta1) \u0026lt;- paste0(rep(\u0026quot;系数\u0026quot;,length(beta1)),1:length(beta1)) eta \u0026lt;- x%*%beta1 y.fit \u0026lt;- eval(h)# compute the fitting values of y quasi.result \u0026lt;- list(\u0026quot;模型的解\u0026quot; = beta1,\u0026quot;算法迭代次数\u0026quot; = k-1,\u0026quot;拟合值\u0026quot; = y.fit) return(quasi.result) }  5.5 单个系数检验 ## 求解广义线性模型单个系数检验 myglmindivi \u0026lt;- function(x,y,b,expr){# # expr 表示模型：logistic, poisson # b 表示由广义线性模型估计得到的估计量 n \u0026lt;- length(x) p \u0026lt;- length(b) beta.hat \u0026lt;- b D \u0026lt;- matrix(0, nrow = n, ncol = p) # get the D matrix if(expr == \u0026quot;logistic\u0026quot;){ h \u0026lt;- expression(1/(1 + exp(-eta)))# the inverse of the link funtion dh \u0026lt;- D(h,\u0026quot;eta\u0026quot;)# the first derivative funtion of h b \u0026lt;- expression(log(1 + exp(theta)))# the b(theta) function of the pdf db \u0026lt;- D(b,\u0026quot;theta\u0026quot;)# the first derivative funtion of b ddb \u0026lt;- D(db,\u0026quot;theta\u0026quot;)# the second derivative funtion of b db.inverse \u0026lt;- expression(log(mu/(1 - mu)))# the inverse of db eta \u0026lt;- x%*%beta.hat delta \u0026lt;- diag(as.vector(eval(dh))) D \u0026lt;- delta%*%x# the D matrix mu \u0026lt;- eval(h) theta \u0026lt;- eval(db.inverse) v.inverse \u0026lt;- diag(1/as.vector(eval(ddb)))# the inverse of the covariance matrix of y } else if(expr == \u0026quot;poisson\u0026quot;){ h \u0026lt;- expression(exp(eta))# the inverse of the link funtion dh \u0026lt;- D(h,\u0026quot;eta\u0026quot;)# the first derivative funtion of h db \u0026lt;- expression(exp(theta))# the first derivative funtion of b ddb \u0026lt;- expression(exp(theta))# the second derivative funtion of b db.inverse \u0026lt;- expression(log(mu))# the inverse of db eta \u0026lt;- x%*%beta.hat delta \u0026lt;- diag(as.vector(eval(dh))) D \u0026lt;- delta%*%x# the D matrix mu \u0026lt;- eval(h) theta \u0026lt;- eval(db.inverse) v.inverse \u0026lt;- diag(1/as.vector(eval(ddb)))# the inverse of the covariance matrix of y } else{ stop(cat(\u0026quot;!!!!!!模型参数错误，请选择给定模型logistic,poisson之一\u0026quot;)) } cc \u0026lt;- solve(t(D)%*%v.inverse%*%D) diagc \u0026lt;- diag(cc) Tt \u0026lt;- 1:p;Tp \u0026lt;- Tt;Ts \u0026lt;- Tt for(i in 1:p){ Tt[i] \u0026lt;- beta.hat[i]/sqrt(diagc[i]) Tp[i] \u0026lt;- 2*(1 - pnorm(Tt[i])) if(Tp[i] \u0026lt; 0.001 ){# 判断置信度 Ts[i] \u0026lt;- c(\u0026quot;***\u0026quot;) } else if(Tp[i] \u0026lt; 0.01 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.001){ Ts[i] \u0026lt;- c(\u0026quot;**\u0026quot;) } else if(Tp[i] \u0026lt; 0.05 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.01){ Ts[i] \u0026lt;- c(\u0026quot;*\u0026quot;) } else if(Tp[i] \u0026lt; 0.1 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.05){ Ts[i] \u0026lt;- c(\u0026quot;.\u0026quot;) } else{ Ts[i] \u0026lt;- c(\u0026quot; \u0026quot;) } } indivitest \u0026lt;- data.frame(\u0026quot;Tu\u0026quot; = Tt,\u0026quot;Tp\u0026quot; = Tp,\u0026quot;Ts\u0026quot; = Ts) return(indivitest) }  5.6 区间估计 ## the confidence interval of the generalized linear model myglminterval \u0026lt;- function(x,y,b,expr,alpha = 0.05){ n \u0026lt;- length(x) p \u0026lt;- length(b) beta.hat \u0026lt;- b D \u0026lt;- matrix(0, nrow = n, ncol = p) # get the D matrix if(expr == \u0026quot;logistic\u0026quot;){ h \u0026lt;- expression(1/(1 + exp(-eta)))# the inverse of the link funtion dh \u0026lt;- D(h,\u0026quot;eta\u0026quot;)# the first derivative funtion of h b \u0026lt;- expression(log(1 + exp(theta)))# the b(theta) function of the pdf db \u0026lt;- D(b,\u0026quot;theta\u0026quot;)# the first derivative funtion of b ddb \u0026lt;- D(db,\u0026quot;theta\u0026quot;)# the second derivative funtion of b db.inverse \u0026lt;- expression(log(mu/(1 - mu)))# the inverse of db eta \u0026lt;- x%*%beta.hat delta \u0026lt;- diag(as.vector(eval(dh))) D \u0026lt;- delta%*%x# the D matrix mu \u0026lt;- eval(h) theta \u0026lt;- eval(db.inverse) v.inverse \u0026lt;- diag(1/as.vector(eval(ddb)))# the inverse of the covariance matrix of y } else if(expr == \u0026quot;poisson\u0026quot;){ h \u0026lt;- expression(exp(eta))# the inverse of the link funtion dh \u0026lt;- D(h,\u0026quot;eta\u0026quot;)# the first derivative funtion of h db \u0026lt;- expression(exp(theta))# the first derivative funtion of b ddb \u0026lt;- expression(exp(theta))# the second derivative funtion of b db.inverse \u0026lt;- expression(log(mu))# the inverse of db eta \u0026lt;- x%*%beta.hat delta \u0026lt;- diag(as.vector(eval(dh))) D \u0026lt;- delta%*%x# the D matrix mu \u0026lt;- eval(h) theta \u0026lt;- eval(db.inverse) v.inverse \u0026lt;- diag(1/as.vector(eval(ddb)))# the inverse of the covariance matrix of y } else{ stop(cat(\u0026quot;!!!!!!模型参数错误，请选择给定模型logistic, gompertz, weibull之一\u0026quot;)) } Ti \u0026lt;- matrix(0,nrow = p,ncol = 2) cc \u0026lt;- solve(t(D)%*%v.inverse%*%D) diagc \u0026lt;- diag(cc) for(i in 1:p){ Ti[i,1] \u0026lt;- beta.hat[i] - qnorm((1-alpha/2))*sqrt(diagc[1]) Ti[i,2] \u0026lt;- beta.hat[i] + qnorm((1-alpha/2))*sqrt(diagc[1]) } out \u0026lt;- cbind(beta.hat,Ti) colnames(out) \u0026lt;- c(\u0026quot;Estimator\u0026quot;,\u0026quot;LowerBound\u0026quot;,\u0026quot;UpperBound\u0026quot;) return(out) }  ","id":36,"section":"posts","summary":"1 指数族分布与广义线性模型 1.1 引入指数族分布 在线性模型中，一个重要的条件便是响应变量 $y$ 须服从正态分布。然而，实际问题中 $y$ 并不总是满足正态分布的","tags":["多元统计","回归分析"],"title":"广义线性模型及其一般求解方式","uri":"https://qkai-stat.github.io/2018/06/glm/","year":"2018"},{"content":"这个笔记总结了非线性模型的极大似然估计量和渐进性质，并推导了用于求解模型的Gauss—Newton迭代法。此外，针对每个内容，我还给出了相应的R软件求解算法，并做了相应的模拟。\n1 非线性模型 线性模型建立在自变量和响应变量之间呈线性关系的基础上，但实际数据并不总是如此。当我们没有额外信息认为两者之间的关系为线性时，非线性模型便成了一种选择。\n考虑非线性模型\n$$ y=f\\left( X,\\beta \\right) +\\epsilon $$\n其中 $y\\in \\mathbb{R}^n$，$X\\in \\mathbb{R}^{n\\times p}$，$n$ 表示观测数，$p$ 表示变量数。$\\beta \\in \\mathbb{R}^n$ 表示回归系数。$\\epsilon$ 一般假设成为独立同分布的 $N\\left( 0,\\sigma ^2I_n \\right) $ 高斯随机变量。\n2 模型的极大似然估计 如果我们假设模型的随机误差项是独立同分于均值为0方差为常数 $\\sigma^2$ 的正态分布，那么可以考虑极大似然估计法来估计模型的解。\n首先，根据模型和假设得到似然函数\n$$ \\mathscr{L}\\left( \\beta ,\\sigma ^2 \\right) =\\frac{1}{\\left( 2\\pi \\sigma ^2 \\right) ^{n/2}}\\exp \\left[ -\\frac{1}{2\\sigma ^2}\\left( y-f\\left( X,\\beta \\right) \\right) ^{\\mathrm{T}}\\left( y-f\\left( X,\\beta \\right) \\right) \\right] $$\n对上式取对数，得到对数似然函数\n$$ \\ln \\mathscr{L}\\left( \\beta ,\\sigma ^2 \\right) =-\\frac{n}{2}\\ln \\left( 2\\pi \\sigma ^2 \\right) -\\frac{1}{2\\sigma ^2}\\left( y-f\\left( X,\\beta \\right) \\right) ^{\\mathrm{T}}\\left( y-f\\left( X,\\beta \\right) \\right) $$\n将对数似然方程对 $\\beta$ 进行求偏导，根据极值必要条件令为零，从而得到得分方程（score equation）\n$$ \\begin{split} \\frac{\\partial \\ln \\mathscr{L}\\left( \\beta ,\\sigma ^2 \\right)}{\\partial \\beta}\u0026amp;=\\frac{\\partial}{\\partial \\beta}\\left[ -\\frac{1}{2\\sigma ^2}\\left( y-f\\left( X,\\beta \\right) \\right) ^{\\mathrm{T}}\\left( y-f\\left( X,\\beta \\right) \\right) \\right] \\newline \u0026amp;=-\\frac{1}{2\\sigma ^2}\\frac{\\partial \\left( y-f\\left( X,\\beta \\right) \\right) ^{\\mathrm{T}}}{\\partial \\beta}\\left( y-f\\left( X,\\beta \\right) \\right) \\newline \u0026amp;=-\\frac{1}{\\sigma ^2}\\frac{\\partial f\\left( X,\\beta \\right) ^{\\mathrm{T}}}{\\partial \\beta}\\left( y-f\\left( X,\\beta \\right) \\right) \\newline \u0026amp;=-\\frac{1}{\\sigma ^2}\\left( \\frac{\\partial f\\left( X,\\beta \\right)}{\\partial \\beta ^{\\mathrm{T}}} \\right) ^{\\mathrm{T}}\\left( y-f\\left( X,\\beta \\right) \\right) \\newline \u0026amp;=0 \\end{split} $$\n上式利用了矩阵微分中的结论\n$$ \\frac{\\mathrm{d}f\\left( Y\\left( X \\right) \\right)}{\\mathrm{d}X}=\\frac{\\mathrm{d}Y^{\\mathrm{T}}}{\\mathrm{d}X}\\cdot \\frac{\\mathrm{d}f\\left( Y \\right)}{\\mathrm{d}Y} $$\n其中 $X\\in \\mathbb{R}^{n\\times 1}$，$Y\\in \\mathbb{R}^{n\\times 1}$，$f\\in \\mathbb{R}$ 的结论。进一步，上式等价于\n$$ \\left( \\frac{\\partial f\\left( X,\\beta \\right)}{\\partial \\beta ^{\\mathrm{T}}} \\right) ^{\\mathrm{T}}\\left( y-f\\left( X,\\beta \\right) \\right) =0 $$\n由于 $f(X, \\beta)$ 表示均值函数，将其记作 $\\mu$，并用 $\\hat{\\mu}$ 表示 $f(X,\\beta)$ 中 $\\beta$ 被其估计量 $b$ 替换的结果。那么根据上式可以推导出模型的估计量满足\n$$ \\left( \\frac{\\partial \\hat{\\mu}}{\\partial \\beta ^{\\mathrm{T}}} \\right) ^{\\mathrm{T}}\\left( y-\\hat{\\mu} \\right) =0 $$\n这时我们记 $\\frac{\\partial \\hat{\\mu}}{\\partial \\beta ^{\\mathrm{T}}}=D\\in \\mathbb{R}^{n\\times p}$，其中 $D_{ij}=\\frac{\\partial f\\left( X_i,\\beta \\right)}{\\partial \\beta _j}$，那么就有\n$$ D^{\\mathrm{T}}\\left( y-\\hat{\\mu} \\right) =0 $$\n 这里可以看出极大似然估计量和最小二乘估计量的形式是一致的。 之所以使用似然法，是因为在线性模型、广义线性模型中似然法更普适。\n 3 迭代求解算法 在非线性回归模型中，求解最小二乘估计量（极大似然估计量）的一个广泛应用的方法是将期望函数线性化，然后利用Gauss—Newton迭代法进行求解。\n考虑非线性模型，将其在 点处进行Taylor展开\n$$ y=f\\left( X,\\beta \\right) +\\epsilon =f\\left( X,b_0 \\right) +\\frac{\\partial f\\left( X,\\beta \\right)}{\\partial \\beta ^{\\mathrm{T}}}\\left( \\beta -b_0 \\right) +\\epsilon $$\n令 $y_0=y-f\\left( X,b_0 \\right) =y-f_0$，$\\theta _0=\\beta -b_0$，$D_0=\\left[ \\frac{\\partial f\\left( X,\\beta \\right)}{\\partial \\beta ^{\\mathrm{T}}} \\right] _{\\beta =b_0}$，则上式可以写成\n$$ y_0=D_0\\theta _0+\\epsilon $$\n那么 $\\theta_0$ 的最小二乘估计量为\n$$ \\begin{split} \\hat{\\theta}_0 \u0026amp;=\\left( D _{0}^{\\mathrm{T}}D_0 \\right) ^{-1}D _{0}^{\\mathrm{T}}y_0 \\newline \u0026amp;=\\left( D _{0}^{\\mathrm{T}}D_0 \\right) ^{-1}D _{0}^{\\mathrm{T}}\\left( y-f_0 \\right) \\end{split} $$\n因为 $\\theta _0=\\beta -b_0$，我们用\n$$ b_1=b_0+\\hat{\\theta}_0 $$\n作为未知参数 $\\beta$ 的一个修正估计。那么，我们就按照这个逻辑不停得修正 $\\beta$，也就是说有\n$$ \\begin{split} b _{k+1}\u0026amp;=b _k+\\hat{\\theta} _k \\newline \u0026amp;=b _k+\\left( D _{k}^{\\mathrm{T}}D _k \\right) ^{-1}D _{k}^{\\mathrm{T}}\\left( y-f _k \\right) \\end{split} $$\n当这种修正直到收敛（前后两个估计的改变量非常小）时，即\n$$ \\frac{\\lVert b_{k+1}-b_k \\rVert}{\\lVert b_k \\rVert}\u0026lt;\\delta $$\n迭代结束，其中 $\\delta$ 是某个很小的数，比如说 $10^{-6}$。\n4 估计量的渐近正态性 根据前面分析可知 $\\beta$ 估计量为\n$$ \\hat{\\beta}\\doteq \\beta +\\left( D^{\\mathrm{T}}D \\right) ^{-1}D^{\\mathrm{T}}\\left( y-f \\right) $$\n其中 $D=\\frac{\\partial \\mu}{\\partial \\beta ^{\\mathrm{T}}}=\\frac{\\partial f\\left( X,\\beta \\right)}{\\partial \\beta ^{\\mathrm{T}}}$。对上式进行分析，等式右侧的随机项只有 $y$ 一项，这时我们考察有\n$$ \\begin{split} E\\hat{\\beta}\u0026amp;=\\beta +E\\left[ \\left( D^{\\mathrm{T}}D \\right) ^{-1}D^{\\mathrm{T}}\\left( y-f \\right) \\right] \\newline \u0026amp;=\\beta +\\left( D^{\\mathrm{T}}D \\right) ^{-1}D^{\\mathrm{T}}E\\left( y-f \\right) \\newline \u0026amp;=\\beta +\\left( D^{\\mathrm{T}}D \\right) ^{-1}D^{\\mathrm{T}}\\left( Ey-f \\right) \\newline \u0026amp;=\\beta \\end{split} $$\n和\n$$ \\begin{split} Var\\left( \\hat{\\beta} \\right) \u0026amp;=Var\\left[ \\beta +\\left( D^{\\mathrm{T}}D \\right) ^{-1}D^{\\mathrm{T}}\\left( y-f \\right) \\right] \\newline \u0026amp;=Var\\left[ \\left( D^{\\mathrm{T}}D \\right) ^{-1}D^{\\mathrm{T}}\\left( y-f \\right) \\right] \\newline \u0026amp;=\\left( D^{\\mathrm{T}}D \\right) ^{-1}D^{\\mathrm{T}}Var\\left( y-f \\right) D\\left( D^{\\mathrm{T}}D \\right) ^{-1} \\newline \u0026amp;=\\sigma ^2\\left( D^{\\mathrm{T}}D \\right) ^{-1} \\end{split} $$\n进一步，上式求出了得分函数\n$$ S\\left( \\beta \\right) =\\frac{1}{\\sigma ^2}D^{\\mathrm{T}}\\left( y-\\mu \\right) $$\n这是一个 $\\mathbb{R}^{p\\times 1}$ 中的列向量，并且\n$$ ES=\\frac{1}{\\sigma ^2}ED^{\\mathrm{T}}\\left( y-\\mu \\right) =D^{\\mathrm{T}}\\left( Ey-\\mu \\right) =0 $$\n所以\n$$ \\begin{split} Cov\\left( S \\right) \u0026amp;=\\frac{1}{\\sigma ^4}E\\left( S-ES \\right) \\left( S-ES \\right) ^{\\mathrm{T}}=\\frac{1}{\\sigma ^4}ESS^{\\mathrm{T}} \\newline \u0026amp;=\\frac{1}{\\sigma ^4}E\\left[ D^{\\mathrm{T}}\\left( y-\\mu \\right) \\left( y-\\mu \\right) ^{\\mathrm{T}}D \\right] \\newline \u0026amp;=\\frac{1}{\\sigma ^4}D^{\\mathrm{T}}E\\left[ \\left( y-\\mu \\right) \\left( y-\\mu \\right) ^{\\mathrm{T}} \\right] D \\newline \u0026amp;=\\frac{1}{\\sigma ^4}D^{\\mathrm{T}}Var\\left( y \\right) D=\\frac{1}{\\sigma ^2}D^{\\mathrm{T}}D \\end{split} $$\n上式得到的协方差矩阵称之为Fisher信息矩阵，记做 $I(\\beta)$。所以，$\\hat{\\beta}$ 的协方差矩阵也可以写成\n$$ Cov\\left( \\beta \\right) =I^{-1}\\left( \\beta \\right) $$\n重新考察 $\\hat{\\beta}$ 的近似表达式，由于 $\\hat{\\beta}$ 是近似关于 $y$ 的一个线性组合，而 $y$ 是服从正态分布的，那么 $\\hat{\\beta}$ 也近似服从正态分布，根据所求的结果有\n$$ \\hat{\\beta}\\ \\sim N\\left( \\beta ,I^{-1}\\left( \\beta \\right) \\right) $$\n如果 $y$ 不是独立的，协方差矩阵为 $V$，可以推出下面结果\n$$ S\\left( \\beta \\right) =D^{\\mathrm{T}}V^{-1}\\left( y-\\mu \\right) $$\n且\n$$ \\hat{\\beta}\\ \\dot{\\sim} N\\left( \\beta ,I^{-1}\\left( \\beta \\right) \\right) $$\n5 模拟和实例 根据前面的分析可以自己用R编写非线性模型求解函数，以及显著性检验的函数。相关代码附在最后供参考。\n 迭代算法毕竟是局部最优，模拟的初始值最好取在真实值附近。如果是实际问题，可以根据经验、利用OLS估计等最为初始值。\n 5.1 Logistic模型 考虑Logistic增长模型\n$$ y=\\frac{\\beta _1}{1+\\beta _2e^{-\\beta _3x}}+\\varepsilon $$\n在本例中取 $\\beta_1=5$，$\\beta_2=6$，$\\beta_3 = 3$；并且 服从标准正态分布，样本观测数取5000。 $\\epsilon$ 是服从标准正态分布的噪声。在模拟中，使用Gauss—Newton迭代法给定初始迭代值 $b_0=\\left( 4.5,5.2,3.5 \\right) ^{\\mathrm{T}}$，程序迭代5次后收敛，得到系数估计如下图所示\n可见算法的效果还是不错的。\n5.2 Gompertz模型 考虑Gompertz模型\n$$ y=\\beta _1\\exp \\left( -\\beta _2e^{-\\beta _3x} \\right) +\\varepsilon $$\n在本例中取 $\\beta_1=30$，$\\beta_2=14$，$\\beta_3 = 3$；并且 $x$ 服从标准正态分布，样本观测数取5000。 $\\epsilon$ 是服从标准正态分布的噪声。在模拟中，使用Gauss—Newton迭代法给定初始迭代值 $b_0=\\left( 24.5,15.2,3.5 \\right) ^{\\mathrm{T}}$，程序迭代6次后收敛，得到系数估计如下图所示\n可见算法的效果还是不错的。\n5.3 Weibull模型 考虑Weibull模型\n$$ y=\\beta _1-\\beta _2\\exp \\left( -\\beta _3x^{\\beta _4} \\right) +\\varepsilon $$\n在本例中取 $\\beta_1=5$，$\\beta_2=1$，$\\beta_3 = 3$，$\\beta_4=2$；并且 $x$ 服从 $[0,1]$ 的均匀分布，样本观测数取5000。$\\epsilon$ 是服从标准正态分布的噪声。在模拟中，使用Gauss—Newton迭代法给定初始迭代值 ，程序迭代6次后收敛，得到系数估计如下图所示\n可见算法的效果还是不错的。\n5.4 Michaelis-Menten模型 考虑Michaelis-Menten模型\n$$ y=\\frac{\\beta _1x}{\\beta _2+x}+\\varepsilon $$\n对puromycin数据\n   Concentration ($y$) Velocity ($x_1$) Velocity ($x_2$)     0.02 47 76   0.06 97 107   0.11 123 139   0.22 152 159   0.56 191 201   1.10 200 207    考虑上述Michaelis-Menten模型，使用Gauss—Newton迭代法给定初始迭代值 $b_0=\\left( 205.00,0.08 \\right) ^{\\mathrm{T}}$，程序迭代5次后收敛，得到系数估计\n$$ b=\\left( 212.6837,0.0641 \\right) ^{\\mathrm{T}} $$\n利用得到的参数估计计算拟合值，画出图像如下所示\n图中圆点表示真实数据，虚线是得到的Michaelis-Menten模型估计方程。从图中看出，拟合的效果还是不错的。\n5.5 渐近正态性验证 考虑5.1中的Logistic模型，在本次试验中分别取观测数目为50,500,1000和5000来进行模拟，每次模拟都做500次，然后画出每次参数估计中 $\\beta_1$ 的核密度曲线，并与对应的正态密度曲线进行对比，结果如下所示\n从图中可以看出，$\\beta_1$ 的渐近正态性得到了验证。\n6 自编函数代码 6.1 Logistic模型系数求解 mynlmlogistic \u0026lt;- function(x,y,b,N = 5000,e = 1e-10){# # x is the design matrix # b is the starting value of the iteration # N is the upper bound of the times of the iteration # e is the convergence criteria n \u0026lt;- length(x) mylogistic \u0026lt;- expression(a1/(1 + a2*exp(-a3*t))) partial.a1 \u0026lt;- D(mylogistic,\u0026quot;a1\u0026quot;) partial.a2 \u0026lt;- D(mylogistic,\u0026quot;a2\u0026quot;) partial.a3 \u0026lt;- D(mylogistic,\u0026quot;a3\u0026quot;) k \u0026lt;- 1 bk \u0026lt;- b b \u0026lt;- b + 1# to enter the iteration while(sum((bk - b) ^ 2) / sum(b ^ 2) \u0026gt;= e) { # get the D matrix D \u0026lt;- matrix(0, nrow = n, ncol = 3) a1 \u0026lt;- bk[1] a2 \u0026lt;- bk[2] a3 \u0026lt;- bk[3] for (i in 1:n) { t \u0026lt;- x[i] D[i, 1] \u0026lt;- eval(partial.a1) D[i, 2] \u0026lt;- eval(partial.a2) D[i, 3] \u0026lt;- eval(partial.a3) } t \u0026lt;- x f \u0026lt;- eval(mylogistic)# compute the value of f b \u0026lt;- bk# denote the (k)th valuve by b bk \u0026lt;- bk + solve(t(D)%*%D)%*%t(D)%*%(as.matrix(y - f))# denote the (k+1)th value by bk # check if it is divergent if(k \u0026gt; N){ cat(\u0026quot;算法不收敛，已达到最大迭代次数：\u0026quot;,N,\u0026quot;\\n\u0026quot;) cat(\u0026quot;此时得到的解为：\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) print(bk) break } else{ k \u0026lt;- k + 1 } } colnames(bk) \u0026lt;- c(\u0026quot;估计值\u0026quot;) rownames(bk) \u0026lt;- paste0(rep(\u0026quot;系数\u0026quot;,length(bk)),1:length(bk)) logistic.result \u0026lt;- list(\u0026quot;模型的解\u0026quot; = bk,\u0026quot;算法迭代次数\u0026quot; = k-1) return(logistic.result) }  6.2 Gompertz模型系数求解 mynlmgompertz \u0026lt;- function(x,y,b,N = 5000,e = 1e-10){# # x is the design matrix # b is the starting value of the iteration # N is the upper bound of the times of the iteration # e is the convergence criteria n \u0026lt;- length(x) mygompertz \u0026lt;- expression(a1*exp(-a2*exp(-a3*t))) partial.a1 \u0026lt;- D(mygompertz,\u0026quot;a1\u0026quot;) partial.a2 \u0026lt;- D(mygompertz,\u0026quot;a2\u0026quot;) partial.a3 \u0026lt;- D(mygompertz,\u0026quot;a3\u0026quot;) k \u0026lt;- 1 bk \u0026lt;- b b \u0026lt;- b + 1# to enter the iteration while(sum((bk - b) ^ 2) / sum(b ^ 2) \u0026gt;= e) { # get the D matrix D \u0026lt;- matrix(0, nrow = n, ncol = 3) a1 \u0026lt;- bk[1] a2 \u0026lt;- bk[2] a3 \u0026lt;- bk[3] for (i in 1:n) { t \u0026lt;- x[i] D[i, 1] \u0026lt;- eval(partial.a1) D[i, 2] \u0026lt;- eval(partial.a2) D[i, 3] \u0026lt;- eval(partial.a3) } t \u0026lt;- x f \u0026lt;- eval(mygompertz)# compute the value of f b \u0026lt;- bk# denote the (k)th valuve by b bk \u0026lt;- bk + solve(t(D)%*%D)%*%t(D)%*%(as.matrix(y - f))# denote the (k+1)th value by bk # check if it is divergent if(k \u0026gt; N){ cat(\u0026quot;算法不收敛，已达到最大迭代次数：\u0026quot;,N,\u0026quot;\\n\u0026quot;) cat(\u0026quot;此时得到的解为：\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) print(bk) break } else{ k \u0026lt;- k + 1 } } colnames(bk) \u0026lt;- c(\u0026quot;估计值\u0026quot;) rownames(bk) \u0026lt;- paste0(rep(\u0026quot;系数\u0026quot;,length(bk)),1:length(bk)) gompertz.result \u0026lt;- list(\u0026quot;模型的解\u0026quot; = bk,\u0026quot;算法迭代次数\u0026quot; = k-1) return(gompertz.result) }  6.3 Weibull模型系数求解 mynlmweibull \u0026lt;- function(x,y,b,N = 5000,e = 1e-10){# # x is the design matrix # b is the starting value of the iteration # N is the upper bound of the times of the iteration # e is the convergence criteria n \u0026lt;- length(x) myweibull \u0026lt;- expression(a1 - a2*exp(-a3*(t^a4))) partial.a1 \u0026lt;- D(myweibull,\u0026quot;a1\u0026quot;) partial.a2 \u0026lt;- D(myweibull,\u0026quot;a2\u0026quot;) partial.a3 \u0026lt;- D(myweibull,\u0026quot;a3\u0026quot;) partial.a4 \u0026lt;- D(myweibull,\u0026quot;a4\u0026quot;) k \u0026lt;- 1 bk \u0026lt;- b b \u0026lt;- b + 1# to enter the iteration while(sum((bk - b) ^ 2) / sum(b ^ 2) \u0026gt;= e) { # get the D matrix D \u0026lt;- matrix(0, nrow = n, ncol = 4) a1 \u0026lt;- bk[1] a2 \u0026lt;- bk[2] a3 \u0026lt;- bk[3] a4 \u0026lt;- bk[4] for (i in 1:n) { t \u0026lt;- x[i] D[i, 1] \u0026lt;- eval(partial.a1) D[i, 2] \u0026lt;- eval(partial.a2) D[i, 3] \u0026lt;- eval(partial.a3) D[i, 4] \u0026lt;- eval(partial.a4) } t \u0026lt;- x f \u0026lt;- eval(myweibull)# compute the value of f b \u0026lt;- bk# denote the (k)th valuve by b bk \u0026lt;- bk + solve(t(D)%*%D)%*%t(D)%*%(as.matrix(y - f))# denote the (k+1)th value by bk # check if it is divergent if(k \u0026gt; N){ cat(\u0026quot;算法不收敛，已达到最大迭代次数：\u0026quot;,N,\u0026quot;\\n\u0026quot;) cat(\u0026quot;此时得到的解为：\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) print(bk) break } else{ k \u0026lt;- k + 1 } } colnames(bk) \u0026lt;- c(\u0026quot;估计值\u0026quot;) rownames(bk) \u0026lt;- paste0(rep(\u0026quot;系数\u0026quot;,length(bk)),1:length(bk)) weibull.result \u0026lt;- list(\u0026quot;模型的解\u0026quot; = bk,\u0026quot;算法迭代次数\u0026quot; = k-1) return(weibull.result) }  6.4 Michaelis-Menten模型系数求解 mynlmmicmen \u0026lt;- function(x,y,b,N = 5000,e = 1e-10){# # x is the design matrix # b is the starting value of the iteration # N is the upper bound of the times of the iteration # e is the convergence criteria n \u0026lt;- length(x) mymicmen \u0026lt;- expression(a1*t/(a2 + t)) partial.a1 \u0026lt;- D(mymicmen,\u0026quot;a1\u0026quot;) partial.a2 \u0026lt;- D(mymicmen,\u0026quot;a2\u0026quot;) k \u0026lt;- 1 bk \u0026lt;- b b \u0026lt;- b + 1# to enter the iteration while(sum((bk - b) ^ 2) / sum(b ^ 2) \u0026gt;= e) { # get the D matrix D \u0026lt;- matrix(0, nrow = n, ncol = 2) a1 \u0026lt;- bk[1] a2 \u0026lt;- bk[2] for (i in 1:n) { t \u0026lt;- x[i] D[i, 1] \u0026lt;- eval(partial.a1) D[i, 2] \u0026lt;- eval(partial.a2) } t \u0026lt;- x f \u0026lt;- eval(mymicmen)# compute the value of f b \u0026lt;- bk# denote the (k)th valuve by b bk \u0026lt;- bk + solve(t(D)%*%D)%*%t(D)%*%(as.matrix(y - f))# denote the (k+1)th value by bk # check if it is divergent if(k \u0026gt; N){ cat(\u0026quot;算法不收敛，已达到最大迭代次数：\u0026quot;,N,\u0026quot;\\n\u0026quot;) cat(\u0026quot;此时得到的解为：\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) print(bk) break } else{ k \u0026lt;- k + 1 } } colnames(bk) \u0026lt;- c(\u0026quot;估计值\u0026quot;) rownames(bk) \u0026lt;- paste0(rep(\u0026quot;系数\u0026quot;,length(bk)),1:length(bk)) mymicmen.result \u0026lt;- list(\u0026quot;模型的解\u0026quot; = bk,\u0026quot;算法迭代次数\u0026quot; = k-1) return(mymicmen.result) }  6.5 单个系数检验 ## 求解非线性模型单个系数检验 mynlmindivi \u0026lt;- function(x,y,b,expr){# # expr 表示模型：logistic, gompertz, weibull n \u0026lt;- length(x) p \u0026lt;- length(b) t \u0026lt;- x D \u0026lt;- matrix(0, nrow = n, ncol = p) # get the D matrix if(expr == \u0026quot;logistic\u0026quot;){ mylogistic \u0026lt;- expression(a1/(1 + a2*exp(-a3*t))) partial.a1 \u0026lt;- D(mylogistic,\u0026quot;a1\u0026quot;) partial.a2 \u0026lt;- D(mylogistic,\u0026quot;a2\u0026quot;) partial.a3 \u0026lt;- D(mylogistic,\u0026quot;a3\u0026quot;) a1 \u0026lt;- b[1] a2 \u0026lt;- b[2] a3 \u0026lt;- b[3] D[, 1] \u0026lt;- eval(partial.a1) D[, 2] \u0026lt;- eval(partial.a2) D[, 3] \u0026lt;- eval(partial.a3) } else if(expr == \u0026quot;gompertz\u0026quot;){ mygompertz \u0026lt;- expression(a1*exp(-a2*exp(-a3*t))) partial.a1 \u0026lt;- D(mygompertz,\u0026quot;a1\u0026quot;) partial.a2 \u0026lt;- D(mygompertz,\u0026quot;a2\u0026quot;) partial.a3 \u0026lt;- D(mygompertz,\u0026quot;a3\u0026quot;) a1 \u0026lt;- b[1] a2 \u0026lt;- b[2] a3 \u0026lt;- b[3] D[, 1] \u0026lt;- eval(partial.a1) D[, 2] \u0026lt;- eval(partial.a2) D[, 3] \u0026lt;- eval(partial.a3) } else if(expr == \u0026quot;weibull\u0026quot;){ myweibull \u0026lt;- expression(a1 - a2*exp(-a3*(t^a4))) partial.a1 \u0026lt;- D(myweibull,\u0026quot;a1\u0026quot;) partial.a2 \u0026lt;- D(myweibull,\u0026quot;a2\u0026quot;) partial.a3 \u0026lt;- D(myweibull,\u0026quot;a3\u0026quot;) partial.a4 \u0026lt;- D(myweibull,\u0026quot;a4\u0026quot;) a1 \u0026lt;- b[1] a2 \u0026lt;- b[2] a3 \u0026lt;- b[3] a4 \u0026lt;- b[4] D[, 1] \u0026lt;- eval(partial.a1) D[, 2] \u0026lt;- eval(partial.a2) D[, 3] \u0026lt;- eval(partial.a3) D[, 4] \u0026lt;- eval(partial.a4) } else{ stop(cat(\u0026quot;!!!!!!模型参数错误，请选择给定模型logistic, gompertz, weibull之一\u0026quot;)) } cc \u0026lt;- solve(t(D)%*%D) diagc \u0026lt;- diag(cc) Tt \u0026lt;- 1:p;Tp \u0026lt;- Tt;Ts \u0026lt;- Tt for(i in 1:p){ Tt[i] \u0026lt;- b[i]/sqrt(diagc[i]) Tp[i] \u0026lt;- 2*(1 - pnorm(Tt[i])) if(Tp[i] \u0026lt; 0.001 ){# 判断置信度 Ts[i] \u0026lt;- c(\u0026quot;***\u0026quot;) } else if(Tp[i] \u0026lt; 0.01 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.001){ Ts[i] \u0026lt;- c(\u0026quot;**\u0026quot;) } else if(Tp[i] \u0026lt; 0.05 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.01){ Ts[i] \u0026lt;- c(\u0026quot;*\u0026quot;) } else if(Tp[i] \u0026lt; 0.1 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.05){ Ts[i] \u0026lt;- c(\u0026quot;.\u0026quot;) } else{ Ts[i] \u0026lt;- c(\u0026quot; \u0026quot;) } } indivitest \u0026lt;- data.frame(\u0026quot;Tu\u0026quot; = Tt,\u0026quot;Tp\u0026quot; = Tp,\u0026quot;Ts\u0026quot; = Ts) return(indivitest) }  6.6 区间估计 ##求解非线性模型区间估计 mynlminterval \u0026lt;- function(x,y,b,expr,alpha = 0.05){ n \u0026lt;- length(x) p \u0026lt;- length(b) t \u0026lt;- x D \u0026lt;- matrix(0, nrow = n, ncol = p) # get the D matrix if(expr == \u0026quot;logistic\u0026quot;){ mylogistic \u0026lt;- expression(a1/(1 + a2*exp(-a3*t))) partial.a1 \u0026lt;- D(mylogistic,\u0026quot;a1\u0026quot;) partial.a2 \u0026lt;- D(mylogistic,\u0026quot;a2\u0026quot;) partial.a3 \u0026lt;- D(mylogistic,\u0026quot;a3\u0026quot;) a1 \u0026lt;- b[1] a2 \u0026lt;- b[2] a3 \u0026lt;- b[3] D[, 1] \u0026lt;- eval(partial.a1) D[, 2] \u0026lt;- eval(partial.a2) D[, 3] \u0026lt;- eval(partial.a3) } else if(expr == \u0026quot;gompertz\u0026quot;){ mygompertz \u0026lt;- expression(a1*exp(-a2*exp(-a3*t))) partial.a1 \u0026lt;- D(mygompertz,\u0026quot;a1\u0026quot;) partial.a2 \u0026lt;- D(mygompertz,\u0026quot;a2\u0026quot;) partial.a3 \u0026lt;- D(mygompertz,\u0026quot;a3\u0026quot;) a1 \u0026lt;- b[1] a2 \u0026lt;- b[2] a3 \u0026lt;- b[3] D[, 1] \u0026lt;- eval(partial.a1) D[, 2] \u0026lt;- eval(partial.a2) D[, 3] \u0026lt;- eval(partial.a3) } else if(expr == \u0026quot;weibull\u0026quot;){ myweibull \u0026lt;- expression(a1 - a2*exp(-a3*(t^a4))) partial.a1 \u0026lt;- D(myweibull,\u0026quot;a1\u0026quot;) partial.a2 \u0026lt;- D(myweibull,\u0026quot;a2\u0026quot;) partial.a3 \u0026lt;- D(myweibull,\u0026quot;a3\u0026quot;) partial.a4 \u0026lt;- D(myweibull,\u0026quot;a4\u0026quot;) a1 \u0026lt;- b[1] a2 \u0026lt;- b[2] a3 \u0026lt;- b[3] a4 \u0026lt;- b[4] D[, 1] \u0026lt;- eval(partial.a1) D[, 2] \u0026lt;- eval(partial.a2) D[, 3] \u0026lt;- eval(partial.a3) D[, 4] \u0026lt;- eval(partial.a4) } else{ stop(cat(\u0026quot;!!!!!!模型参数错误，请选择给定模型logistic, gompertz, weibull之一\u0026quot;)) } Ti \u0026lt;- matrix(0,nrow = p,ncol = 2) cc \u0026lt;- solve(t(D)%*%D) diagc \u0026lt;- diag(cc) for(i in 1:p){ Ti[i,1] \u0026lt;- b[i] - qnorm((1-alpha/2))*sqrt(diagc[1]) Ti[i,2] \u0026lt;- b[i] + qnorm((1-alpha/2))*sqrt(diagc[1]) } out \u0026lt;- cbind(b,Ti) colnames(out) \u0026lt;- c(\u0026quot;Estimator\u0026quot;,\u0026quot;LowerBound\u0026quot;,\u0026quot;UpperBound\u0026quot;) return(out) }  ","id":37,"section":"posts","summary":"这个笔记总结了非线性模型的极大似然估计量和渐进性质，并推导了用于求解模型的Gauss—Newton迭代法。此外，针对每个内容，我还给出了相应","tags":["多元统计","回归分析"],"title":"非线性模型的参数估计和统计性质","uri":"https://qkai-stat.github.io/2018/05/nlm/","year":"2018"},{"content":"本文总结了线性模型的主要知识点，分别为参数估计，包括最小二乘估计和极大似然估计，区间估计，假设检验。此外，针对每个内容，本文还给出了相应的R软件求解算法，并做了相应的模拟。\n设在线性模型中：$y\\in \\mathbb{R}^n$，$X\\in \\mathbb{R}^{n\\times p}$，$n$ 表示观测数，$p$ 表示变量数。$\\beta \\in \\mathbb{R}^p$ 表示回归系数。$\\epsilon \\sim N\\left( 0,\\sigma ^2I_n \\right) $，是为独立同分布的高斯随机变量。\n1 估计量的表达式 1.1 最小二乘估计量  有截距项的参数估计：  首先令带有截距项的线性模型表达式为：\n$$ y=1\\beta _0+X\\beta+\\epsilon $$\n根据最小二乘思想可知目标函数为 $f\\left( \\beta _0,\\beta \\right) =\\lVert y-1\\beta _0-X\\beta \\rVert _{2}^{2} $，所以将 $f\\left( \\beta _0,\\beta \\right) $ 展开有：\n$$ \\begin{split} f\\left( \\beta _0,\\beta \\right) =\u0026amp;y^{\\mathrm{T}}y+\\beta _01^{\\mathrm{T}}1\\beta _0+2\\beta ^{\\mathrm{T}}X^{\\mathrm{T}}1\\beta _0 \\newline \u0026amp;+\\beta ^{\\mathrm{T}}X^{\\mathrm{T}}X\\beta -2\\beta _01^{\\mathrm{T}}y-2y^{\\mathrm{T}}X\\beta \\end{split} $$\n然后分别对 $\\beta_0$，$\\beta$ 求偏导并令为零：\n$$ \\begin{split} \\frac{\\partial f}{\\beta _0}\u0026amp;=2\\cdot 1^{\\mathrm{T}}1\\beta _0+2\\beta ^{\\mathrm{T}}X ^{\\mathrm{T}}1-2y ^{\\mathrm{T}}1=0 \\newline \\frac{\\partial f}{\\beta }\u0026amp;=2X^{\\mathrm{T}}1\\beta _0+2X^{\\mathrm{T}}X\\beta -2X ^{\\mathrm{T}}y=0 \\end{split} $$\n由上式第1式求得：$\\hat{\\beta}_0=\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}y-\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}X\\beta =\\bar{y}-\\bar{x}^{\\mathrm{T}}\\hat{\\beta} $，其中 $\\bar{X}=\\left( \\bar{x}_1,\\cdots ,\\bar{x}_p \\right) ^{\\mathrm{T}}$ 表示由 $X$ 的每一列的均值组成的列向量。然后将求得的结果带入到第2式中，有：\n$$ X^{\\mathrm{T}}1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}y-X^{\\mathrm{T}}1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}X\\beta +X^{\\mathrm{T}}X\\beta -X^{\\mathrm{T}}y=0 $$\n整理可得：\n$$ -X^{\\mathrm{T}}\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] y+X^{\\mathrm{T}}\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] X\\beta =0 $$\n注意到矩阵 $I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}$ 是对称幂等的，并且 $\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) X$ 可以看成是 $X$ 的每个元素减去所在列的均值得到的新矩阵（就是每一列元素进行中心化）。则记\n$$ X_c=\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] X $$\n所以可以得到\n$$ X_{c}^{\\mathrm{T}}X_c\\beta =X_{c}^{\\mathrm{T}}y $$\n故而得到有截距的最小二乘参数估计为：\n$$ \\begin{split} \\hat{\\beta}_0\u0026amp;=\\bar{y}-\\bar{X}^{\\mathrm{T}}\\hat{\\beta} \\newline \\hat{\\beta}\u0026amp;=\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\end{split} $$\n对所得到的估计进行分析，显然有：\n$$ \\begin{split} E\\hat{\\beta}\u0026amp;=E\\left[ \\left( X _{c}^{\\mathrm{T}}X _c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\right] \\newline \u0026amp;=E\\left[ \\left( X _{c}^{\\mathrm{T}}X _c \\right) ^{-1}X _{c}^{\\mathrm{T}}\\left( 1\\beta _0+X\\beta +\\epsilon \\right) \\right] \\newline \u0026amp;=\\left( X _{c}^{\\mathrm{T}}X _c \\right) ^{-1}X _{}^{\\mathrm{T}}\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) \\left( 1\\beta _0+X\\beta \\right) \\newline \u0026amp;=\\left( X _{c}^{\\mathrm{T}}X _c \\right) ^{-1}X _{}^{\\mathrm{T}}\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) X\\beta \\newline \u0026amp;=\\left( X _{c}^{\\mathrm{T}}X _c \\right) ^{-1}X _{c}^{\\mathrm{T}}X _c\\beta \\newline \u0026amp;=\\beta \\end{split} $$\n这里利用了 $\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) $ 对称幂等的性质。同理有\n$$ \\begin{split} E\\hat{\\beta}_0\u0026amp;=E\\left( \\bar{y}-\\bar{X}^{\\mathrm{T}}\\hat{\\beta} \\right) \\newline \u0026amp;=\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}Ey-\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}X\\beta \\newline \u0026amp;=\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}\\left( 1\\beta _0+X\\beta \\right) -\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}X\\beta \\newline \u0026amp;=\\beta _0 \\end{split} $$\n因此 $\\beta_0$，$\\beta$ 都是无偏估计量。 下面考虑他们的方差，则有\n$$ \\begin{split} Var\\left( \\hat{\\beta} \\right) \u0026amp;=Var\\left( \\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{c}^{\\mathrm{T}}y \\right) \\newline \u0026amp;=\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{c}^{\\mathrm{T}}Var\\left( y \\right) X_{c}^{}\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1} \\newline \u0026amp;=\\sigma ^2\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1} \\end{split} $$\n和\n$$ \\begin{split} Var\\left( \\hat{\\beta}_0 \\right) \u0026amp;=Var\\left( \\bar{y}-\\bar{X}^{\\mathrm{T}}\\hat{\\beta} \\right) \\newline \u0026amp;=Var\\left[ \\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}y-\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}X\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\right] \\newline \u0026amp;=\\frac{\\sigma ^2}{n^2}\\left[ 1^{\\mathrm{T}}1-0-0+1^{\\mathrm{T}}X\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X^{\\mathrm{T}}1 \\right] \\newline \u0026amp;=\\sigma ^2\\left( \\frac{1}{n}+\\bar{X}^{\\mathrm{T}}\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}\\bar{X} \\right) \\end{split} $$\n上式只需注意到 $1^{\\mathrm{T}}X_{c}=1^{\\mathrm{T}}\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) X=0$ 即可。最后，计算 $\\beta_0$ 和 $\\beta$ 之间的协方差：\n$$ \\begin{split} Cov\\left( \\hat{\\beta}_0,\\hat{\\beta} \\right) \u0026amp;=Cov\\left( \\bar{y}-\\bar{X}^{\\mathrm{T}}\\hat{\\beta},\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\right) \\newline \u0026amp;=\\sigma ^2\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}\\left( I-X\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}} \\right) X _{c}\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1} \\newline \u0026amp;=\\sigma ^2\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}\\left( X _{c}\\left( X _{c}^{\\mathrm{T}}X _c \\right) ^{-1}-X\\left( X _{c}^{\\mathrm{T}}X _c \\right) ^{-1} \\right) \\newline \u0026amp;=-\\sigma ^2\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}X\\left( X _{c}^{\\mathrm{T}}X _c \\right) ^{-1} \\newline \u0026amp;=-\\sigma ^2\\bar{X}^{\\mathrm{T}}\\left( X _{c}^{\\mathrm{T}}X _c \\right) ^{-1} \\end{split} $$\n 不带截距项的参数估计：  假设 $\\hat{\\beta}$ 是参数 $\\beta$ 的最小二乘估计量，$\\hat{y}$ 是相应的 $y$ 的最小二乘估计，则根据最小二乘估计的思想，最小化以下目标函数即可：\n$$ \\hat{\\beta}=\\mathrm{arg}\\ \\mathop {\\min} \\limits_\\beta\\lVert y-X\\beta \\rVert _{2}^{2} $$\n令 $f\\left( \\beta \\right) =\\lVert y-X\\beta \\rVert _{2}^{2}$，对 $\\beta$ 进行求导有：\n$$ \\begin{split} \\frac{\\partial f\\left( \\beta \\right)}{\\partial \\beta}\u0026amp;=-2X^{\\mathrm{T}}y+2X^{\\mathrm{T}}X\\beta \\newline \\frac{\\partial ^2f\\left( \\beta \\right)}{\\partial \\beta ^2}\u0026amp;=2X^{\\mathrm{T}}X \\end{split} $$\n根据极值的必要条件，令上式为零有：\n$$ \\hat{\\beta}=\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y $$\n显然在求解中并不需要 假定 $\\epsilon$ 一定服从正态分布。\n对所得到的估计进行分析，显然有：\n$$ \\begin{split} E\\hat{\\beta}\u0026amp;=E\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y \\newline \u0026amp;=\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}X\\beta \\newline \u0026amp;=\\beta \\end{split} $$\n因此 $\\hat{\\beta}$ 是无偏估计量。 下面考虑他的方差：\n$$ \\begin{split} Var\\left( \\hat{\\beta} \\right) \u0026amp;=Var\\left( \\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y \\right) \\newline \u0026amp;=\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}Var\\left( y \\right) X\\left( X^{\\mathrm{T}}X \\right) ^{-1} \\newline \u0026amp;=\\sigma ^2\\left( X^{\\mathrm{T}}X \\right) ^{-1} \\end{split} $$\n1.2 极大似然估计量 首先给出多元正态联合密度函数公式：\n$$ f\\left( x|\\mu ,\\Sigma \\right) =\\left[ \\left( 2\\pi \\right) ^{-\\frac{n}{2}}|\\Sigma |^{-\\frac{1}{2}} \\right] \\exp \\left[ -\\frac{1}{2}\\left( x-\\mu \\right) ^{\\mathrm{T}}\\Sigma ^{-1}\\left( x-\\mu \\right) \\right] $$\n其中 $x$ 服从 $N(\\mu, \\Sigma)$ 分布。在本问题中，我们假定 $\\Sigma =\\sigma ^2I$，也就是随机变量是独立同分布于高斯分布的。\n因此，根据极大似然原理得到似然函数：\n$$ \\mathscr{L}\\left( \\beta ,\\sigma ^2 \\right) =\\left( 2\\pi \\sigma ^2 \\right) ^{-\\frac{n}{2}}\\exp \\left[ -\\frac{1}{2\\sigma ^2}\\left( y-X\\beta \\right) ^{\\mathrm{T}}\\left( y-X\\beta \\right) \\right] $$\n取对数得到对数似然函数：\n$$ \\ln \\mathscr{L}\\left( \\beta ,\\sigma ^2 \\right) =-\\frac{n}{2}\\ln \\left( 2\\pi \\sigma ^2 \\right) -\\frac{1}{2\\sigma ^2}\\left( y-X\\beta \\right) ^{\\mathrm{T}}\\left( y-X\\beta \\right) $$\n上式分别对 $\\beta$ 和 $\\sigma^2$ 求偏导：\n$$ \\begin{split} \\frac{\\partial \\ln \\mathscr{L}}{\\partial \\beta}\u0026amp;=-\\frac{1}{2\\sigma ^2}\\left( -2X^{\\mathrm{T}}y+X^{\\mathrm{T}}X\\beta \\right) =0 \\newline \\frac{\\partial \\ln \\mathscr{L}}{\\partial \\sigma ^2}\u0026amp;=-\\frac{n}{2\\sigma ^2}+\\frac{1}{2\\sigma ^4}\\left( y-X\\beta \\right) ^{\\mathrm{T}}\\left( y-X\\beta \\right) \\end{split} $$\n显然根据第1式得到 $\\beta$ 的极大似然估计量为：\n$$ \\hat{\\beta}=\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y $$\n并且我们还可以得到 $\\sigma^2$ 的估计量：\n$$ \\hat{\\sigma}^2=\\frac{RSS}{n} $$\n其中 $RSS=\\left( y-\\hat{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y} \\right) $ 是残差平方和。\n 从这里可以看出，在正态分布的假设下，最大似然法和最小二乘法的得到的估计量是一致的。 因此，线性模型下使用最小二乘法或者极大似然法估计系数都可以。然而，这种等价性在其他分布类型的回归问题中不成立。\n 2 方差分析 2.1 带截距项的方差分解 首先给出模型总的离差平方和（$SYY$），回归平方和（$SS_{Reg}$）和残差平方和（$RSS$）的定义：\n$$ \\begin{split} SYY\u0026amp;=\\left( y-\\bar{y}1 \\right) ^{\\mathrm{T}}\\left( y-1\\bar{y} \\right) \\newline SS_{Reg}\u0026amp;=\\left( \\hat{y}-1\\bar{y} \\right) ^{\\mathrm{T}}\\left( \\hat{y}-1\\bar{y} \\right) \\newline RSS\u0026amp;=\\left( y-\\hat{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y} \\right) \\end{split} $$\n则对 $SYY$ 有：\n$$ \\begin{split} SYY\u0026amp;=\\left( y-\\hat{y}+\\hat{y}-1\\bar{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y}+\\hat{y}-1\\bar{y} \\right) \\newline \u0026amp;=\\left( y-\\hat{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y} \\right) +\\left( \\hat{y}-1\\bar{y} \\right) ^{\\mathrm{T}}\\left( \\hat{y}-1\\bar{y} \\right)+2\\left( y-\\hat{y} \\right) ^{\\mathrm{T}}\\left( \\hat{y}-1\\bar{y} \\right) \\newline \u0026amp;=SS_{\\mathrm{Re}g}+RSS+2\\left( \\hat{y}-1\\bar{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y} \\right) \\end{split} $$\n考虑带有截距项的模型的参数估计：\n$$ \\begin{split} \\hat{\\beta}_0\u0026amp;=\\bar{y}-\\bar{X}^{\\mathrm{T}}\\hat{\\beta} \\newline \\hat{\\beta}\u0026amp;=\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\end{split} $$\n由此可以得到 $\\hat{y}$ 为：\n$$ \\begin{split} \\hat{y}\u0026amp;=1\\hat{\\beta}_0+X\\hat{\\beta} \\newline \u0026amp;=1\\left[ \\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}y-\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}X\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\right]+X\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\newline \u0026amp;=1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}y+\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] X\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\newline \u0026amp;=1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}y+X_c\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\end{split} $$\n所以：\n$$ \\begin{split} y-\\hat{y}\u0026amp;=\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] y-X_c\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\newline \\hat{y}-1\\bar{y}\u0026amp;=X_c\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\end{split} $$\n所以：\n$$ \\begin{split} \\left( \\hat{y}-1\\bar{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y} \\right) \u0026amp;=y^{\\mathrm{T}}X_c\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] y \\newline \u0026amp;\\quad-y^{\\mathrm{T}}X_c\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\end{split} $$\n而 $\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] $ 对称幂等，则有：\n$$ \\begin{split} X _{c}^{\\mathrm{T}}\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] \u0026amp;=X^{\\mathrm{T}}\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] ^2 \\newline \u0026amp;=X^{\\mathrm{T}}\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] \\newline \u0026amp;=X _{c}^{\\mathrm{T}} \\end{split} $$\n所以得到 $\\left( \\hat{y}-1\\bar{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y} \\right)$。这样我们就得到一个重要的分解恒等式：\n$$ SYY=RSS+SS_{\\mathrm{Reg}} $$\n注意，在按照前面各个方差的定义下，只有带截距项的模型才使得上式成立。 通常的模型都是带有截距的，因此在实际运用中，按照前面定义的不同离差是比较常见的。\n 在本节不同离差的定义下，不带截距项的模型分解恒等式不一定成立。\n 2.2 通用型方差分解式 除了以上形式的方差类型的定义方式外，还有一种模型方差分解的定义：\n$$ \\begin{split} SYY\u0026amp;=y^{\\mathrm{T}}y \\newline SS_{Reg}\u0026amp;=\\hat{y}^{\\mathrm{T}}\\hat{y} \\newline RSS\u0026amp;=\\left( y-\\hat{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y} \\right) \\end{split} $$\n将上式中的 $RSS$ 乘开得到：\n$$ RSS=y^{\\mathrm{T}}y+\\hat{y}^{\\mathrm{T}}\\hat{y}-2y^{\\mathrm{T}}\\hat{y} $$\n考虑不带截距项的模型有 $\\hat{\\beta}=\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y$，所以 $\\hat{y}=X\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y$，故而有：\n$$ \\begin{split} \\hat{y}^{\\mathrm{T}}\\hat{y}\u0026amp;=y^{\\mathrm{T}}X\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}X\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y \\newline \u0026amp;=y^{\\mathrm{T}}X\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y \\newline y^{\\mathrm{T}}\\hat{y}\u0026amp;=y^{\\mathrm{T}}X\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y=\\hat{y}^{\\mathrm{T}}\\hat{y} \\end{split} $$\n所以将上式结果带入到 $RSS$ 中得到：\n$$ \\begin{split} RSS\u0026amp;=y^{\\mathrm{T}}y+\\hat{y}^{\\mathrm{T}}\\hat{y}-2y^{\\mathrm{T}}\\hat{y}=y^{\\mathrm{T}}y+\\hat{y}^{\\mathrm{T}}\\hat{y}-2\\hat{y}^{\\mathrm{T}}\\hat{y} \\newline \u0026amp;=y^{\\mathrm{T}}y-\\hat{y}^{\\mathrm{T}}\\hat{y} \\newline \u0026amp;=SYY-SS_{\\mathrm{Re}g} \\end{split} $$\n这样就证明了方差分解公式成立。\n 在本节方差的定义下，带截距项模型的分解恒等式也成立。\n 通过上述分析可以看出，如果是带有截距项的模型，一般按照第一种定义进行分解；如果是不带截距项的模型，则按照式第二种定义进行分解。\n2.3 方差分析表  带截距项的模型  对带截距项的模型有：\n$$ \\begin{split} \\hat{\\beta}_0\u0026amp;=\\bar{y}-\\bar{X}^{\\mathrm{T}}\\hat{\\beta} \\newline \\hat{\\beta}\u0026amp;=\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\end{split} $$\n根据2.1节各个离差的定义，假定设计矩阵 $X$ 列满秩且 $\\epsilon \\sim N\\left( 0,\\sigma ^2I_n \\right)$。\n虑总离差平方和 $SYY$：\n$$ \\begin{split} SYY\u0026amp;=\\left( y-1\\bar{y} \\right) ^{\\mathrm{T}}\\left( y-1\\bar{y} \\right) \\newline \u0026amp;=y^{\\mathrm{T}}\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) ^{\\mathrm{T}}\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) y \\newline \u0026amp;=y^{\\mathrm{T}}\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) y \\end{split} $$\n注意到 $P_1=1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}$ 对称、幂等且 $I-P_1$ 秩为 $n-1$。对 $y=1\\beta _0+X\\beta+\\epsilon $，容易得到 $y\\sim N\\left( \\hat{\\mu},\\sigma ^2I_n \\right) $，其中 $\\hat{\\mu}=1\\beta _0+X\\beta$，故而：\n$$ \\frac{SYY}{\\sigma ^2}=\\left( y/\\sigma \\right) ^{\\mathrm{T}}\\left( I-P_1 \\right) \\left( y/\\sigma \\right) \\sim \\chi _{n-1}^{2}\\left( \\frac{\\hat{\\mu}^{\\mathrm{T}}\\left( I-P_1 \\right) \\hat{\\mu}}{\\sigma ^2} \\right) $$\n考虑残差平方和 $RSS$：\n$$ \\begin{split} RSS \u0026amp;=\\left( y-\\hat{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y} \\right) \\newline \u0026amp;=\\left( y-1\\hat{\\beta}_0-X\\hat{\\beta} \\right) ^{\\mathrm{T}}\\left( y-1\\hat{\\beta}_0-X\\hat{\\beta} \\right) \\newline \u0026amp;=y^{\\mathrm{T}}\\left( I-P_1-P _{X_c}+P_1P _{X_c}+P _{X_c}P_1 \\right) y \\end{split} $$\n其中 $P_{X_c}$ 秩为 $p$ 且满足：\n$$ \\begin{split} P_{X_c}\u0026amp;=X_c\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}} \\newline \u0026amp;=\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) X\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}} \\newline \u0026amp;=\\left( I-P_1 \\right) X\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}} \\end{split} $$\n所以有 $\\left( I-P _1 \\right) P _{X _c}=P _{X _c}$，同理可以验证 $P _{X _c}\\left( I-P _1 \\right) =P _{X _c}$ 也成立。则对 $\\left( I-P _1-P _{X _c}+P _1P _{X _c}+P _{X _c}P _1 \\right) $ 有：\n$$ \\begin{split} \u0026amp;I-P _1-P _{X _c}+P _1P _{X _c}+P _{X _c}P _1 \\newline \u0026amp;=I-\\left( I-P _1 \\right) P _{X _c}-P _1+P _{X _c}P _1 \\newline \u0026amp;=I-P _{X _c}-P _1+P _{X _c}P _1 \\newline \u0026amp;=I-P _1-P _{X _c}\\left( I-P _1 \\right) \\newline \u0026amp;=I-P _1-P _{X _c} \\end{split} $$\n可以验证 $I-P_1-P_{X_c}$ 是对称、幂等的且秩为 $n-p-1$，因此\n$$ \\frac{RSS}{\\sigma ^2}=\\left( \\frac{y}{\\sigma} \\right) ^{\\mathrm{T}}\\left( I-P_1-P_{X_c} \\right) \\left( \\frac{y}{\\sigma} \\right) \\sim \\chi _{n-p-1}^{2} $$\n这里的卡方分布中心没有偏移是因为\n$$ \\begin{split} \\hat{\\mu}^{\\mathrm{T}}\\left( I-P _1-P _{X _c} \\right) \u0026amp;=\\left( 1\\beta _0+X\\beta \\right) ^{\\mathrm{T}}\\left( I-P _1-P _{X _c} \\right) \\newline \u0026amp;=\\left( \\beta _01^{\\mathrm{T}}-\\beta _01^{\\mathrm{T}}P _1 \\right) -\\beta _01^{\\mathrm{T}}P _{X _c}+\\beta^{\\mathrm{T}}X^{\\mathrm{T}}-\\beta^{\\mathrm{T}}X^{\\mathrm{T}}P _1-\\beta^{\\mathrm{T}}X^{\\mathrm{T}}P _{X _c} \\newline \u0026amp;=0-0+\\beta^{\\mathrm{T}}X^{\\mathrm{T}}\\left( I-P _1 \\right) -\\beta^{\\mathrm{T}}X^{\\mathrm{T}}P _{X _c} \\newline \u0026amp;=0 \\end{split} $$\n考虑回归平方和 $SS _{Reg}$：\n$$ \\begin{split} SS _{Reg}\u0026amp;=\\left( \\hat{y}-1\\bar{y} \\right) ^{\\mathrm{T}}\\left( \\hat{y}-1\\bar{y} \\right) \\newline \u0026amp;=y^{\\mathrm{T}}X _c\\left( X _{c}^{\\mathrm{T}}X _c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\newline \u0026amp;=y^{\\mathrm{T}}P _{X _c}y \\end{split} $$\n结合 $y\\sim N\\left( \\hat{\\mu},\\sigma ^2I_n \\right)$，$\\hat{\\mu}=1\\beta _0+X\\beta+\\mu$，故而\n$$ \\frac{SS_{Reg}}{\\sigma ^2}=\\left( y/\\sigma \\right) ^{\\mathrm{T}}P_{X_c}\\left( y/\\sigma \\right) \\sim \\chi _{p}^{2}\\left( \\frac{\\tilde{\\mu}^{\\mathrm{T}}P _{X_c}\\tilde{\\mu}}{\\sigma ^2} \\right) $$\n 观察到 $\\left( I-P_1-P_{X_c} \\right) \\times P_{X_c}=0$，故而 $RSS$ 和 $SS_{Reg}$ 是独立同分布的。\n 综合上述可得到带截距项的模型方差分析表\n   方差来源 平方和 自由度 $df$ 均方     回归 $SS_{Reg}$ $p$ $SS_{Reg}/p$   残差 $RSS$ $n-p-1$ $RSS/(n-p-1)$   总离差 $SYY$ $n-1$      不带截距的模型  对不带截距项的模型有：\n$$ \\hat{\\beta}=\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y $$\n根据式2.2各个离差的定义，假定设计矩阵 $X$ 列满秩且 $\\epsilon \\sim N\\left( 0,\\sigma ^2I_n \\right) $。\n考虑总离差平方和 $SYY$：\n其中 $y=X\\beta +\\epsilon \\sim \\left( \\tilde{\\mu},\\sigma ^2I_n \\right) $，$\\tilde{\\mu}=X\\beta $，故而\n$$ \\frac{SYY}{\\sigma ^2}=\\left( y/\\sigma \\right) ^{\\mathrm{T}}\\left( y/\\sigma \\right) \\sim \\chi _{n}^{2}\\left( \\frac{\\tilde{\\mu}^{\\mathrm{T}}\\tilde{\\mu}}{\\sigma ^2} \\right) $$\n考虑残差平方和 $RSS$\n$$ \\begin{split} RSS=\\left( y-\\hat{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y} \\right) \u0026amp;=\\left( y-X\\hat{\\beta} \\right) ^{\\mathrm{T}}\\left( y-X\\hat{\\beta} \\right) \\newline \u0026amp;=\\left( y-X\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y \\right) ^{\\mathrm{T}}\\left( y-X\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y \\right) \\newline \u0026amp;=y^{\\mathrm{T}}\\left( I-P_X \\right) y \\end{split} $$\n其中 $P_X=X\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}$ 是空间 $\\mathscr{R}\\left( X \\right) $ 的正交投影算子。根据矩阵知识， $I-P_X$ 对称、幂等且秩为 $n-p$，所以\n$$ \\frac{RSS}{\\sigma ^2}=\\left( y/\\sigma \\right) ^{\\mathrm{T}}\\left( I-P_X \\right) \\left( y/\\sigma \\right) \\sim \\chi _{n-p}^{2} $$\n这里的卡方分布也是没有中心偏移的。\n考虑回归平方和 $SS_{Reg}$\n$$ SS_{Reg}=\\hat{y}^{\\mathrm{T}}\\hat{y}=y^{\\mathrm{T}}P_Xy $$\n结合 $y=X\\beta +\\epsilon \\sim N\\left( \\tilde{\\mu},\\sigma ^2I_n \\right)$，$\\tilde{\\mu}=X\\beta +\\mu $，故而\n$$ \\frac{SS_{Reg}}{\\sigma ^2}=\\left( y/\\sigma \\right) ^{\\mathrm{T}}P_X\\left( y/\\sigma \\right) \\sim \\chi _{p}^{2}\\left( \\frac{\\tilde{\\mu}^{\\mathrm{T}}P_X\\tilde{\\mu}}{\\sigma ^2} \\right) $$\n 通过上述分析，观察到 $\\left( I-P_X \\right) \\times P_X=0$，故而 $RSS$ 和 $SS_{Reg}$ 是独立同分布的。\n 最后，综合上述可得到不带截距项的模型方差分析表\n   方差来源 平方和 自由度 $df$ 均方     回归 $SS_{Reg}$ $p$ $SS_{Reg}/p$   残差 $RSS$ $n-p$ $RSS/(n-p)$   总离差 $SYY$ $n$     3 统计显著性检验 3.1 模型显著性  带截距项的模型  在这个检验前提下，我们的原假设和备择假设通常是\n$$ \\begin{split} \u0026amp;H_0:\\beta=0 \\newline \u0026amp;H_1:\\beta\\ne 0 \\end{split} $$\n根据前述可知\n$$ \\begin{split} \u0026amp;\\frac{RSS}{\\sigma^2}\\ \\sim \\chi_{n-p-1}^{2} \\newline \u0026amp;\\frac{SS_{Reg}}{\\sigma^2} \\sim \\chi_{p}^{2}\\left( \\frac{\\hat{\\mu}^{\\mathrm{T}}P_{X_c}\\hat{\\mu}}{\\sigma^2} \\right) \\end{split} $$\n其中 $\\hat{\\mu}=1\\beta _0+X\\beta$，故而当原假设成立时有 $\\hat{\\mu}=1\\beta _0$，因此\n$$ \\begin{split} \\hat{\\mu}^{\\mathrm{T}}\\left( I-P _1-P _{X _c} \\right) \\hat{\\mu}\u0026amp;=\\beta _{0}^{2}1^{\\mathrm{T}}\\left( I-P _1-P _{X _c} \\right) 1 \\newline \u0026amp;=\\beta _{0}^{2}\\left( 1^{\\mathrm{T}}1-1^{\\mathrm{T}}1-1^{\\mathrm{T}}P _{X _c}1 \\right) \\newline \u0026amp;=-\\beta _{0}^{2}1^{\\mathrm{T}}P _{X _c}1 \\end{split} $$\n而对 $1^{\\mathrm{T}}P_{X_c}1$ 有\n$$ \\begin{split} 1^{\\mathrm{T}}P_{X_c}1\u0026amp;=1^{\\mathrm{T}}X_c\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{c}^{\\mathrm{T}}1 \\newline \u0026amp;=1^{\\mathrm{T}}\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) X\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{c}^{\\mathrm{T}}1 \\newline \u0026amp;=\\left( 1^{\\mathrm{T}}-1^{\\mathrm{T}} \\right) X\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{c}^{\\mathrm{T}}1 \\newline \u0026amp;=0 \\end{split} $$\n所以 $\\hat{\\mu}^{\\mathrm{T}}\\left( I-P_1-P_{X_c} \\right) \\hat{\\mu}=0$，同理可知 $\\hat{\\mu}^{\\mathrm{T}}P_{X_c}\\hat{\\mu}=0$，故而在原假设成立的条件下有\n$$ \\begin{split} \\frac{RSS}{\\sigma^2}\\ \u0026amp;\\sim \\chi_{n-p-1}^{2} \\newline \\frac{SS_{Reg}}{\\sigma^2} \u0026amp;\\sim \\chi_{p}^{2} \\end{split} $$\n所以，在 $H_0$ 成立的条件下有\n$$ F_0=\\frac{SS_{Reg}/p}{RSS/\\left( n-p-1 \\right)}\\sim F_{p,n-p-1} $$\n这样当给定置信水平 $\\alpha$，在原假设成立的条件下回归平方和 $SS_{Reg}$ 较小，而残差平方和 $RSS$则较大，因此检验统计量 $F_0$ 就应该较小。那么，拒绝原假设的条件就是 $F_0\\ge F_{\\alpha ,p,n-p-1}$。 这里 $F_{\\alpha ,p,n-p-1}$ 表示上侧 $\\alpha$ 分位数（下同）。\n 不带截距项的模型  在这个检验前提下，我们的原假设和备择假设通常是\n$$ \\begin{split} \u0026amp;H_0:\\beta _1=\\beta _2=\\cdots =\\beta _p=0 \\newline \u0026amp;H_1:\\beta _j\\ne 0,\\mathrm{至少对一}个j\\mathrm{成立} \\end{split} $$\n类似的，当原假设成立时有 $\\hat{\\mu}=0$，则\n$$ \\begin{split} \\frac{RSS}{\\sigma^2}\\ \u0026amp;\\sim \\chi_{n-p}^{2} \\newline \\frac{SS_{Reg}}{\\sigma^2} \u0026amp;\\sim \\chi_{p}^{2} \\end{split} $$\n所以，在原假设成立的条件下有\n$$ F_0=\\frac{SS_{Reg}/p}{RSS/\\left( n-p \\right)}~F_{p,n-p} $$\n拒绝原假设的条件同前。\n3.2 单个系数的检验 单个系数的检验通常和从模型中增删变量有关。一般来说，增加一个额外的变量到模型中去，不会使得回归平方和 减小，也不会使得残差平方和 $SS_{Reg}$ 增大。但是，回归平方和 $SS_{Reg}$ 的一点点增大能否充分的保证在模型中引入了一个冗余变量？我们之所以关心，是因为增加一个冗余变量到模型中确实会增大均方误差，因而将模型的可用性降低了。\n检验任何一个单独的回归系数 $\\beta_j$ 的假设通常是\n$$ \\begin{split} \u0026amp;H_0:\\beta _j=0 \\newline \u0026amp;H_1:\\beta _j\\ne 0 \\end{split} $$\n 带截距项的模型  对带截距项的模型有\n$$ \\hat{\\beta}=\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{c}^{\\mathrm{T}}y $$\n因为 $E\\hat{\\beta}=\\beta$ 和 $Var\\left( \\hat{\\beta} \\right) =\\sigma ^2\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}$，显然有\n$$ \\frac{\\hat{\\beta}_j-\\beta _j}{\\sqrt{\\sigma ^2C _{jj}}}\\sim N\\left( 0,1 \\right) $$\n其中 $C_{jj}$ 是矩阵 $\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}$ 第 个对角元。但是由于一般情况下 $\\sigma^2$ 未知，所以我们再考虑 $RSS$ 有\n$$ \\frac{RSS}{\\sigma ^2}\\ \\sim \\chi _{n-p-1}^{2} $$\n所以有\n$$ \\frac{\\left( n-p-1 \\right) s^2}{\\sigma ^2}\\sim \\chi _{n-p-1}^{2} $$\n其中 $s^2$ 是样本的方差，是可以通过样本计算得到的。那么，在原假设成立的条件下有\n$$ t_0 = \\frac{\\hat{\\beta}_j}{\\sqrt{s^2C _{jj}}} \\sim t _{n-p-1} $$\n因此，给定置信水平 $\\alpha$ 在原假设成立的条件下，上式 $t_0$ 的绝对值就不会特别大。那么，拒绝 $H_0$ 的条件就是：$| t_0 |\u0026gt;t_{\\alpha /2,n-p-1}$。\n对于截距项有\n$$ \\frac{\\hat{\\beta}_0-\\beta _0}{\\sqrt{\\sigma ^2\\left( 1/n+\\bar{X}^{\\mathrm{T}}\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}\\bar{X} \\right)}}\\sim N\\left( 0,1 \\right) $$\n因此\n$$ t_0 = \\frac{\\hat{\\beta}_j}{\\sqrt{s^2(1/n + \\bar{X} ^{\\mathrm{T}}(X _c^{\\mathrm{T}}X _c)^{-1}\\bar{X})}} \\sim t _{n-p-1} $$\n综述，给定置信水平 $\\alpha$ 在原假设 $H_0$ 成立的条件下，拒绝 $H_0$ 的条件就是：$| t_0 |\u0026gt;t_{\\alpha /2,n-p-1}$。\n 不带截距项的模型  对不带截距项的模型有\n$$ \\hat{\\beta}=\\left( X_{}^{\\mathrm{T}}X_{} \\right) ^{-1}X_{}^{\\mathrm{T}}y $$\n类似的，因为 $E\\hat{\\beta}=\\beta $ 和 $E\\hat{\\beta}=\\beta $，所以\n$$ \\frac{\\hat{\\beta}_j-\\beta_j}{\\sqrt{\\sigma ^2C _{jj}}} \\sim N\\left( 0,1 \\right) $$\n其中 $C_{jj}$ 是矩阵 $(X^{\\mathrm{T}}X)^{-1}$ 第 $j$ 个对角元。但是由于一般情况下 $\\sigma^2$ 未知，所以我们再考虑 $RSS$ 有\n$$ \\frac{RSS}{\\sigma ^2}\\ \\sim \\chi _{n-p}^{2} $$\n所以有\n$$ \\frac{\\left( n-p \\right) s^2}{\\sigma ^2} \\sim \\chi _{n-p}^{2} $$\n其中 $s^2$ 是样本的方差。那么在原假设成立的条件下有\n$$ t_0 = \\frac{\\hat{\\beta}_j}{\\sqrt{s^2C _{jj}}} \\sim t _{n-p} $$\n综述，给定置信水平 $\\alpha$ 在原假设 $H_0$ 成立的条件下，拒绝 $H_0$ 的条件就是：$| t_0 |\u0026gt;t_{\\alpha /2,n-p}$。\n4 区间估计 4.1 单个回归系数的置信区间  带截距项的区间估计  根据3.2节的分析和讨论，对于不带截距项的模型有\n$$ \\frac{\\hat{\\beta}_j-\\beta _j}{\\sqrt{s^2C _{jj}}}\\sim t _{n-p-1} $$\n所以，给定置信水平 $\\alpha$ 下 $\\beta_j$ 置信区间为：\n$$ \\hat{\\beta} _{j} - t _{\\alpha /2,n-p-1}\\sqrt{s^2C _{jj}}\\le \\beta _j\\le \\hat{\\beta}_j+t _{\\alpha /2,n-p-1}\\sqrt{s^2C _{jj}} $$\n 不带截距项的区间估计  类似的，给定置信水平 $\\alpha$ 下 $\\beta_j$ 置信区间为\n$$ \\hat{\\beta} _{j} - t _{\\alpha /2,n-p}\\sqrt{s^2C _{jj}}\\le \\beta _j\\le \\hat{\\beta}_j+t _{\\alpha /2,n-p}\\sqrt{s^2C _{jj}} $$\n4.2 回归系数的联合置信区域 前面的置信区间都是针对单个系数进行的，因此置信水平只对一个区间有效。但是，很多问题中需要让置信水平对所有的区间有效，这就是联合置信区域（simultaneous confidence intervals）。\n 带截距项的模型  由3.2节可知\n$$ \\left( \\hat{\\beta}-\\beta \\right) \\sim N\\left( 0,\\sigma ^2\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1} \\right) $$\n所以\n$$ \\frac{\\left( \\hat{\\beta}-\\beta \\right) ^{\\mathrm{T}}\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}\\left( \\hat{\\beta}-\\beta \\right)}{\\sigma ^2}\\sim \\chi _{p}^{2} $$\n这里 $\\sigma^2$ 未知，为构造检验统计量再考虑 $RSS$ 有\n$$ \\frac{RSS}{\\sigma ^2} \\sim \\chi _{n-p-1}^{2} $$\n所以得到\n$$ \\frac{\\left( \\hat{\\beta}-\\beta \\right) ^{\\mathrm{T}}\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}\\left( \\hat{\\beta}-\\beta \\right) /p}{RSS/\\left( n-p-1 \\right)} \\sim F_{p,n-p-1} $$\n那么在置信水平 $\\alpha$ 下，所有的回归系数需要满足\n$$ \\frac{\\left( \\hat{\\beta}-\\beta \\right) ^{\\mathrm{T}}\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}\\left( \\hat{\\beta}-\\beta \\right) /p}{RSS/\\left( n-p-1 \\right)}\\le F_{\\alpha ,p,n-p-1} $$\n 不带截距项的模型  同理可知，对于不带截距项的模型，在给定置信水平 $\\alpha$ 下全部系数需要满足\n$$ \\frac{\\left( \\hat{\\beta}-\\beta \\right) ^{\\mathrm{T}}\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}\\left( \\hat{\\beta}-\\beta \\right) /p}{RSS/\\left( n-p-1 \\right)}\\le F_{\\alpha ,p,n-p} $$\n对于前述联合置信区域的不等式描述的是一个椭圆约束区域。 当 $p=2$ 时，这个区域相当简单；但是当 $p\\geq 2$ 时，问题就变得复杂的多。\n5 模拟分析 我在简单回归分析中介绍了R求解线性模型的方法，这里我们根据前面的理论分析结果，用R编写系数求解的函数、单个系数的显著性检验函数。这些代码附在最后供参考。\n我们考虑模型\n$$ y=1\\beta _0+X\\beta+\\epsilon $$\n其中 $\\beta _0=3$，$\\beta=\\left( 1,3,2,1,4,5,2,6 \\right) ^{\\mathrm{T}}$，$y\\in \\mathbb{R}^{100}$。根据前述分析，使用最小二乘法进行估计，在R软件中得到结果如下\n可以看出估计得结果很好，模型和单个系数都通过了检验。\n当然，我们可以对相同的数据使用R软件求解带截距的线性模型，得到的结果如下图所示。可以看出，两者是一致的。\n此外，我们也可以对模型考虑不带截距项的估计。这样得到的结果如下图所示\n对比R软件内置函数求解的结果\n首先，两者得到的结果是一样的。其次，我们发现对于一个本身带有截距项的模型使用不带截距项的方法求解，得到的效果就不好。 因为可以看到变量 $x_4$ 的系数估计没有通过检验。\n我们可以通过计算模型得到的残差，如学生化残差，通过分析残差的分布和对应的正态分布之间的异同，从而对回归结果做出分析和判断。如下图所示，是带截距模型的学生化残差分布图（左）以及不带截距的学生化残差分布图（右）\n从图上可以看出来，使用正确的模型得到的残差分布和正态分布较为接近，说明估计的效果好；而使用错误的模型得到的结果，残差分布和正态分布相差较大，估计的效果不好。\n6 自编函数代码 6.1 模型求解 实际使用，主要运行该代码即可。这个代码整合了参数估计、检验等代码，将结果合并输出。\n##求解线性模型 mylm \u0026lt;- function(x,y,method = \u0026quot;ols\u0026quot;,intercept = T){# # 本程序用来求解线性模型参数估计 区间估计和假设检验 # method可选：ols,max 前者最小二乘 后者极大似然估计 # intercept为T表示模型带截距 为F表示模型不带截距 # 求解估计量 b \u0026lt;- mylmestimate(x,y,method = method,intercept = intercept) # 求解离差平方和分解 rsquare \u0026lt;- mylmsquare(x,y,b,intercept = intercept) # 单个系数检验 indivi \u0026lt;- mylmindivi(x,y,b,intercept = intercept) # 模型检验 lmtest \u0026lt;- mylmtest(x,y,b,intercept = intercept) if(intercept){ bname \u0026lt;- c(\u0026quot;(Intercept)\u0026quot;,paste0(rep(\u0026quot;x\u0026quot;,p),1:p)) } else{ bname \u0026lt;- paste0(rep(\u0026quot;x\u0026quot;,p),1:p) } estimate.test \u0026lt;- data.frame( \u0026quot;Beta\u0026quot; = bname, \u0026quot;Estimate\u0026quot; = b, \u0026quot;t.value\u0026quot; = indivi$Tt, \u0026quot;P.value\u0026quot; = indivi$Tp, \u0026quot;Sig.\u0026quot; = indivi$Ts ) # 输出结果 if(intercept){ cat(\u0026quot;Coefficients:\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) print(estimate.test) cat(\u0026quot;---\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) cat(\u0026quot;Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) cat(\u0026quot;---\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) cat(\u0026quot;Multiple R-squared:\u0026quot;,(rsquare$sreg/rsquare$syy),\u0026quot;\u0026quot;) cat(\u0026quot;Adjusted R-squared:\u0026quot;,(1-(rsquare$rss*(n-1))/(rsquare$syy*(n-p-1))),\u0026quot;\\n\u0026quot;) cat(\u0026quot;F-statistic:\u0026quot;,lmtest$f0,\u0026quot;\u0026quot;) cat(\u0026quot;on\u0026quot;,p,\u0026quot;\u0026quot;) cat(\u0026quot;and\u0026quot;,(n-p-1),\u0026quot;\u0026quot;) cat(\u0026quot;DF,\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\u0026quot;) cat(\u0026quot;p-value:\u0026quot;,lmtest$p,\u0026quot;\u0026quot;) } else{ cat(\u0026quot;Coefficients:\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) print(estimate.test) cat(\u0026quot;---\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) cat(\u0026quot;Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) cat(\u0026quot;---\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) cat(\u0026quot;Multiple R-squared:\u0026quot;,(rsquare$sreg/rsquare$syy),\u0026quot;\u0026quot;) cat(\u0026quot;Adjusted R-squared:\u0026quot;,(1-(rsquare$rss*n)/(rsquare$syy*(n-p))),\u0026quot;\\n\u0026quot;) cat(\u0026quot;F-statistic:\u0026quot;,lmtest$f0,\u0026quot;\u0026quot;) cat(\u0026quot;on\u0026quot;,p,\u0026quot;\u0026quot;) cat(\u0026quot;and\u0026quot;,(n-p),\u0026quot;\u0026quot;) cat(\u0026quot;DF,\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\u0026quot;) cat(\u0026quot;p-value:\u0026quot;,lmtest$p,\u0026quot;\u0026quot;) } }  6.2 参数估计 ##求解线性模型参数估计 mylmestimate \u0026lt;- function(x,y,method = \u0026quot;ols\u0026quot;,intercept = T){# n \u0026lt;- length(y) if(method == \u0026quot;ols\u0026quot;){ if(intercept){ xbar0 \u0026lt;- apply(x,2,mean) # 按x的列计算均值 xbar1 \u0026lt;- matrix(rep(xbar0,n),nrow = n,byrow = T) xcenter \u0026lt;- x - xbar1 # 得到按列中心化的矩阵x # 得到最小二乘估计 b1 \u0026lt;- solve(t(xcenter)%*%xcenter)%*%t(xcenter)%*%y b0 \u0026lt;- mean(y) - t(xbar0)%*%b1 b \u0026lt;- c(b0,b1) } else{ # 得到最小二乘估计 b \u0026lt;- solve(t(x)%*%x)%*%t(x)%*%y } } else{ if(intercept){ xbar0 \u0026lt;- apply(x,2,mean) # 按x的列计算均值 xbar1 \u0026lt;- matrix(rep(xbar0,n),nrow = n,byrow = T) xcenter \u0026lt;- x - xbar1 # 得到按列中心化的矩阵x # 得到极大似然估计 b1 \u0026lt;- solve(t(xcenter)%*%xcenter)%*%t(xcenter)%*%y b0 \u0026lt;- mean(y) - t(xbar0)%*%b1 b \u0026lt;- c(b0,b1) } else{ # 得到极大似然估计 b \u0026lt;- solve(t(x)%*%x)%*%t(x)%*%y } } return(b) }  6.3 假设检验 ##求解线性模型假设检验 mylmtest \u0026lt;- function(x,y,b,intercept = T){ n \u0026lt;- dim(x)[1] p \u0026lt;- dim(x)[2] if(intercept){ ybar \u0026lt;- mean(y) yhat \u0026lt;- b[1] + x%*%b[2:(p+1)]#计算回归值 # 计算各个离差平方和 rss \u0026lt;- t((y - yhat))%*%(y - yhat) sreg \u0026lt;- t((yhat - ybar))%*%(yhat - ybar) # 计算检验统计量 f0 \u0026lt;- (sreg/p)/(rss/(n - p - 1)) # 输出p值 p \u0026lt;- 1 - pf(f0,p,n-p-1) } else{ yhat \u0026lt;- x%*%b#计算回归值 # 计算各个离差平方和 rss \u0026lt;- t((y - yhat))%*%(y - yhat) sreg \u0026lt;- t(yhat)%*%(yhat) # 计算检验统计量 f0 \u0026lt;- (sreg/p)/(rss/(n - p)) # 输出p值 p \u0026lt;- 1 - pf(f0,p,n-p) } modeltest \u0026lt;- data.frame(\u0026quot;f0\u0026quot; = f0,\u0026quot;p\u0026quot; = p) return(modeltest) }  6.4 单个系数检验 ##求解线性模型单个系数检验 mylmindivi \u0026lt;- function(x,y,b,intercept = T){ n \u0026lt;- dim(x)[1] p \u0026lt;- dim(x)[2] if(intercept){ yhat \u0026lt;- b[1] + x%*%b[2:(p+1)]#计算回归值 s \u0026lt;- t((y - yhat))%*%(y - yhat)/(n-p-1) #计算MSE xbar0 \u0026lt;- apply(x,2,mean) # 按x的列计算均值 xbar1 \u0026lt;- matrix(rep(xbar0,n),nrow = n,byrow = T) xcenter \u0026lt;- x - xbar1 # 得到按列中心化的矩阵x cc \u0026lt;- solve(t(xcenter)%*%xcenter) diagc \u0026lt;- diag(cc) Tt \u0026lt;- 1:(p+1);Tp \u0026lt;- Tt;Ts \u0026lt;- Tt Tt[1] \u0026lt;- b[1]/sqrt(s*(1/n + t(xbar0)%*%cc%*%xbar0)) Tp[1] \u0026lt;- 2*(1 - pt(Tt[1],n-p-1)) if(Tp[1] \u0026lt; 0.001 ){# 判断置信度 Ts[1] \u0026lt;- c(\u0026quot;***\u0026quot;) } else if(Tp[1] \u0026lt; 0.01 \u0026amp;\u0026amp; Tp[1] \u0026gt;= 0.001){ Ts[1] \u0026lt;- c(\u0026quot;**\u0026quot;) } else if(Tp[1] \u0026lt; 0.05 \u0026amp;\u0026amp; Tp[1] \u0026gt;= 0.01){ Ts[1] \u0026lt;- c(\u0026quot;*\u0026quot;) } else if(Tp[1] \u0026lt; 0.1 \u0026amp;\u0026amp; Tp[1] \u0026gt;= 0.05){ Ts[1] \u0026lt;- c(\u0026quot;.\u0026quot;) } else{ Ts[1] \u0026lt;- c(\u0026quot; \u0026quot;) } for(i in 2:(p+1)){ Tt[i] \u0026lt;- b[i]/sqrt(s*diagc[i-1]) Tp[i] \u0026lt;- 2*(1 - pt(Tt[i],n-p-1)) if(Tp[i] \u0026lt; 0.001 ){# 判断置信度 Ts[i] \u0026lt;- c(\u0026quot;***\u0026quot;) } else if(Tp[i] \u0026lt; 0.01 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.001){ Ts[i] \u0026lt;- c(\u0026quot;**\u0026quot;) } else if(Tp[i] \u0026lt; 0.05 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.01){ Ts[i] \u0026lt;- c(\u0026quot;*\u0026quot;) } else if(Tp[i] \u0026lt; 0.1 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.05){ Ts[i] \u0026lt;- c(\u0026quot;.\u0026quot;) } else{ Ts[i] \u0026lt;- c(\u0026quot; \u0026quot;) } } } else{ yhat \u0026lt;- x%*%b#计算回归值 s \u0026lt;- t((y - yhat))%*%(y - yhat)/(n-p) #计算MSE cc \u0026lt;- solve(t(x)%*%x) diagc \u0026lt;- diag(cc) Tt \u0026lt;- 1:p;Tp \u0026lt;- Tt;Ts \u0026lt;- Tt for(i in 1:p){ Tt[i] \u0026lt;- b[i]/sqrt(s*diagc[i]) Tp[i] \u0026lt;- 2*(1 - pt(Tt[i],n-p)) if(Tp[i] \u0026lt; 0.001 ){# 判断置信度 Ts[i] \u0026lt;- c(\u0026quot;***\u0026quot;) } else if(Tp[i] \u0026lt; 0.01 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.001){ Ts[i] \u0026lt;- c(\u0026quot;**\u0026quot;) } else if(Tp[i] \u0026lt; 0.05 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.01){ Ts[i] \u0026lt;- c(\u0026quot;*\u0026quot;) } else if(Tp[i] \u0026lt; 0.1 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.05){ Ts[i] \u0026lt;- c(\u0026quot;.\u0026quot;) } else{ Ts[i] \u0026lt;- c(\u0026quot; \u0026quot;) } } } indivitest \u0026lt;- data.frame(\u0026quot;Tt\u0026quot; = Tt,\u0026quot;Tp\u0026quot; = Tp,\u0026quot;Ts\u0026quot; = Ts) return(indivitest) }  6.5 区间估计 ##求解线性模型区间估计 mylminterval \u0026lt;- function(x,y,b,alpha = 0.05,intercept = T){ n \u0026lt;- dim(x)[1] p \u0026lt;- dim(x)[2] if(intercept){ Ti \u0026lt;- matrix(0,nrow = (p+1),ncol = 2) yhat \u0026lt;- b[1] + x%*%b[2:(p+1)]#计算回归值 s \u0026lt;- t((y - yhat))%*%(y - yhat)/(n-p-1) #计算MSE xbar0 \u0026lt;- apply(x,2,mean) # 按x的列计算均值 xbar1 \u0026lt;- matrix(rep(xbar0,n),nrow = n,byrow = T) xcenter \u0026lt;- x - xbar1 # 得到按列中心化的矩阵x cc \u0026lt;- solve(t(xcenter)%*%xcenter) diagc \u0026lt;- diag(cc) Ti[1,1] \u0026lt;- b[1] - qt((1-alpha/2),n-p-1)*sqrt(s*diagc[1]) Ti[1,2] \u0026lt;- b[1] + qt((1-alpha/2),n-p-1)*sqrt(s*diagc[1]) for(i in 2:(p+1)){ Ti[i,1] \u0026lt;- b[i] - qt((1-alpha/2),n-p-1)*sqrt(s*diagc[1]) Ti[i,2] \u0026lt;- b[i] + qt((1-alpha/2),n-p-1)*sqrt(s*diagc[1]) } } else{ Ti \u0026lt;- matrix(0,nrow = p,ncol = 2) yhat \u0026lt;- x%*%b#计算回归值 s \u0026lt;- t((y - yhat))%*%(y - yhat)/(n-p) #计算MSE cc \u0026lt;- solve(t(x)%*%x) diagc \u0026lt;- diag(cc) for(i in 1:p){ Ti[i,1] \u0026lt;- b[i] - qt((1-alpha/2),n-p-1)*sqrt(s*diagc[1]) Ti[i,2] \u0026lt;- b[i] + qt((1-alpha/2),n-p-1)*sqrt(s*diagc[1]) } } colnames(Ti) \u0026lt;- c(\u0026quot;LowerBound\u0026quot;,\u0026quot;UpperBound\u0026quot;) return(Ti) }  6.6 离差平方和分解 ##求解线性模型离差平方和分解 mylmsquare \u0026lt;- function(x,y,b,intercept = T){ if(intercept){ ybar \u0026lt;- mean(y) yhat \u0026lt;- b[1] + x%*%b[2:(p+1)]#计算回归值 # 计算各个离差平方和 rss \u0026lt;- t((y - yhat))%*%(y - yhat) sreg \u0026lt;- t((yhat - ybar))%*%(yhat - ybar) syy \u0026lt;- t((y - ybar))%*%(y - ybar) } else{ yhat \u0026lt;- x%*%b#计算回归值 # 计算各个离差平方和 rss \u0026lt;- t((y - yhat))%*%(y - yhat) sreg \u0026lt;- t(yhat)%*%(yhat) syy \u0026lt;- t(y)%*%y } rsquare \u0026lt;- data.frame(\u0026quot;syy\u0026quot; = syy,\u0026quot;rss\u0026quot; = rss,\u0026quot;sreg\u0026quot; = sreg) return(rsquare) }  6.7 各种残差 ##求解线性模型各种残差 mylmresidual \u0026lt;- function(x,y,b,intercept = T){ n \u0026lt;- dim(x)[1] p \u0026lt;- dim(x)[2] if(intercept){ yhat \u0026lt;- b[1] + x%*%b[2:(p+1)]# 计算回归值 sig.hat \u0026lt;- t(y - yhat)%*%(y - yhat)/(n-p-1)# 计算方差估计值 # 计算帽子矩阵对角元 xbar0 \u0026lt;- apply(x,2,mean) # 按x的列计算均值 xbar1 \u0026lt;- matrix(rep(xbar0,n),nrow = n,byrow = T) xcenter \u0026lt;- x - xbar1 # 得到按列中心化的矩阵x p1 \u0026lt;- matrix(1/n,nrow = n,ncol = n) h \u0026lt;- p1 + xcenter%*%solve(t(xcenter)%*%xcenter)%*%t(xcenter) diagh \u0026lt;- diag(h) # 计算原始残差 originres \u0026lt;- y - yhat # 计算standardized residuals standres \u0026lt;- (y - yhat)/sqrt(sig.hat*rep(1,n)) # 计算student residuals studres \u0026lt;- (y - yhat)/sqrt(sig.hat*(1 - diagh)) # 计算PRESS residual pressres \u0026lt;- (y - yhat)/(1 - diagh) } else{ yhat \u0026lt;- x%*%b# 计算回归值 sig.hat \u0026lt;- t((y - yhat))%*%(y - yhat)/(n-p)# 计算方差估计值 # 计算原始残差 originres \u0026lt;- y - yhat # 计算帽子矩阵对角元 h \u0026lt;- x%*%solve(t(x)%*%x)%*%t(x) diagh \u0026lt;- diag(h) # 计算standardized residuals standres \u0026lt;- (y - yhat)/sqrt(sig.hat*rep(1,n)) # 计算student residuals studres \u0026lt;- (y - yhat)/sqrt(sig.hat*(1 - diagh)) # 计算PRESS residual pressres \u0026lt;- (y - yhat)/(1 - diagh) } lmres \u0026lt;- data.frame(\u0026quot;oringinal.Residual\u0026quot; = originres, \u0026quot;Standardized.Residual\u0026quot; = standres, \u0026quot;Student.Residual\u0026quot; = studres, \u0026quot;PRESS.Residual\u0026quot; = pressres) return(lmres) }  ","id":38,"section":"posts","summary":"本文总结了线性模型的主要知识点，分别为参数估计，包括最小二乘估计和极大似然估计，区间估计，假设检验。此外，针对每个内容，本文还给出了相应的R","tags":["多元统计","回归分析"],"title":"线性模型的理论与求解","uri":"https://qkai-stat.github.io/2018/04/lm/","year":"2018"},{"content":"惊雷起，\n一霎清明夜雨袭。\n帘外风轻，\n潺潺春语，\n入梦浇睡意。\n心思尽去残花里。\n流水无情空悲寂。\n明朝晓看红湿处，\n杨柳依依，\n梨花风起，\n愁染缙纭堤。\n","id":39,"section":"posts","summary":"惊雷起， 一霎清明夜雨袭。 帘外风轻， 潺潺春语， 入梦浇睡意。 心思尽去残花里。 流水无情空悲寂。 明朝晓看红湿处， 杨柳依依， 梨花风起， 愁染缙纭堤。","tags":["虎溪岁月","诗文"],"title":"夜雨清明","uri":"https://qkai-stat.github.io/2018/04/yyqm/","year":"2018"},{"content":"渝州故郡，重庆新府。横断巴楚，环扣水枢。三都之地，嘉陵长江汇韵；四方要冲，名家骚客聚属。都督刘公之雅望，重大建始；校长叶公之懿范，往来鸿儒。禹功有继，文翁留㤖。今哉重大，居985。名师云集，学子仰慕。花香大地，翠拥缙湖。廊腰缦回，林荫杏道真佳境；勾心斗角，诗照长亭好学处。\n余辞乡别离，求学万里。客居虎溪，专业统计。驽钝术浅，常慕鹦鹉之机；乏善可陈，空怀击流之意。所赖数统师亲，传道授业答疑；同窗修睦，直谅多闻明礼。四载光阴荏苒，一处心思凭寄。\n肄业在即，毕设期近。杨虎吾师，富学可亲。教授博导，治学严谨。幸蒙教诲，循循善行。洞火灵犀，醍醐灌顶。师兄师姐，帮助殷殷。评讲文献，指点迷津。心念及此，感激不尽。\n余智术有限，学浅才疏。草创成文，难免谬误。请洒潘江，各倾陆海云尔。\n","id":40,"section":"posts","summary":"渝州故郡，重庆新府。横断巴楚，环扣水枢。三都之地，嘉陵长江汇韵；四方要冲，名家骚客聚属。都督刘公之雅望，重大建始；校长叶公之懿范，往来鸿儒。","tags":["诗文","虎溪岁月"],"title":"本科毕设致谢","uri":"https://qkai-stat.github.io/2017/06/bkbszx/","year":"2017"},{"content":"东汉末年，桓灵败政。君失道于百姓；民毋念及“火德”。贪狼盛而紫薇衰，豪强起而暴民乱。权佞当道，途有哀鸿饿殍；英雄逐鹿，有志待价而沽。狼烟四起，烽鼓难息，州吞郡合，而终致天下三分。\n期间名人，不乏有数:盖拥兵自重者，冀州袁绍；勇冠三军者，徐州吕布；衣食祖业者，益州刘璋；风流座客者，荆州刘表。然则公台寡断，奉先乏谋，季玉暗弱，景升志穷……盖以英雄称者，屈指可数矣——江东孙吴，跨江连郡，拒荆刘于江渚，败许曹于赤壁； 汉中刘备，盘踞西蜀，外抚羌夷余力，内安万民有术。此二人宏才雅量，志图天下，可以英雄称焉。然余独爱者，绝非孙刘，唯曹孟德一人而已。\n为操之论，一言难尽。余观夫操之人，亦臣亦君，亦盗亦民；所谓臣君，操之气度，所谓盗民，操之性情。臣君盗民，论其一生，古人云：美玉微瑕。此之余何独慕之。\n先时，宦竖乱朝，外戚干政，黄巾起而荼毒，董贼入而祸乱。惜哉，旧臣不臣，幽泣拥喟于室；新盟虽结，各怀鬼胎于心。操，身非苗裔，本承阉宦之嗣；胸藏鸿图，力尽汉臣之实。所以臣之节气，尽燃于胸，奈何持节守将，徒壁上观。呜呼!谋刀不在，檄文已失；潜龙勿用，养兵屯势。迨及气候稍聚，兵容初整，囊贤达逐汉鹿，挟天子令不臣。柔远能迩，悖(dūn)德允元。若弗尧察舜恭，则音律夺伦，文辞难兴（xìng），四凶作宰，百姓不亲。所以谓操有君度，诚乱世之枭雄。是故英雄不必至贤，而能贤贤，其身未必良将，而善将将。\n及至九锡服銮，出警入跸，左右维诺，庸命卑恭。文若在前，季珪难终，时谓阿瞒窃汉之心昭昭。但帝位尤忝，汉庙未隳。姑妄言之，或操诡谲，扶汉至伪，实则矫诏以便其意欤！抑或留待子嗣，登阁僭位，旧臣切念新恩，以期戮力而为。虽然，盗名之性，穷口莫辩。使操后观司马事，未知其意何如?噫!奈何天不假年，痼疾难消，病笃岌危，梦魇复绕。未尝嘱白帝之托，恍若山野小民，而忧妻妾营生。盖戎马峥峥寿尽，富贵闻达浮云，岂不闻鸟之将死，其鸣也哀；人之将死，其情也善。\n嗟乎， 余辞乡别离，求学万里，毕业将近，投笔无期。驽钝术浅，尝慕鹦鹉之机；乏善可陈，空怀击流之意。今复观三国旧事，聊以赋文，暂长精神而已。\n","id":41,"section":"posts","summary":"东汉末年，桓灵败政。君失道于百姓；民毋念及“火德”。贪狼盛而紫薇衰，豪强起而暴民乱。权佞当道，途有哀鸿饿殍；英雄逐鹿，有志待价而沽。狼烟四起","tags":["虎溪岁月","诗文"],"title":"曹操赋","uri":"https://qkai-stat.github.io/2017/03/ccf/","year":"2017"},{"content":"渝州冬早立，近日尚秋衣。\n今明披绣闼，始觉冬意袭。\n瑟瑟西风浸，萧萧梧叶稀。\n是处斑驳景，唯独缙纭堤。\n擎伞盖已去，漏春仍昨夕。\n相对相忘言，只道无和羲。\n","id":42,"section":"posts","summary":"渝州冬早立，近日尚秋衣。 今明披绣闼，始觉冬意袭。 瑟瑟西风浸，萧萧梧叶稀。 是处斑驳景，唯独缙纭堤。 擎伞盖已去，漏春仍昨夕。 相对相忘言，只道无和","tags":["虎溪岁月","诗文"],"title":"冬日过缙纭堤","uri":"https://qkai-stat.github.io/2016/11/drgjyd/","year":"2016"},{"content":"蓝桥古驿无踪觅，\n不见崔郎遇云英。\n可怜浆向虽易乞，\n难容相访饮牛津。\n","id":43,"section":"posts","summary":"蓝桥古驿无踪觅， 不见崔郎遇云英。 可怜浆向虽易乞， 难容相访饮牛津。","tags":["虎溪岁月","诗文"],"title":"读纳兰词","uri":"https://qkai-stat.github.io/2016/11/dnlc/","year":"2016"},{"content":"银杏叶黄暑气收，\n天凉月高怕登楼。\n怕登楼。\n一见鸿书销眉愁。\n夜浣碧纱掩薄秋，\n累君多奔走，\n愿撷晚霞作兰舟。\n君言舟摇珠帘招，\n风也飘飘雨潇潇。\n雨潇潇。\n赌书廊前有陈雕。\n我抱长琴在虹桥，\n银字流水调，\n不教红樱负绿蕉。\n","id":44,"section":"posts","summary":"银杏叶黄暑气收， 天凉月高怕登楼。 怕登楼。 一见鸿书销眉愁。 夜浣碧纱掩薄秋， 累君多奔走， 愿撷晚霞作兰舟。 君言舟摇珠帘招， 风也飘飘雨潇潇。 雨潇潇。","tags":["虎溪岁月","诗文"],"title":"过银杏道","uri":"https://qkai-stat.github.io/2016/10/gyxd/","year":"2016"},{"content":"骤雨如烟喧小楼，\n群芳摇落意无休。\n望断巴山犹未尽，\n点点雨丝点点愁。\n","id":45,"section":"posts","summary":"骤雨如烟喧小楼， 群芳摇落意无休。 望断巴山犹未尽， 点点雨丝点点愁。","tags":["虎溪岁月","诗文"],"title":"松园记雨","uri":"https://qkai-stat.github.io/2016/06/syjy/","year":"2016"},{"content":"盛夏偏作晚秋天，\n小晴忽复雨涟涟。\n一任灯花自消落，\n辗转心思顾影怜。\n","id":46,"section":"posts","summary":"盛夏偏作晚秋天， 小晴忽复雨涟涟。 一任灯花自消落， 辗转心思顾影怜。","tags":["虎溪岁月","诗文"],"title":"夏日有感","uri":"https://qkai-stat.github.io/2016/05/xryg/","year":"2016"},{"content":" 最喜欢这首诗！也许我天生就是一个懒散的人吧~\n 独坐轩窗向小楼，\n斜阳灼灼木幽幽。\n懒理红尘烦扰事，\n遥映菱花慢梳头。\n","id":47,"section":"posts","summary":"最喜欢这首诗！也许我天生就是一个懒散的人吧~ 独坐轩窗向小楼， 斜阳灼灼木幽幽。 懒理红尘烦扰事， 遥映菱花慢梳头。","tags":["虎溪岁月","诗文"],"title":"无题","uri":"https://qkai-stat.github.io/2016/05/wt/","year":"2016"},{"content":"北风呼号雪飘飘，一人在家甚无聊。\n前日刚说有雨雪，明朝又要防寒潮。\n漫将闲书消冷寂，且煮陈茶度寒宵。\n无意庭院凄风紧，腊梅残落竟逍遥。\n","id":48,"section":"posts","summary":"北风呼号雪飘飘，一人在家甚无聊。 前日刚说有雨雪，明朝又要防寒潮。 漫将闲书消冷寂，且煮陈茶度寒宵。 无意庭院凄风紧，腊梅残落竟逍遥。","tags":["诗文"],"title":"初雪","uri":"https://qkai-stat.github.io/2016/01/cx/","year":"2016"},{"content":"凉风沁骨别暑天，\n芙蓉燎栗雨涟涟。\n十五中秋明月夜，\n卧听吴郎斧声坚。\n","id":49,"section":"posts","summary":"凉风沁骨别暑天， 芙蓉燎栗雨涟涟。 十五中秋明月夜， 卧听吴郎斧声坚。","tags":["虎溪岁月","诗文"],"title":"中秋有感","uri":"https://qkai-stat.github.io/2015/09/zqyg/","year":"2015"},{"content":" 2012年，读高二的我看到高三学长和学姐在举行“百日誓师大会”。他们的喊声不由得触动了我，让我对未来很是憧憬。几日后，便写下了这首诗。\n 江流浮沉风云涌，江畔何人起敛容。\n跣足常趋握发殿，躬身频至玉蟾宫。\n雄心勃勃扫漠北，虎视眈眈向江东。\n待到青梅煮酒日，拔剑示君论英雄。\n","id":50,"section":"posts","summary":"2012年，读高二的我看到高三学长和学姐在举行“百日誓师大会”。他们的喊声不由得触动了我，让我对未来很是憧憬。几日后，便写下了这首诗。 江流浮","tags":["虎溪岁月","诗文"],"title":"咏曹操","uri":"https://qkai-stat.github.io/2012/03/ycc/","year":"2012"}],"tags":[{"title":"git","uri":"https://qkai-stat.github.io/tags/git/"},{"title":"r语言","uri":"https://qkai-stat.github.io/tags/r%E8%AF%AD%E8%A8%80/"},{"title":"优化算法","uri":"https://qkai-stat.github.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"},{"title":"凸优化","uri":"https://qkai-stat.github.io/tags/%E5%87%B8%E4%BC%98%E5%8C%96/"},{"title":"分类算法","uri":"https://qkai-stat.github.io/tags/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"},{"title":"变量选择","uri":"https://qkai-stat.github.io/tags/%E5%8F%98%E9%87%8F%E9%80%89%E6%8B%A9/"},{"title":"回归分析","uri":"https://qkai-stat.github.io/tags/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/"},{"title":"多元统计","uri":"https://qkai-stat.github.io/tags/%E5%A4%9A%E5%85%83%E7%BB%9F%E8%AE%A1/"},{"title":"数学史","uri":"https://qkai-stat.github.io/tags/%E6%95%B0%E5%AD%A6%E5%8F%B2/"},{"title":"数据可视化","uri":"https://qkai-stat.github.io/tags/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"title":"文本分析","uri":"https://qkai-stat.github.io/tags/%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90/"},{"title":"机器学习","uri":"https://qkai-stat.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"title":"算法","uri":"https://qkai-stat.github.io/tags/%E7%AE%97%E6%B3%95/"},{"title":"经典算法理论","uri":"https://qkai-stat.github.io/tags/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/"},{"title":"虎溪岁月","uri":"https://qkai-stat.github.io/tags/%E8%99%8E%E6%BA%AA%E5%B2%81%E6%9C%88/"},{"title":"诗文","uri":"https://qkai-stat.github.io/tags/%E8%AF%97%E6%96%87/"},{"title":"高维数据分析","uri":"https://qkai-stat.github.io/tags/%E9%AB%98%E7%BB%B4%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}]}